{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5POXAmvydqQZ"
      },
      "source": [
        "# Introduction to Deep Learning with Keras\n",
        "## Using the Keras API to Implement Deep Learning in Python\n",
        "\n",
        "[Keras](https://keras.io/) is a high-level neural networks API, capable of running on top of [__Tensorflow__](https://github.com/tensorflow/tensorflow), [__Theano__](https://github.com/Theano/Theano), and [CNTK](https://github.com/Microsoft/cntk). It enables fast experimentation through a high level, user-friendly, modular and extensible API. Keras can also be run on both CPU and GPU.\n",
        "\n",
        "Keras was developed and is maintained by [Francois Chollet](https://twitter.com/fchollet?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor) and is part of the Tensorflow core, which makes it Tensorflow's __preferred__ high-level API.\n",
        "\n",
        "This tutorial explains how to use Keras for deep learning.\n",
        "\n",
        "In this tutorial, we will go over the basics of Keras including the two most used Keras models (Sequential and Functional), the core layers as well as some preprocessing functionalities.\n",
        "\n",
        "## Table of Contents\n",
        "- [Keras Installation](#Keras-Installation)\n",
        "- [Loading the Dataset](#Loading-the-Dataset)\n",
        "- [Creating a Sequential Model](#Creating-a-sequential-model-in-Keras)\n",
        "- [Dissecting the NN](#Dissecting-the-NN)\n",
        "- [Creating a model with the Functional API](#Creating-a-model-with-the-Functional-API)\n",
        "- [Compile a model](#Compile-a-model)\n",
        "- [Augmenting Image Data](#Augmenting-Image-data)\n",
        "- [Fit a Model](#Fit-a-model)\n",
        "- [Conclusion](#Conclusion)\n",
        "- [Exercise/Assignment](#Exercise/Assignment)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFNFVAiqdqQe"
      },
      "source": [
        "## Keras Installation\n",
        "\n",
        "Keras is a front-end API - which means it handles user interation (I/O) and requires a working deep learning backend. Among the three we listed above, we are going to use __TensorFlow__ (TF) as the backend in this tutorial. TF is also the default backend for Keras.\n",
        "\n",
        "You can use following command to install __TF__:\n",
        "```bash\n",
        "pip install tensorflow\n",
        "```\n",
        "\n",
        "Refer to this [link](https://www.tensorflow.org/install/) regarding TF installation.\n",
        "\n",
        "After installing TF, you can use following command to install __Keras__:\n",
        "```bash\n",
        "pip install keras\n",
        "```\n",
        "or\n",
        "```bash\n",
        "conda install keras\n",
        "```\n",
        "\n",
        "Refer to this [link](https://keras.io/#installation) regarding Keras installation.\n",
        "\n",
        "__NOTE__: If you are going to use TF as the backend of Keras, you do not have to configure the link. Otherwise (e.g. using Theano as the backend), you will need to change the backend in the `keras.json` file.\n",
        "\n",
        "For the purpose of this tutorial, both TF and Keras are pre-installed on your instances."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZJ3QRSedqQf"
      },
      "source": [
        "## Loading the Dataset\n",
        "\n",
        "Keras provides [seven different datasets](https://keras.io/datasets/), which can be loaded in using Keras directly. These include image datasets as well as a [house price](http://lib.stat.cmu.edu/datasets/boston) and a [movie review](https://ai.stanford.edu/~amaas/data/sentiment/) datasets.\n",
        "\n",
        "In this article, we will use the [MNIST dataset](http://yann.lecun.com/exdb/mnist/), which contains 70000 28x28 grayscale images with 10 different classes (`0`-`9`). Keras splits it in a __training set__ with `60000` instances and a __testing set__ with `10000` instances.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIk5PnvalFt-"
      },
      "outputs": [],
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WECDF1ImdqQf",
        "outputId": "acbe2ad8-e2e7-470d-bfea-b1cfdee24df2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPe5j-6idqQf",
        "outputId": "52aad211-6da1-4fd5-c73e-48057f72e5c9",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
              "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
              "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
              "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
              "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
              "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
              "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
              "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
              "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
              "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
              "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
              "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
              "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
              "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
              "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]], dtype=uint8)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Look at one of the instances\n",
        "x_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yY7AIlo9dqQg",
        "outputId": "8c004b42-589e-4bca-dd62-820fbfb5ed1d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(28, 28)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We can also look at the dimensionalities of the instances\n",
        "x_train[0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypDAnLMldqQg"
      },
      "source": [
        "Depending on your version of Keras, you may see a message showing that you are `Using TensorFlow backend`.\n",
        "\n",
        "From results above, we know that the images are stored in `28 x 28 ndarray` objects.\n",
        "\n",
        "To feed the images to a [convolutional neural network (CNN)](http://cs231n.github.io/convolutional-networks/) we transform the dataframe to four dimensions. This can be done using numpys reshape method. We will also transform the data into floats and normalize it.\n",
        "\n",
        "__NOTE 1__: For most part of deep learning, we can only handle floats or integers.\n",
        "\n",
        "__NOTE 2__: Can you guess why we want to normalize the data here?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUpKVqPndqQg"
      },
      "outputs": [],
      "source": [
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGj-kdBcdqQh"
      },
      "source": [
        "We can understand the first part of above code block, where we set the datatype as `float` and normalize the value to `[0,1]` range. But what does the second part do?\n",
        "\n",
        "Actually, neural networks treat data as tensors (hence the name TensorFlow) - and tensors are multi-dimensional vectors. In this case, we convert the images to add a value `1` as the last dimension of the tensors. Particularly in image classification (which is what we are doing here), the value of `1` indicates one color channel (i.e. __grayscale images__) - if the images are colored, we can use a color channel of `3`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocLkxlMFdqQh",
        "outputId": "16b4ddf7-b028-4b34-9139-09ee8ba52c9c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(28, 28, 1)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_train[0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCMLs8hLdqQh"
      },
      "source": [
        "We will also transform our labels into an [one-hot encoding](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f) using the `to_categorical()` method from Keras.\n",
        "\n",
        "__NOTE__: in data mining, OHE is not required - but in most cases of deep learning, we will perform OHE on the categorical target. The reason is by default neural nets treat all values as continuous (`float`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "LCqkDaGNdqQh"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPgqYxn8dqQh",
        "outputId": "1180a6cd-4bee-4cc4-aeb5-8c80496982e1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# let's look at our encoded target\n",
        "y_train[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WAzg99rdqQi"
      },
      "source": [
        "### YOUR TURN HERE - ANSWER THE QUESTION\n",
        "Q: What digit does the instance `y_train[0]` correspond to?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de2kSEZ9dqQi"
      },
      "source": [
        "5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvJ009XBdqQi"
      },
      "source": [
        "# Creating a sequential model in Keras\n",
        "The easiest way of creating a model in Keras is by using the sequential API, which lets you stack one layer after the other. The problem with the sequential API is that it doesn‚Äôt allow models to have multiple inputs or outputs, which are needed for some problems.\n",
        "\n",
        "Nevertheless, the sequential API is a perfect choice for most problems.\n",
        "\n",
        "To create a CNN we only need to create a `Sequential` object and use the `add` function to add layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "2BhHlsqIdqQi"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters=32, kernel_size=(5,5), activation='relu', input_shape=x_train.shape[1:]))\n",
        "model.add(Conv2D(filters=32, kernel_size=(5,5), activation='relu'))\n",
        "model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(rate=0.25))\n",
        "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(rate=0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(rate=0.5))\n",
        "model.add(Dense(10, activation='softmax'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7PBtE95dqQi"
      },
      "source": [
        "## Dissecting the NN\n",
        "\n",
        "Now you see we used five types of layers from Keras, namely `Conv2D`, `MaxPool2D`, `Dense`, `Flatten` and `Dropout`. Let's briefly look at what they do, respectively.\n",
        "\n",
        "### Conv2D and Maxpool2D\n",
        "\n",
        "These two are a pair. Convolution (`Conv2D`) in general means we take the 5x5x3 filter and slide it over the complete image and along the way take the dot product between the filter and chunks of the input image.\n",
        "\n",
        "Original Image and Sliding Window\n",
        "<img src= 'https://cdn-images-1.medium.com/max/800/1*FUEkm0JghT3ab8P7p9c5Qg.png' />\n",
        "\n",
        "Convolution\n",
        "<img src= 'https://cdn-images-1.medium.com/max/800/1*3sfzVenrdS5MWGsmSCbx3A.png' />\n",
        "<img src = 'https://cdn-images-1.medium.com/max/800/1*mcBbGiV8ne9NhF3SlpjAsA.png' />\n",
        "\n",
        "You may wonder how do we convert a section of image (via sliding window) to a certain number - that is the magic of pooling. The most popular pooling technique is Maxpooling (`Maxpool2D`).\n",
        "\n",
        "(Max-) Pooling\n",
        "<img src = 'https://cdn-images-1.medium.com/max/800/1*jU_Mp73fXzh9_ffvtnbrDQ.png' />\n",
        "\n",
        "If you wish to know more regarding Convolution and Pooling, [this article](https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/) or [this article](https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/) can be good starting point.\n",
        "\n",
        "Another point worthing noting here is the activation function. Activation function decides, whether a neuron should be __activated__ (output a `True` or `1` value) or not (output a `False` or `0` value) by calculating weighted sum and further adding bias with it. The purpose of the activation function is to introduce __non-linearity__ into the output of a neuron. In this tutorial we used the __Rectified Linear Unit (ReLU)__ activation function in the `conv2D` layers - given any value $x$, if $x > 0$ then output $x$; otherwise output $0$.\n",
        "\n",
        "<img src = 'https://cdn-images-1.medium.com/max/800/1*DfMRHwxY1gyyDmrIAd-gjQ.png' />\n",
        "\n",
        "We use a `softmax` activation to get the final output of the NN. Generally, we use __softmax__ activation instead of __sigmoid__ with the cross-entropy loss because softmax activation distributes the probability throughout each output node. If the problem is a binary classification, using sigmoid is same as softmax. For multi-class classification use `softmax` with `cross-entropy`.\n",
        "\n",
        "\n",
        "<img serc = 'https://cdn-images-1.medium.com/max/800/1*670CdxchunD-yAuUWdI7Bw.png' />\n",
        "\n",
        "More discussions regarding ReLU and other activation function can be found [here](https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0).\n",
        "\n",
        "### Flatten and Dense\n",
        "\n",
        "After maxpooling, we have pooled feature maps from images. The flatten layer, as the name suggests, flatten our pooled feature map into a column like in the image below.\n",
        "\n",
        "<img src = 'https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/73_blog_image_1.png' />\n",
        "\n",
        "A dense layer represents a matrix vector multiplication. (assuming your batch size is 1) The values in the matrix are the trainable parameters which get updated during backpropagation.\n",
        "\n",
        "So you get a m dimensional vector as output. A dense layer thus is used to change the dimensions of your vector. Mathematically speaking, it applies a rotation, scaling, translation transform to your vector.\n",
        "\n",
        "### Dropout\n",
        "\n",
        "A dropout layer is used for regularization where you randomly set some of the dimensions of your input vector to be zero with probability `ùëòùëíùëíùëù_ùëùùëüùëúùëè`. A dropout layer does not have any trainable parameters (i.e. nothing gets updated during backward pass of backpropagation). To ensure that expected sum of vectors fed to this layer remains the same if no dropout was applied, the remaining dimensions which are not set to zero are scaled by $\\frac{1}{ùëòùëíùëíùëù\\_ùëùùëüùëúùëè}$.\n",
        "\n",
        "### IMPORTANT\n",
        "Remember in the beginning we said that Keras is a high-level API that builds on the backend like TF? You must wonder why we need such an API - the answer is Keras makes our life easier. For instance, a similar (and much __simpler__) NN would be written in TF like below:\n",
        "```python\n",
        "def cnn_model_fn(features, labels, mode):\n",
        "  \"\"\"Model function for CNN.\"\"\"\n",
        "  # Input Layer\n",
        "  input_layer = tf.reshape(features[\"x\"], [-1, 28, 28, 1])\n",
        "\n",
        "  # Convolutional Layer #1\n",
        "  conv1 = tf.layers.conv2d(\n",
        "      inputs=input_layer,\n",
        "      filters=32,\n",
        "      kernel_size=[5, 5],\n",
        "      padding=\"same\",\n",
        "      activation=tf.nn.relu)\n",
        "\n",
        "  # Pooling Layer #1\n",
        "  pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
        "\n",
        "  # Convolutional Layer #2 and Pooling Layer #2\n",
        "  conv2 = tf.layers.conv2d(\n",
        "      inputs=pool1,\n",
        "      filters=64,\n",
        "      kernel_size=[5, 5],\n",
        "      padding=\"same\",\n",
        "      activation=tf.nn.relu)\n",
        "  pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
        "\n",
        "  # Dense Layer\n",
        "  pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
        "  dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
        "  dropout = tf.layers.dropout(\n",
        "      inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
        "\n",
        "  # Logits Layer\n",
        "  logits = tf.layers.dense(inputs=dropout, units=10)\n",
        "\n",
        "  predictions = {\n",
        "      # Generate predictions (for PREDICT and EVAL mode)\n",
        "      \"classes\": tf.argmax(input=logits, axis=1),\n",
        "      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
        "      # `logging_hook`.\n",
        "      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
        "  }\n",
        "\n",
        "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
        "\n",
        "  # Calculate Loss (for both TRAIN and EVAL modes)\n",
        "  loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
        "\n",
        "  # Configure the Training Op (for TRAIN mode)\n",
        "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
        "    train_op = optimizer.minimize(\n",
        "        loss=loss,\n",
        "        global_step=tf.train.get_global_step())\n",
        "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
        "\n",
        "  # Add evaluation metrics (for EVAL mode)\n",
        "  eval_metric_ops = {\n",
        "      \"accuracy\": tf.metrics.accuracy(\n",
        "          labels=labels, predictions=predictions[\"classes\"])\n",
        "  }\n",
        "  return tf.estimator.EstimatorSpec(\n",
        "      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
        "```\n",
        "\n",
        "### Returning to Our Sequential Model\n",
        "The code above first of creates a `Sequential` object and adds a few convolutional, maxpooling and dropout layers. It then flattens the output and passes it two a last dense and dropout layer before passing it to our output layer. If you aren‚Äôt confident build a CNN check out [this great tutorial](http://cs231n.github.io/convolutional-networks/).\n",
        "\n",
        "The sequential API also supports another syntax where the layers are passed to the constructor directly.\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(filters=32, kernel_size=(5,5), activation='relu', input_shape=x_train.shape[1:]),\n",
        "    Conv2D(filters=32, kernel_size=(5,5), activation='relu'),\n",
        "    MaxPool2D(pool_size=(2, 2)),\n",
        "    Dropout(rate=0.25),\n",
        "    Conv2D(filters=64, kernel_size=(3,3), activation='relu'),\n",
        "    Conv2D(filters=64, kernel_size=(3,3), activation='relu'),\n",
        "    MaxPool2D(pool_size=(2, 2)),\n",
        "    Dropout(rate=0.25),\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(rate=0.5),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8JrCcAudqQm"
      },
      "source": [
        "# Creating a model with the Functional API\n",
        "Alternatively, the functional API allows you to create the same models but offers you more flexibility at the cost of simplicity and readability.\n",
        "\n",
        "The functional API allows you to create models that have a lot more __flexibility__ as you can easily define models where layers connect to more than just the previous and next layers. In fact, you can connect layers to (literally) any other layer. As a result, creating __complex network__s such as siamese networks and residual networks become possible. The functional API is also useful in [transfer learning](https://machinelearningmastery.com/transfer-learning-for-deep-learning/).\n",
        "\n",
        "It can be used with multiple input and output layers as well as shared layers, which enables you to build really complex network structures.\n",
        "\n",
        "When using the functional API we always need to pass the previous layer to the current layer. It also requires the use of an input layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ev5StzHadqQm"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "# from keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout, Input\n",
        "\n",
        "inputs = Input(shape=x_train.shape[1:])\n",
        "\n",
        "x = Conv2D(filters=32, kernel_size=(5,5), activation='relu')(inputs)\n",
        "x = Conv2D(filters=32, kernel_size=(5,5), activation='relu')(x)\n",
        "x = MaxPool2D(pool_size=(2, 2))(x)\n",
        "x = Dropout(rate=0.25)(x)\n",
        "\n",
        "x = Conv2D(filters=64, kernel_size=(3,3), activation='relu')(x)\n",
        "x = Conv2D(filters=64, kernel_size=(3,3), activation='relu')(x)\n",
        "x = MaxPool2D(pool_size=(2, 2))(x)\n",
        "x = Dropout(rate=0.25)(x)\n",
        "\n",
        "x = Flatten()(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dropout(rate=0.5)(x)\n",
        "predictions = Dense(10, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tseVMI0mdqQm"
      },
      "source": [
        "__NOTE__: as a rule of thumb, if I am training a preliminary/baseline model, I will use the sequential API; if I am training a model for finetuning into a final model, I will use the functional API.\n",
        "\n",
        "# Compile a model\n",
        "Before we can start training our model we need to configure the learning process. For this, we need to specify an __optimizer__, a __loss function__ and optionally some __metrics__ like _accuracy_.\n",
        "\n",
        "The [loss function](https://www.youtube.com/watch?v=IVVVjBSk9N0) is a measure on how good our model is at achieving the given objective. [Here](https://keras.io/losses/) is a list of all loss functions supported by Keras.\n",
        "\n",
        "An [optimizer](https://towardsdatascience.com/a-quick-guide-to-neural-network-optimizers-with-applications-in-keras-e4635dd1cca4) is used to minimize the loss(objective) function by updating the weights using the gradients. [Here](https://keras.io/optimizers/) is a list of all optimizers supported by Keras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rh9GtPspdqQm"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wu68WDMUdqQn",
        "outputId": "2373003f-ef1e-4750-8062-fb5c0196d419"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 24, 24, 32)        832       \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 20, 20, 32)        25632     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 10, 10, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 10, 10, 32)        0         \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 8, 8, 64)          18496     \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 6, 6, 64)          36928     \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 3, 3, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 3, 3, 64)          0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 576)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 256)               147712    \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 232,170\n",
            "Trainable params: 232,170\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# We should always check if the model architecture fits what we want\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNALkHqidqQn"
      },
      "source": [
        "### YOUR TURN HERE - ANSWER THE QUESTION\n",
        "Use your own words, along with some research on the `categorical_entropy`, explain why we use it, rather than any other loss function supported by Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlC22Y9tdqQn"
      },
      "source": [
        "We use categorical cross-entropy because of its suitability for classification problems, gradient efficiency, and compatibility with softmax activation. It also measures the dissimilarity between prediceted class probabilities and true class labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BzPtR0WdqQn"
      },
      "source": [
        "### YOUR TURN HERE - ANSWER THE QUESTION\n",
        "Explain what is the difference between a loss function and an evaluation metric."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4M-veqadqQn"
      },
      "source": [
        "Loss function is used during training to optimize the model's parameters by measuring the error between the predicted and actual values. On the other hand, evaluation metrics are used after training to assess the overall performance of the model and determine how well it will solve the specific problem that we have"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMeEKa2idqQn"
      },
      "source": [
        "# Augmenting Image data\n",
        "Augmentation is an optional step of creating more data from existing ones. As you know, we are always in demand for more labelled training data - that is in particular true with NN/DL since we need more data to train the network.\n",
        "\n",
        "For images you can to little transformations like rotating the image, zooming into the image, adding noise and many more.\n",
        "\n",
        "This helps to make the model more robust and solves the problem of having not enough data. Keras has a method called ImageDataGenerator which can be used for augmenting images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "OATLcqVYdqQn"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "  rotation_range=10,\n",
        "  zoom_range=0.1,\n",
        "  width_shift_range=0.1,\n",
        "  height_shift_range=0.1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCguQciAdqQo"
      },
      "source": [
        "This `ImageDataGenerator` will create new images that have been rotated, zoomed in or out, and shifted in width and height.\n",
        "\n",
        "# Fit a model\n",
        "Now that we defined and compiled our model it‚Äôs ready for training. To train a model we would normally use the `fit` method but because we are using a datagenerator we will use `fit_generator` and pass it our generator, X data, y data as well as the [number of epochs and the batch size](https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9). We will also pass it a _validation set_ so we can monitor the __loss__ and __accuracy__ on both sets as well as `steps_per_epoch` which is required when using a generator and is just set to the length of the training set divided by the `batch_size`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmcDAVLkdqQo",
        "outputId": "b0296c7c-4d1e-4559-c4af-532339016b57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "937/937 [==============================] - ETA: 0s - batch: 468.0000 - size: 63.9658 - loss: 0.3965 - acc: 0.8718"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates = self.state_updates\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "937/937 [==============================] - 252s 268ms/step - batch: 468.0000 - size: 63.9658 - loss: 0.3965 - acc: 0.8718 - val_loss: 0.0452 - val_acc: 0.9850\n",
            "Epoch 2/5\n",
            "937/937 [==============================] - 250s 266ms/step - batch: 468.0000 - size: 63.9658 - loss: 0.1281 - acc: 0.9621 - val_loss: 0.0317 - val_acc: 0.9898\n",
            "Epoch 3/5\n",
            "937/937 [==============================] - 247s 263ms/step - batch: 468.0000 - size: 63.9658 - loss: 0.0965 - acc: 0.9724 - val_loss: 0.0238 - val_acc: 0.9917\n",
            "Epoch 4/5\n",
            "937/937 [==============================] - 248s 265ms/step - batch: 468.0000 - size: 63.9658 - loss: 0.0791 - acc: 0.9772 - val_loss: 0.0197 - val_acc: 0.9937\n",
            "Epoch 5/5\n",
            "937/937 [==============================] - 236s 252ms/step - batch: 468.0000 - size: 63.9658 - loss: 0.0700 - acc: 0.9796 - val_loss: 0.0213 - val_acc: 0.9930\n"
          ]
        }
      ],
      "source": [
        "epochs = 5\n",
        "batch_size = 64\n",
        "history = model.fit(datagen.flow(x_train, y_train, batch_size=batch_size), epochs=epochs,\n",
        "                              validation_data=(x_test, y_test), steps_per_epoch=x_train.shape[0]//batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoPcsDuDdqQo"
      },
      "source": [
        "The training history showed above contain rich information that we can use. For instance, it can provide important information regarding overfitting of a model. There are a few rules that we can use to determine if a NN is overfitted.\n",
        "\n",
        "__Primary Rule__: if the validation evaluation metric (e.g. `val_acc`) is not improving over epochs, then the NN is likely overfitted.\n",
        "\n",
        "__Seconday Rule__: if the training evaluation metric (e.g. `acc`) is constantly significantly higher than the validation evaluation metric (e.g. `val_acc`), then the NN is likely overfitted.\n",
        "\n",
        "If a NN overfits, you can apply steps like _early stopping_ in training - [this](https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/) would be a good intro for _early stopping_.\n",
        "\n",
        "To better monitor the training history, we can visualize our training and testing accuracy and loss for each epoch so we can get intuition about the performance of our model. The accuracy and loss over epochs are saved in the history variable we got whilst training and we will use Matplotlib to visualize this data.\n",
        "\n",
        "We first visualize the __evaluation metric__ (_accuracy_) between training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqIBYSxYdqQo"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "plt.plot(history.history['acc'], label='training accuracy')\n",
        "plt.plot(history.history['val_acc'], label='testing accuracy')\n",
        "plt.title('Accuracy')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(loc='lower right')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fL6paS7idqQo"
      },
      "source": [
        "Then we can visualize the loss between training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeEDMIr-dqQo"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['loss'], label='training loss')\n",
        "plt.plot(history.history['val_loss'], label='testing loss')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbHYD3ZgdqQo"
      },
      "source": [
        "From above graphs, if you observe _accuracy_ is not improving, or _loss_ is not decreasing, on the __validation set__, the NN is likely overfitting.\n",
        "\n",
        "In the graphs above we can see that our model isn‚Äôt overfitting as well as that we could train more epochs because the validation loss is still decreasing. You can also use some built-in tool like [TensorBoard](https://fizzylogic.nl/2017/05/08/monitor-progress-of-your-keras-based-neural-network-using-tensorboard/) for monitoring the training process.\n",
        "\n",
        "That is all for this tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPxXOrtydqQp"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this tutorial, you are introduced to a high-level deep learning API, namely Keras. Keras can help build NNs in a easy way. It also provides a lot of built-in functions and methods help evaluate and monitor the training process. Even though we did not talk about it, Keras can also help with the _deployment_ of a trained/evaluated NN.\n",
        "\n",
        "If you would like to know more regarding Keras, please refer to [this article](https://www.pyimagesearch.com/2018/09/10/keras-tutorial-how-to-get-started-with-keras-deep-learning-and-python/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5b1fBOcdqQp"
      },
      "source": [
        "# Exercise/Assignment\n",
        "\n",
        "Please complete [this article on Datacamp](https://www.datacamp.com/community/tutorials/deep-learning-python) to further understanding Keras. Unlike the image data we used in the tutorial, this article focuses on traditional tabular data. You are __REQUIRED__ to provide screenshots of all code snippets need completion (a total of __15__, with __RESULTS__) as your submission. Please paste all screenshots into a __SINGLE__ document and submit it in **Google Classroom**, along with this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YZzvQLFkdqQp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.4.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}