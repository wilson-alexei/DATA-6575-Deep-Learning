{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANiapfwTGHuI"
      },
      "source": [
        "# Part 1: Simple Guide to Hyper Parameter Tuning in Neural Networks\n",
        "\n",
        "To ensure the performane of neural networks, we cannot directly use them _out-of-the-box_.\n",
        "\n",
        "In this tutorial, a step-by-step walkthrough on hyperparameter optimization in your neural networks is provided."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN7GlCswGlHX"
      },
      "source": [
        "## What are we doing here?\n",
        "\n",
        "In this tutorial, we will be optimizing a neural network and performing hyperparameter tuning in order to obtain a high-performing model on the [Beale function](https://en.wikipedia.org/wiki/Test_functions_for_optimization) — one of many test functions commonly used for studying the effectiveness of various optimization techniques.\n",
        "\n",
        "This analysis can be reused for any function, and you should be able to try this out yourself by applying this to your course project.\n",
        "\n",
        "Personally, I find that optimizing a neural network can be incredibly frustrating unless you have a __clear and well-defined__ procedure to follow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx6FmGpOHNiX"
      },
      "source": [
        "## What is Beale’s Function?\n",
        "\n",
        "Neural networks are fairly commonplace now in industry and research, but an embarrassingly large proportion of people, even experienced data scientists are unable to work with them well enough to be able to produce high-performing networks that are capable of outperforming most other algorithms.\n",
        "\n",
        "When applied mathematicians develop a new optimization algorithm, one thing they like to do is test it on a __test function__, which is sometimes called an _artificial landscape_. These artificial landscapes help us find a way of comparing the performance of various algorithms in terms of their:\n",
        "- Convergence (how fast they reach the answer)\n",
        "- Precision (how close do they approximate the exact answer)\n",
        "- Robustness (do they perform well for all functions or just a small subset)\n",
        "- General performance (e.g. computational complexity)\n",
        "\n",
        "From just scrolling down the Wikipedia article on optimization test functions, you can see that some of the functions are pretty nasty. Many of them have been chosen as they highlight specific issues that can plague other optimization algorithms. Thus, we select a relatively innocuous-looking function called the __Beale function__.\n",
        "The Beale function looks like this:\n",
        "![the Beale Function](https://miro.medium.com/max/1400/0*b6VbjuQQJVXxd_rE.jpg)\n",
        "\n",
        "This function does not look particularly terrifying, right? The reason this is a test function is that it assesses how well the optimization algorithms perform when in flat regions with very shallow gradients. In these cases, it is particularly difficult for gradient-based optimization procedures to reach any kind of minimum, as they are unable to learn effectively. This landscape is analogous to the __loss surface__ of a neural network. When training a neural network, the goal is to find the __global minimum__ on the loss surface by performing some form of optimization — typically [_stochastic gradient descent_](https://en.wikipedia.org/wiki/Stochastic_gradient_descent). In other words, we want to reach the central part in the Beale function surface since it has lower loss value.\n",
        "\n",
        "In other words, there are two preliminary questions you need to answer when considering optmizing your neural networks:\n",
        "- What is the __loss function__?\n",
        "- How to __minimize__ the output of your loss function?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aKiziPdIYFI"
      },
      "source": [
        "### Preliminary: A Basic Beale Function\n",
        "\n",
        "Let's build a basic Beale function using python. To keep things simple, we assume the data only contains _two-dimensions_ (`x` and `y`). The mathematic function of Beale function is:\n",
        "\n",
        "$$ f_{Beale}(x, y) = (1.5 - x + x \\times y)^2 + (2.25 - x + (x \\times y)^2)^2 + (2.625 - x + (x \\times y)^3)^2 $$\n",
        "\n",
        "Let's implement above function in Python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aa1D_8dZFzus"
      },
      "source": [
        "# define Beale's function which we want to minimize\n",
        "def objective(vec): # assume `vec` is a two-dimensional vector\n",
        "    x,y = vec[0], vec[1] # `x` is the first dimension of `vec` and `y` is the second\n",
        "    return (1.5 - x + x*y)**2 + (2.25 - x + x*y**2)**2 + (2.625 - x + x*y**3)**2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAVAqvAhOriF"
      },
      "source": [
        "We then set some function boundaries since we have ballpark estimates for where the minimum is in this case (from our plot), as well as a step size for our grid mesh.\n",
        "\n",
        "Basically, we are setting up the artifical surface in a $ 10 \\times 10 $ area, and for every step in the search, we can only move a step of $1$ in the area."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUXwogB3OgWo"
      },
      "source": [
        "# function boundaries\n",
        "xmin, xmax, xstep = -5, 5, 1\n",
        "ymin, ymax, ystep = -5, 5, 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxtH0IZsPQFT"
      },
      "source": [
        "We then make a mesh grid of points based on this information and are ready to find the minimum. First we can create a point as a random starting point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKz3EEpAPL4M"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "x1, y1 = np.meshgrid(np.arange(xmin, xmax + xstep, xstep), np.arange(ymin, ymax + ystep, ystep))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQUhTGJqPkp8"
      },
      "source": [
        "Now we make a (terrible) initial guess."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPqScCFNPden"
      },
      "source": [
        "# initial guess\n",
        "x0 = [4., 4.]\n",
        "f0 = objective(x0)\n",
        "print (f0) # this is the output from our Beale function"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67_yYqkNPusm"
      },
      "source": [
        "Given the value you can see that the initial value (_loss_) looks terrible.\n",
        "\n",
        "We then use the `scipy.optimize` function and see what answer pops out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kn4OzG9TPsrQ"
      },
      "source": [
        "from scipy.optimize import minimize\n",
        "\n",
        "bnds = ((xmin, xmax), (ymin, ymax)) # define the boundaries of search\n",
        "minimum = minimize(objective, x0, bounds=bnds) # search the area  to minimize the function\n",
        "print(minimum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeY6RtOMQL7z"
      },
      "source": [
        "It looks like the answer is `(3, 0.5)`, and, if you plug these values into the equation you do indeed find that this is the minimum."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZuQHRY3QEEz"
      },
      "source": [
        "objective([3, 0.5]) # this should equal to 0 (minimum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_kXaHPqQc8x"
      },
      "source": [
        "## Optimization in Neural Networks\n",
        "A neural network can be defined as a framework that combines inputs and tries to guess the output.\n",
        "\n",
        "If we are lucky enough to have some results, called “the ground truth” (aka. __labels__), to compare the outputs produced by the network, we can calculate the error (difference between $\\hat{y}$ and $y$). So the network guesses, calculates some error function, guesses again, trying to minimize this error, guesses again, until the error does not go down any more. This is __optimization__.\n",
        "\n",
        "In neural networks, the most commonly used optimization algorithms are flavors of GD (gradient descent). The objective function used in gradient descent is the loss function which we want to minimize.\n",
        "This tutorial will lean heavily on Keras now, so here is a brief Keras refresher."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Orvkjd4pRB_Z"
      },
      "source": [
        "### A Keras Refresher\n",
        "Keras is a Python library for deep learning that can run on top of both Theano or TensorFlow, two powerful Python libraries for fast numerical computing created and released by Facebook and Google, respectively.\n",
        "\n",
        "Keras was developed to make developing deep learning models as fast and easy as possible for research and practical applications. It runs on Python 2.7 or 3.5 and can seamlessly execute on GPUs and CPUs.\n",
        "\n",
        "Keras is built on the idea of a model. At its core we have a sequence of layers called the Sequential model which is a linear stack of layers. Keras also provides the functional API, a way to define complex models, such as multi-output models, directed acyclic graphs, or models with shared layers.\n",
        "\n",
        "We can summarize the construction of deep learning models in Keras using the Sequential model as follows:\n",
        "1. Define your model: create a Sequential model and add layers.\n",
        "2. Compile your model: specify loss function and optimizers and call the .compile() function.\n",
        "3. Fit your model: train the model on data by calling the .fit() function.\n",
        "4. Make predictions: use the model to generate predictions on new data by calling functions such as .evaluate() or .predict().\n",
        "\n",
        "If you recall from your machine learnig knowledge with `scikit-learn`, most of the modeling also contain step 1, 3, and 4.\n",
        "\n",
        "You may be asking yourself — how can you examine the performance of the model as it is running? This is a good question, and the answer to that is using __callbacks__."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESpJfuuFRpDu"
      },
      "source": [
        "### Callbacks: taking a peek into our model while it’s training\n",
        "You can look at what is happening in various stages of your model by using `callbacks`.\n",
        "\n",
        "A `callback` is a set of functions to be applied at given stages of the training procedure. You can use `callbacks` to get a view on any intermediate states and statistics of the model during any phase in training.\n",
        "\n",
        "You can pass a list of `callbacks` (as the keyword argument callbacks) to the `.fit()` method of the Sequential or Model classes. The relevant methods of the callbacks will then be called at each stage of the training.\n",
        "\n",
        "- A callback function you are already familiar with is `keras.callbacks.History()`. This is automatically included in `.fit()`.\n",
        "- Another very useful one is `keras.callbacks.ModelCheckpoint` which saves the model with its weights at a certain point in the training. This can prove useful if your model is running for a __long time__ and a system failure happens. Not all is lost then. It's a good practice to save the model weights only _when an improvement is observed as measured by the `acc`, for example_.\n",
        "- `keras.callbacks.EarlyStopping` stops the training when a monitored quantity has __stopped improving__.\n",
        "- `keras.callbacks.LearningRateScheduler` will change the learning rate during training.\n",
        "\n",
        "We will apply some callbacks later. For full documentation on callbacks see [here](https://keras.io/callbacks/)\n",
        ".\n",
        "The first thing we must do is import a lot of different functions in order to make our lives easier.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G38BcUS6QRct"
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras import datasets\n",
        "\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.callbacks import History\n",
        "\n",
        "from tensorflow.keras import losses\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "print(\"Your tensorflow version is: \", tf.__version__)\n",
        "print(\"Your keras version is: \", tf.keras.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGeWnaZQSxPg"
      },
      "source": [
        "Another additional step you can do if you want your network to work using random numbers but for the result to be __repeatable__ is to use a `random seed`. This basically produces the same sequence of numbers each time, although they are still _pseudorandom_ (these are a great way for comparing models and also testing for reproducibility).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hh2dCYmFSdQY"
      },
      "source": [
        "\n",
        "# fix random seed for reproducibility\n",
        "np.random.seed(2020)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUrRwtJPS8tO"
      },
      "source": [
        "### Step 1 — Deciding on the network topology (not really considered optimization but is obviously very important)\n",
        "\n",
        "We will use the MNIST dataset which consists of grayscale images of handwritten digits (0–9) whose dimension is 28x28 pixels. Each pixel is 8 bits (representing different value in a gray scale) so its value ranges from 0 to 255.\n",
        "\n",
        "Obtaining the dataset is very easy since there is a function for it built-in to `Keras`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xO6bwt0WS57n"
      },
      "source": [
        "mnist = keras.datasets.mnist\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "x_train.shape, y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCixIMcJTa-F"
      },
      "source": [
        "Our output for our X and Y data is `(60000, 28, 28)` and `(60000,1)` respectively.\n",
        "\n",
        "It is always a good suggestion to print some of the data to check the values (and the data type if necessary).\n",
        "\n",
        "\n",
        "We can check the training data by looking at one image of each of the digits to make sure that none of them are missing from our data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMgT8UCSTToj"
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "for i in range(10):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(x_train[i], cmap=plt.cm.binary)\n",
        "    plt.xlabel(y_train[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4FYFlKsTyuE"
      },
      "source": [
        "The last check is for the dimensions of the training and test sets, which can be done relatively easily:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaGRqcpNTiMF"
      },
      "source": [
        "\n",
        "print(f'We have {x_train.shape[0]} train samples')\n",
        "print(f'We have {x_test.shape[0]} test samples')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-EZ1wgOT8jX"
      },
      "source": [
        "### Preprocessing the data\n",
        "To run our NN we need to preprocess the data (these steps can be performed interchangeably):\n",
        "\n",
        "1. We need to make the 2D image arrays into 1D (__flatten__ them). We can either perform this by using array reshaping with `numpy.reshape()` or the keras' method for this: a layer called `keras.layers.Flatten` which transforms the format of the images from a 2d-array (of $28 \\times 28$ pixels), to a 1D-array of $28 \\times 28 = 784$ pixels.\n",
        "2. We need to normalize the pixel values (give them values between 0 and 1) using the following transformation:\n",
        "$$ x' = \\frac{x - x_{min}}{x_{max} - x_{min}} $$\n",
        "\n",
        "In our case, the minimum is 0 and the maximum is 255, so the formula becomes simply $𝑥'=𝑥/255$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBEpc0_YT0rf"
      },
      "source": [
        "# normalize the data\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# reshape the data into 1D vectors\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "# Check the column length\n",
        "x_train.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_T137hNwVHKP"
      },
      "source": [
        "x_train[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iiqlDDUVFYQ"
      },
      "source": [
        "We can see now our features are in 784-element vectors, and the values are between `[0, 1]`.\n",
        "\n",
        "We now want to one-hot encode our target (label)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rXMzj2wUtxP"
      },
      "source": [
        "# Convert class vectors to binary class matrices\n",
        "y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.np_utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "y_train[:2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4GQdZu_VqMJ"
      },
      "source": [
        "Now we have prepared the data for training our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KntCAxIBVt7V"
      },
      "source": [
        "### Step 2 — Adjusting the `learning rate`\n",
        "\n",
        "One of the most common optimization algorithms is __Stochastic Gradient Descent__ (SGD). The hyperparameters that can be optimized in SGD are `learning rate`, `momentum`, `decay` and `nesterov`.\n",
        "\n",
        "- `Learning rate` controls the weight at the end of each batch, and\n",
        "- `momentum` controls how much to let the previous update influence the current weight update\n",
        "- `Decay` indicates the learning rate decay over each update, and\n",
        "- `nesterov` takes the value “True” or “False” depending on if we want to apply Nesterov momentum.\n",
        "\n",
        "Typical values for those hyperparameters are `lr=0.01, decay=1e-6, momentum=0.9`, and `nesterov=True`.\n",
        "\n",
        "The learning rate hyperparameter goes into the optimizer function which we will see below. Keras has a default learning rate scheduler in the `SGD` optimizer that decreases the learning rate during the stochastic gradient descent optimization algorithm. The learning rate is decreased according to this formula:\n",
        "$$ lr=lr \\times 1/(1+decay \\times epoch) $$\n",
        "\n",
        "Selecting a good learning rate would definitely impact your model performance. See image below.\n",
        "![learning rates and model performance](https://miro.medium.com/max/1400/1*Dbr6vHLUeUk-fzDf9VWbUw.png)\n",
        "\n",
        "Let’s implement a learning rate adaptation schedule in `Keras`. We'll start with `SGD` and a learning rate value of `0.1`. We will then train the model for `60` epochs and set the decay argument to `0.0016` (`0.1/60`). We also include a momentum value of `0.8` since that seems to work well when using an adaptive learning rate.\n",
        "\n",
        "The reason behind the declining learning rate is that after each round of training, we want the search to be more specific. It is analoguous to tuning the radio seeking a station - at first you can turn the knob fast, then when you get closer you want to turn the knob more carefully in case you missed the station."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEu4fr4YVbyp"
      },
      "source": [
        "epochs=60\n",
        "learning_rate = 0.1\n",
        "decay_rate = learning_rate / epochs # 0.016\n",
        "momentum = 0.8\n",
        "\n",
        "sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1dqgseXXfX_"
      },
      "source": [
        "Next, we build the architecture of the neural network, for illustration purposes, we will build a simple MLP network:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xm0Uc-CjXaYK"
      },
      "source": [
        "# build the model\n",
        "input_dim = x_train.shape[1]\n",
        "\n",
        "lr_model = Sequential()\n",
        "lr_model.add(Dense(64, activation=tf.nn.relu, kernel_initializer='uniform',\n",
        "                input_dim = input_dim))\n",
        "lr_model.add(Dropout(0.1))\n",
        "lr_model.add(Dense(64, kernel_initializer='uniform', activation=tf.nn.relu))\n",
        "lr_model.add(Dense(num_classes, kernel_initializer='uniform', activation=tf.nn.softmax))\n",
        "\n",
        "# compile the model\n",
        "lr_model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYmgfiPaYlHA"
      },
      "source": [
        "We can now run the model and see how well it performs. This took ~20 minutes on Colab and may be faster or slower on yours depending on what machine you are using.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OLbAdKsXp7L"
      },
      "source": [
        "%%time\n",
        "# Fit the model\n",
        "batch_size = int(input_dim/100)\n",
        "\n",
        "lr_model_history = lr_model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiDi4kIsYtDh"
      },
      "source": [
        "After it has finished running, we can plot the accuracy and loss function as a function of epochs for the training and test sets to see how the network performed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJvEecCbYnbU"
      },
      "source": [
        "# Plot the loss function\n",
        "print(\"loss function over training\")\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
        "ax.plot(np.sqrt(lr_model_history.history['loss']), 'r', label='train')\n",
        "ax.plot(np.sqrt(lr_model_history.history['val_loss']), 'b' ,label='val')\n",
        "ax.set_xlabel(r'Epoch', fontsize=20)\n",
        "ax.set_ylabel(r'Loss', fontsize=20)\n",
        "ax.legend()\n",
        "ax.tick_params(labelsize=20);\n",
        "\n",
        "# Plot the accuracy\n",
        "print(\"accuracy over training\")\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
        "ax.plot(np.sqrt(lr_model_history.history['acc']), 'r', label='train')\n",
        "ax.plot(np.sqrt(lr_model_history.history['val_acc']), 'b' ,label='val')\n",
        "ax.set_xlabel(r'Epoch', fontsize=20)\n",
        "ax.set_ylabel(r'Accuracy', fontsize=20)\n",
        "ax.legend()\n",
        "ax.tick_params(labelsize=20);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0A2ezs_CdmCw"
      },
      "source": [
        "We can see from above that we did not overfit the model.\n",
        "\n",
        "We will now look at applying a customized learning rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3oeDqtIdrFs"
      },
      "source": [
        "### Apply a custom learning rate change using LearningRateScheduler\n",
        "\n",
        "Write a function that performs the exponential learning rate decay as indicated by the following formula:\n",
        "\n",
        "$$ 𝑙𝑟=𝑙𝑟_{0} \\times 𝑒^{−𝑘𝑡} $$\n",
        "In which, $𝑙𝑟_{0}$ is the learning rate of the previous epoch, $k$ is the decay rate, and $t$ is the number of epochs.\n",
        "\n",
        "This is very similar to before so I will do this in one code block and describe the differences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oz1yOw2CdigA"
      },
      "source": [
        "epochs = 60\n",
        "learning_rate = 0.1 # initial learning rate\n",
        "decay_rate = 0.1\n",
        "momentum = 0.8\n",
        "\n",
        "# define the optimizer function\n",
        "sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
        "\n",
        "input_dim = x_train.shape[1]\n",
        "num_classes = 10\n",
        "batch_size = 196\n",
        "\n",
        "# build the model\n",
        "exponential_decay_model = Sequential()\n",
        "exponential_decay_model.add(Dense(64, activation=tf.nn.relu, kernel_initializer='uniform', input_dim = input_dim))\n",
        "exponential_decay_model.add(Dropout(0.1))\n",
        "exponential_decay_model.add(Dense(64, kernel_initializer='uniform', activation=tf.nn.relu))\n",
        "exponential_decay_model.add(Dense(num_classes, kernel_initializer='uniform', activation=tf.nn.softmax))\n",
        "\n",
        "# compile the model\n",
        "exponential_decay_model.compile(loss='categorical_crossentropy',\n",
        "                                optimizer=sgd,\n",
        "                                metrics=['acc'])\n",
        "\n",
        "# define the learning rate change\n",
        "def exp_decay(epoch):\n",
        "    lrate = learning_rate * np.exp(-decay_rate*epoch)\n",
        "    return lrate\n",
        "\n",
        "# learning schedule callback\n",
        "loss_history = History()\n",
        "lr_rate = LearningRateScheduler(exp_decay)\n",
        "callbacks_list = [loss_history, lr_rate]\n",
        "\n",
        "# you invoke the LearningRateScheduler during the .fit() phase\n",
        "exponential_decay_model_history = exponential_decay_model.fit(x_train, y_train,\n",
        "                                    batch_size=batch_size,\n",
        "                                    epochs=epochs,\n",
        "                                    callbacks=callbacks_list,\n",
        "                                    verbose=1,\n",
        "                                    validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toqn7fzmedhD"
      },
      "source": [
        "We see here that the only thing that has changed here is the presence of the `exp_decay` function that we defined, and its use in the `LearningRateScheduler` function. Notice we also chose to add them as the callbacks to our model this time.\n",
        "\n",
        "We can now plot the learning rate and loss functions as functions of the number of epochs. The learning rate plot is incredibly smooth as it follows our predefined exponentially decaying function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iAbAKKoeWSb"
      },
      "source": [
        "# Plot the loss function\n",
        "print(\"(above) loss function over training\")\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
        "ax.plot(np.sqrt(exponential_decay_model_history.history['loss']), 'r', label='train')\n",
        "ax.plot(np.sqrt(exponential_decay_model_history.history['val_loss']), 'b' ,label='val')\n",
        "ax.set_xlabel(r'Epoch', fontsize=20)\n",
        "ax.set_ylabel(r'Loss', fontsize=20)\n",
        "ax.legend()\n",
        "ax.tick_params(labelsize=20);\n",
        "\n",
        "# Plot the accuracy\n",
        "print(\"(below) accuracy over training\")\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
        "ax.plot(np.sqrt(exponential_decay_model_history.history['acc']), 'r', label='train')\n",
        "ax.plot(np.sqrt(exponential_decay_model_history.history['val_acc']), 'b' ,label='val')\n",
        "ax.set_xlabel(r'Epoch', fontsize=20)\n",
        "ax.set_ylabel(r'Accuracy', fontsize=20)\n",
        "ax.legend()\n",
        "ax.tick_params(labelsize=20);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSz5tlcBe1C_"
      },
      "source": [
        "You can observe that both the accuracy curve and the loss curve are smoother now as compared to before.\n",
        "\n",
        "This shows you that developing a learning rate scheduler can be a helpful way to improve neural network performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuGQTAcbfL8A"
      },
      "source": [
        "### Step 3 — Choosing an `optimizer` and a `loss function`\n",
        "When constructing a model and using it to make our predictions, for example, to assign label scores to images (“cat”, “plane”, etc), we want to measure our success or failure by defining a “loss” function (or objective function). The goal of optimization is to efficiently calculate the parameters/weights that __minimize this loss function__. `keras` provides various types of loss functions.\n",
        "\n",
        "Sometimes the “loss” function measures the “distance”. We can define this “distance” between two data points in various ways suitable to the problem or dataset. The distance used depends on the data type and the specific problem that is being tackled. For example, in natural language processing (which analyses textual data), the _Hamming distance_ is much more common to use.\n",
        "\n",
        "#### Distance\n",
        "- Euclidean\n",
        "- Manhattan\n",
        "- others such as Hamming which measures distances between strings, for example. The Hamming distance of “carolin” and “cathrin” is 3.\n",
        "\n",
        "#### Loss functions\n",
        "- MSE/RMSE (for regression)\n",
        "- categorical cross-entropy (for classification)\n",
        "- binary cross entropy (for (binary) classification)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKewxj5peyzw"
      },
      "source": [
        "# build the model\n",
        "input_dim = x_train.shape[1]\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation=tf.nn.relu, kernel_initializer='uniform',\n",
        "                input_dim = input_dim)) # fully-connected layer with 64 hidden units\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(64, kernel_initializer='uniform', activation=tf.nn.relu))\n",
        "model.add(Dense(num_classes, kernel_initializer='uniform', activation=tf.nn.softmax))\n",
        "\n",
        "# defining the parameters for RMSprop (I used the keras defaults here)\n",
        "rms = RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=rms,\n",
        "              metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoYNG-BZf130"
      },
      "source": [
        "### Step 4 — Deciding on the batch size and number of epochs\n",
        "The __batch size__ defines the number of samples that will be propagated through the network, in __one iteration__.\n",
        "\n",
        "For instance, let’s say you have 1000 training samples and you want to set up a `batch_size` equal to 100. The algorithm takes the _first_ 100 samples (from $1^{st}$ to $100^{th}$) from the training dataset and trains the network. Next, it takes the _second_ 100 samples (from $101^{st}$ to $200^{th}$) and trains the network again. We can keep doing this procedure until we have propagated all samples through the network (10 iterations).\n",
        "\n",
        "Advantages of using a smaller batch size:\n",
        "- It requires __less memory__. Since you train the network using fewer samples, the overall training procedure requires less memory. That’s especially important if you are not able to fit the whole dataset in your machine’s memory.\n",
        "- Typically networks __train faster__ with mini-batches. That’s because we update the weights after each propagation.\n",
        "Disadvantages of using a smaller batch size:\n",
        "- The smaller the batch, the less accurate the estimate of the gradient will be.\n",
        "\n",
        "The number of epochs is a hyperparameter that defines the number times that the learning algorithm will work through the entire training dataset.\n",
        "\n",
        "One epoch means that each sample in the training dataset has had an opportunity to update the internal model parameters (e.g. aforementioned 100 iterations consist one epoch). An epoch is comprised of one or more batches.\n",
        "\n",
        "There are no hard and fast rules for selecting batch sizes or the number of epochs, and there is no guarantee that increasing the number of epochs provides a better result than a lesser number."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Auounq0IfyWh"
      },
      "source": [
        "%%time\n",
        "batch_size = input_dim\n",
        "epochs = 60\n",
        "\n",
        "model_history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
        "ax.plot(np.sqrt(model_history.history['acc']), 'r', label='train_acc')\n",
        "ax.plot(np.sqrt(model_history.history['val_acc']), 'b' ,label='val_acc')\n",
        "ax.set_xlabel(r'Epoch', fontsize=20)\n",
        "ax.set_ylabel(r'Accuracy', fontsize=20)\n",
        "ax.legend()\n",
        "ax.tick_params(labelsize=20)\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
        "ax.plot(np.sqrt(model_history.history['loss']), 'r', label='train')\n",
        "ax.plot(np.sqrt(model_history.history['val_loss']), 'b' ,label='val')\n",
        "ax.set_xlabel(r'Epoch', fontsize=20)\n",
        "ax.set_ylabel(r'Loss', fontsize=20)\n",
        "ax.legend()\n",
        "ax.tick_params(labelsize=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZg4Qy7ahAxB"
      },
      "source": [
        "You can see from above, as we increase the `batch_size` while holding the number of epochs steady, we reduce the training time from ~20 minutes to ~22 seconds.\n",
        "\n",
        "But you can also observe that both the accuracy and loss curves are \"bumpier\" - meaning the performance of the model has __declined__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvchm8rbg30B"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}