{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "1tYx6Ia3v2RE"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3SOqHtpiICn"
      },
      "source": [
        "# Part 2: Grid Search for Hyper Parameter Tuning in Neural Networks\n",
        "\n",
        "To ensure the performane of neural networks, we cannot directly use them _out-of-the-box_.\n",
        "\n",
        "In the first part of the tutorial, we discussed why do we need to tune the hyper parameters in neural networks. However, tuning these hyper parameters in a _ad hoc_ fashion is both inefficient and error-prone. Thus, we need to find a more systematic way for tuning the hyper parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP4OlmHwivM8"
      },
      "source": [
        "### Overview\n",
        "In this post, I want to show you both how you can use the `scikit-learn`'s grid search capability and give you a suite of examples that you can copy-and-paste into your own project as a starting point.\n",
        "\n",
        "Below is a list of the topics we are going to cover:\n",
        "\n",
        "- How to use Keras models in scikit-learn.\n",
        "- How to use grid search in scikit-learn.\n",
        "- How to tune batch size and training epochs.\n",
        "- How to tune optimization algorithms.\n",
        "- How to tune learning rate and momentum.\n",
        "- How to tune network weight initialization.\n",
        "- How to tune activation functions.\n",
        "- How to tune dropout regularization.\n",
        "- How to tune the number of neurons in the hidden layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9UlIKunjDuA"
      },
      "source": [
        "## How to Use Keras Models in scikit-learn\n",
        "Keras models can be used in scikit-learn by wrapping them with the KerasClassifier or KerasRegressor class.\n",
        "\n",
        "To use these wrappers you must define a function that creates and returns your Keras sequential model, then pass this function to the `build_fn` argument when constructing the `KerasClassifier` class.\n",
        "\n",
        "For example:\n",
        "``` python\n",
        "def create_model():\n",
        "\t...\n",
        "\treturn model\n",
        "\n",
        "model = KerasClassifier(build_fn=create_model)\n",
        "```\n",
        "\n",
        "The constructor for the `KerasClassifier` class can take default arguments that are passed on to the calls to `model.fit()`, such as the number of epochs and the [batch size](https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/).\n",
        "\n",
        "For example:\n",
        "\n",
        "``` python\n",
        "def create_model():\n",
        "\t...\n",
        "\treturn model\n",
        "\n",
        "model = KerasClassifier(build_fn=create_model, epochs=10)\n",
        "```\n",
        "\n",
        "The constructor for the `KerasClassifier` class can also take new arguments that can be passed to your custom `create_model()` function. These new arguments must also be defined in the signature of your `create_model()` function with default parameters.\n",
        "\n",
        "For example:\n",
        "``` python\n",
        "def create_model(dropout_rate=0.0):\n",
        "\t...\n",
        "\treturn model\n",
        "\n",
        "model = KerasClassifier(build_fn=create_model, dropout_rate=0.2)\n",
        "```\n",
        "\n",
        "You can learn more about the [scikit-learn wrapper in Keras API documentation](https://faroit.com/keras-docs/2.0.0/scikit-learn-api/#wrappers-for-the-scikit-learn-api)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBtj9E8Pjzd2"
      },
      "source": [
        "## (Review on) How to Use Grid Search in scikit-learn\n",
        "Grid search is a model hyperparameter optimization technique.\n",
        "\n",
        "In scikit-learn this technique is provided in the `GridSearchCV` class.\n",
        "\n",
        "When constructing this class you must provide a __dictionary of hyperparameters__ to evaluate in the param_grid argument. This is a map of the model parameter name and an array of values to try.\n",
        "\n",
        "By default, `accuracy` is the score that is optimized, but other scores can be specified in the `score` argument of the `GridSearchCV` constructor.\n",
        "\n",
        "By default, the grid search will only use one thread. By setting the `n_jobs` argument in the `GridSearchCV` constructor to -1, the process will use all cores on your machine. Depending on your Keras backend, this may interfere with the main neural network training process.\n",
        "\n",
        "The `GridSearchCV` process will then construct and evaluate one model for each combination of parameters. Cross validation is used to evaluate each individual model and the default of _3-fold cross validation_ is used, although this can be overridden by specifying the cv argument to the `GridSearchCV` constructor.\n",
        "\n",
        "Below is an example of defining a simple grid search:\n",
        "\n",
        "``` python\n",
        "param_grid = dict(epochs=[10,20,30])\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(X, Y)\n",
        "```\n",
        "Once completed, you can access the outcome of the grid search in the result object returned from `grid.fit()`. The `best_score_` attribute provides access to the best score observed during the optimization procedure and the `best_params_` describes the combination of parameters that achieved the best results.\n",
        "\n",
        "You can learn more about the [GridSearchCV class in the scikit-learn API documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tn0xX6K_lXFI"
      },
      "source": [
        "### Problem Description\n",
        "Now that we know how to use Keras models with scikit-learn and how to use grid search in scikit-learn, let’s look at a bunch of examples.\n",
        "\n",
        "All examples will be demonstrated on a small standard machine learning dataset called the Pima Indians onset of diabetes classification dataset. This is a small dataset with all numerical attributes that is easy to work with. The dataset is available [here](https://raw.githubusercontent.com/DrJieTao/diabetesprediction/master/diabetes.csv).\n",
        "\n",
        "As we proceed through the examples in this post, we will aggregate the best parameters. This is not the best way to grid search because parameters can interact, but it is good for demonstration purposes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9imVbcTsmAdF"
      },
      "source": [
        "### Import Packages\n",
        "\n",
        "Import required packages for this tutorial below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cMbxiG_mLwd"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from tensorflow.keras.constraints import max_norm\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "np.random.seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTYsNWXEmWo5"
      },
      "source": [
        "### Define the `create_model()` Function\n",
        "\n",
        "As discussed above, in order to link `keras` with `scikit-learn`, we need to create a function that will initialize and train the model for us. Let's do that now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXd7qDlmmoIS"
      },
      "source": [
        "# Function to create model, required for KerasClassifier\n",
        "def create_model(optimizer = 'sgd', learn_rate=0.001, momentum=0.0, init_mode='uniform', activation = 'sigmoid'):\n",
        "\t# create model\n",
        "\tmodel = Sequential() # a very simple MLP model\n",
        "\tmodel.add(Dense(12, input_dim=8, activation='relu'))\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) # since this is a classification problem, accuracy is fine\n",
        "\treturn model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wFSZaaWl2Kl"
      },
      "source": [
        "### Load the dataset\n",
        "\n",
        "We will load th data as a `pandas` dataframe and take the values out as a `numpy` array. In the realm of deep learning, we prefer `numpy` over `pandas` for efficiency reasons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xozhmh5Eh3LT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "67790798-8046-4e8c-f85a-4d38d2fc93d3"
      },
      "source": [
        "data_url = 'https://raw.githubusercontent.com/DrJieTao/diabetesprediction/master/diabetes.csv'\n",
        "\n",
        "# load dataset\n",
        "pima = pd.read_csv(data_url,  header=0)\n",
        "pima.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
              "0            6      148             72             35        0  33.6   \n",
              "1            1       85             66             29        0  26.6   \n",
              "2            8      183             64              0        0  23.3   \n",
              "3            1       89             66             23       94  28.1   \n",
              "4            0      137             40             35      168  43.1   \n",
              "\n",
              "   DiabetesPedigreeFunction  Age  Outcome  \n",
              "0                     0.627   50        1  \n",
              "1                     0.351   31        0  \n",
              "2                     0.672   32        1  \n",
              "3                     0.167   21        0  \n",
              "4                     2.288   33        1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dee4a5da-0c32-484a-b59e-55eae9afee73\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dee4a5da-0c32-484a-b59e-55eae9afee73')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dee4a5da-0c32-484a-b59e-55eae9afee73 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dee4a5da-0c32-484a-b59e-55eae9afee73');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CH27D7gZnPWp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "813a32f8-6527-42e7-d039-8880719fc756"
      },
      "source": [
        "pima_data = pima.values\n",
        "pima_data[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[6.000e+00, 1.480e+02, 7.200e+01, 3.500e+01, 0.000e+00, 3.360e+01,\n",
              "        6.270e-01, 5.000e+01, 1.000e+00],\n",
              "       [1.000e+00, 8.500e+01, 6.600e+01, 2.900e+01, 0.000e+00, 2.660e+01,\n",
              "        3.510e-01, 3.100e+01, 0.000e+00],\n",
              "       [8.000e+00, 1.830e+02, 6.400e+01, 0.000e+00, 0.000e+00, 2.330e+01,\n",
              "        6.720e-01, 3.200e+01, 1.000e+00],\n",
              "       [1.000e+00, 8.900e+01, 6.600e+01, 2.300e+01, 9.400e+01, 2.810e+01,\n",
              "        1.670e-01, 2.100e+01, 0.000e+00],\n",
              "       [0.000e+00, 1.370e+02, 4.000e+01, 3.500e+01, 1.680e+02, 4.310e+01,\n",
              "        2.288e+00, 3.300e+01, 1.000e+00]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMX0KI2ynw32"
      },
      "source": [
        "# split into input (X) and output (Y) variables\n",
        "X = pima_data[:,:-1]\n",
        "y = pima_data[:,-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_btHjyTBvi7v"
      },
      "source": [
        "We can then create a model without any specified hyper parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1H7_4l9vZ4n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcb78ec7-606b-48a7-df92-7400003fcfde"
      },
      "source": [
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-b6e8d49c6ca2>:2: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  model = KerasClassifier(build_fn=create_model, verbose=0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Pge07wxvsbG"
      },
      "source": [
        "Then we can define the parameter set for the `GridSearchCV`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHlcav1Ovp2u"
      },
      "source": [
        "# define the grid search parameters\n",
        "batch_size = [10, 20, 40, 60, 80, 100]\n",
        "epochs = [10, 50, 100]\n",
        "param_grid = dict(batch_size=batch_size, epochs=epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tYx6Ia3v2RE"
      },
      "source": [
        "We can now initialize the `GridSearchCV` class as `grid` with above parameter set, all the cores in the system, and 3-fold CV.\n",
        "\n",
        "#### Note on Parallelizing Grid Search\n",
        "All examples are configured to use parallelism (n_jobs=-1).\n",
        "\n",
        "If you get an error like the one below:\n",
        "``` python\n",
        "INFO (tf.gof.compilelock): Waiting for existing lock by process '55614' (I am process '55613')\n",
        "INFO (tf.gof.compilelock): To manually release the lock, delete ...\n",
        "```\n",
        "Kill the process and change the code to not perform the grid search in parallel, set `n_jobs=-1`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_I6Arjav1Xv"
      },
      "source": [
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJWLCayWwql8"
      },
      "source": [
        "Now we can print out the results to see which combination of _batch size_ and _number of epoches_ gives us the best performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUXVWWLlwLvh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "502c7115-cae6-413d-8a73-aa01c7ae8d36"
      },
      "source": [
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best: 0.713542 using {'batch_size': 10, 'epochs': 100}\n",
            "0.625000 (0.031412) with: {'batch_size': 10, 'epochs': 10}\n",
            "0.651042 (0.032106) with: {'batch_size': 10, 'epochs': 50}\n",
            "0.713542 (0.001841) with: {'batch_size': 10, 'epochs': 100}\n",
            "0.571615 (0.023939) with: {'batch_size': 20, 'epochs': 10}\n",
            "0.690104 (0.009207) with: {'batch_size': 20, 'epochs': 50}\n",
            "0.679688 (0.020915) with: {'batch_size': 20, 'epochs': 100}\n",
            "0.557292 (0.039365) with: {'batch_size': 40, 'epochs': 10}\n",
            "0.657552 (0.041010) with: {'batch_size': 40, 'epochs': 50}\n",
            "0.630208 (0.003683) with: {'batch_size': 40, 'epochs': 100}\n",
            "0.580729 (0.043420) with: {'batch_size': 60, 'epochs': 10}\n",
            "0.647135 (0.028587) with: {'batch_size': 60, 'epochs': 50}\n",
            "0.651042 (0.004872) with: {'batch_size': 60, 'epochs': 100}\n",
            "0.471354 (0.119437) with: {'batch_size': 80, 'epochs': 10}\n",
            "0.622396 (0.052634) with: {'batch_size': 80, 'epochs': 50}\n",
            "0.669271 (0.027866) with: {'batch_size': 80, 'epochs': 100}\n",
            "0.518229 (0.041626) with: {'batch_size': 100, 'epochs': 10}\n",
            "0.618490 (0.061955) with: {'batch_size': 100, 'epochs': 50}\n",
            "0.631510 (0.008027) with: {'batch_size': 100, 'epochs': 100}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOraEwAXxA-u"
      },
      "source": [
        "We can see that the batch size of 20 and 100 epochs achieved the best result of about 72% accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckNEzFqfzPS6"
      },
      "source": [
        "## How to Tune the Training Optimization Algorithm\n",
        "Keras offers a suite of different state-of-the-art optimization algorithms.\n",
        "\n",
        "In this example, we tune the optimization algorithm used to train the network, each with default parameters.\n",
        "\n",
        "This is an odd example, because often you will choose one approach a priori and instead focus on tuning its parameters on your problem (e.g. see the next example).\n",
        "\n",
        "In the first part of the tutorial, we used the popular `SGD` optimizer. Here we will evaluate the [suite of optimization algorithms supported by the Keras API](http://keras.io/optimizers/).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivERnyIyzi7N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfd877af-66e7-4b4a-8b48-8a5f6560a063"
      },
      "source": [
        "# create model with the tuned hyper parameters\n",
        "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=20, verbose=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-c879c3095a98>:2: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=20, verbose=0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhVAQntizoMY"
      },
      "source": [
        "# define the grid search parameters\n",
        "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
        "param_grid = dict(optimizer=optimizer)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gZNJUJpzt_p"
      },
      "source": [
        "Now we can print out the results to see which _optimizer_ gives us the best performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKyBEQs2zqgj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67a7dd5c-0fb9-41bf-c1ab-9da9bed4a9a3"
      },
      "source": [
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best: 0.700521 using {'optimizer': 'Adagrad'}\n",
            "0.687500 (0.009568) with: {'optimizer': 'SGD'}\n",
            "0.690104 (0.019488) with: {'optimizer': 'RMSprop'}\n",
            "0.700521 (0.012890) with: {'optimizer': 'Adagrad'}\n",
            "0.688802 (0.022402) with: {'optimizer': 'Adadelta'}\n",
            "0.692708 (0.014731) with: {'optimizer': 'Adam'}\n",
            "0.688802 (0.003683) with: {'optimizer': 'Adamax'}\n",
            "0.678385 (0.003683) with: {'optimizer': 'Nadam'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XS3mVTd5AEw_"
      },
      "source": [
        "So we can see the `Adam` optimizer gives us the best results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b8LZ1zgxKZr"
      },
      "source": [
        "## How to Tune Learning Rate and Momentum\n",
        "It is common to pre-select an optimization algorithm to train your network and tune its parameters.\n",
        "\n",
        "By far the most common optimization algorithm is plain old Stochastic Gradient Descent (`SGD`, as we discussed in the previous part) because it is so well understood. In this example, we will look at optimizing the SGD learning rate and momentum parameters.\n",
        "\n",
        "Learning rate controls how much to update the weight at the end of each batch and the momentum controls how much to let the previous update influence the current weight update.\n",
        "\n",
        "We will try a suite of small standard learning rates and a momentum values from 0.2 to 0.8 in steps of 0.2, as well as 0.9 (because it can be a popular value in practice).\n",
        "\n",
        "Generally, it is a good idea to also include the number of epochs in an optimization like this as there is a dependency between the amount of learning per batch (learning rate), the number of updates per epoch (batch size) and the number of epochs.\n",
        "\n",
        "We will use the fine-tuned hyper parameters from above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNqrc1pAw8qg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8426840-a828-46c0-ec11-0abd698d8287"
      },
      "source": [
        "# create model with the tuned hyper parameters\n",
        "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=20, verbose=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-c879c3095a98>:2: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=20, verbose=0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhDjaLk4xopI"
      },
      "source": [
        "# define the grid search parameters and initialize the `grid` object\n",
        "learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
        "momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
        "param_grid = dict(learn_rate=learn_rate, momentum=momentum)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLW5ZlaIz8Zm"
      },
      "source": [
        "Now we can print out the results to see which combination of _learning rate_ and _momentum_ gives us the best performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEXy8wRvxw4B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff664a00-2d09-4e16-e6ab-555ef325201b"
      },
      "source": [
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best: 0.721354 using {'learn_rate': 0.001, 'momentum': 0.8}\n",
            "0.695312 (0.025516) with: {'learn_rate': 0.001, 'momentum': 0.0}\n",
            "0.694010 (0.027126) with: {'learn_rate': 0.001, 'momentum': 0.2}\n",
            "0.699219 (0.027251) with: {'learn_rate': 0.001, 'momentum': 0.4}\n",
            "0.675781 (0.022326) with: {'learn_rate': 0.001, 'momentum': 0.6}\n",
            "0.721354 (0.015073) with: {'learn_rate': 0.001, 'momentum': 0.8}\n",
            "0.667969 (0.033603) with: {'learn_rate': 0.001, 'momentum': 0.9}\n",
            "0.686198 (0.018136) with: {'learn_rate': 0.01, 'momentum': 0.0}\n",
            "0.680990 (0.006639) with: {'learn_rate': 0.01, 'momentum': 0.2}\n",
            "0.694010 (0.015733) with: {'learn_rate': 0.01, 'momentum': 0.4}\n",
            "0.632812 (0.046983) with: {'learn_rate': 0.01, 'momentum': 0.6}\n",
            "0.700521 (0.020752) with: {'learn_rate': 0.01, 'momentum': 0.8}\n",
            "0.674479 (0.018414) with: {'learn_rate': 0.01, 'momentum': 0.9}\n",
            "0.691406 (0.017758) with: {'learn_rate': 0.1, 'momentum': 0.0}\n",
            "0.682292 (0.030314) with: {'learn_rate': 0.1, 'momentum': 0.2}\n",
            "0.670573 (0.024360) with: {'learn_rate': 0.1, 'momentum': 0.4}\n",
            "0.675781 (0.017758) with: {'learn_rate': 0.1, 'momentum': 0.6}\n",
            "0.678385 (0.043537) with: {'learn_rate': 0.1, 'momentum': 0.8}\n",
            "0.682292 (0.016053) with: {'learn_rate': 0.1, 'momentum': 0.9}\n",
            "0.703125 (0.022326) with: {'learn_rate': 0.2, 'momentum': 0.0}\n",
            "0.696615 (0.021236) with: {'learn_rate': 0.2, 'momentum': 0.2}\n",
            "0.707031 (0.011049) with: {'learn_rate': 0.2, 'momentum': 0.4}\n",
            "0.673177 (0.032578) with: {'learn_rate': 0.2, 'momentum': 0.6}\n",
            "0.683594 (0.025315) with: {'learn_rate': 0.2, 'momentum': 0.8}\n",
            "0.684896 (0.021236) with: {'learn_rate': 0.2, 'momentum': 0.9}\n",
            "0.664062 (0.009568) with: {'learn_rate': 0.3, 'momentum': 0.0}\n",
            "0.699219 (0.005524) with: {'learn_rate': 0.3, 'momentum': 0.2}\n",
            "0.697917 (0.017566) with: {'learn_rate': 0.3, 'momentum': 0.4}\n",
            "0.687500 (0.022999) with: {'learn_rate': 0.3, 'momentum': 0.6}\n",
            "0.683594 (0.011049) with: {'learn_rate': 0.3, 'momentum': 0.8}\n",
            "0.673177 (0.046694) with: {'learn_rate': 0.3, 'momentum': 0.9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuKngWs6ARNT"
      },
      "source": [
        "So we can see that the combinatio of `learning_rate = 0.001` and the `momentum = 0.4` returns the best result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NJooEmFAvxP"
      },
      "source": [
        "### How to Tune Network Weight Initialization\n",
        "Neural network weight initialization used to be simple: use small random values.\n",
        "\n",
        "Now there is a suite of different techniques to choose from. Keras provides a [laundry list](http://keras.io/initializations/).\n",
        "\n",
        "In this example, we will look at tuning the selection of network weight initialization by evaluating all of the available techniques.\n",
        "\n",
        "We will use the same weight initialization method on each layer. Ideally, it may be better to use different weight initialization schemes according to the activation function used on each layer. In this part, we use rectifier for the hidden layer. We use sigmoid for the output layer because the predictions are binary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkLT7e28AC37"
      },
      "source": [
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=20, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbPwx2yjS52t"
      },
      "source": [
        "# define the grid search parameters\n",
        "init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero',\n",
        "             'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
        "param_grid = dict(init_mode=init_mode)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "810RftwjS_NN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "7fefc0fe-2d40-4588-cf5a-74aa8f3e802c"
      },
      "source": [
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.695312 using {'init_mode': 'uniform'}\n",
            "0.695312 (0.011500) with: {'init_mode': 'uniform'}\n",
            "0.680990 (0.021236) with: {'init_mode': 'lecun_uniform'}\n",
            "0.682292 (0.016367) with: {'init_mode': 'normal'}\n",
            "0.695312 (0.016877) with: {'init_mode': 'zero'}\n",
            "0.670573 (0.019225) with: {'init_mode': 'glorot_normal'}\n",
            "0.692708 (0.022402) with: {'init_mode': 'glorot_uniform'}\n",
            "0.662760 (0.013279) with: {'init_mode': 'he_normal'}\n",
            "0.673177 (0.008027) with: {'init_mode': 'he_uniform'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXXcwuUbTvaG"
      },
      "source": [
        "We can see that the best results were achieved with a __uniform__ weight initialization scheme achieving a performance of about 72%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58-rozYwTyM3"
      },
      "source": [
        "### How to Tune the Neuron Activation Function\n",
        "The activation function controls the __non-linearity__ of individual neurons and when to fire.\n",
        "\n",
        "Generally, the rectifier activation function is the most popular, but it used to be the sigmoid and the tanh functions and these functions may still be more suitable for different problems.\n",
        "\n",
        "In this part, we will evaluate the suite of [different activation functions](http://keras.io/activations/) available in Keras. We will only use these functions in the hidden layer, as we require a __sigmoid__ activation function in the output for the binary classification problem.\n",
        "\n",
        "Generally, it is a good idea to prepare data to the range of the different transfer functions, which we will not do in this case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-U4jsXGTq5F"
      },
      "source": [
        "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOWJ1B70Uudd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3313af05-c084-4a36-8e92-29574898d790"
      },
      "source": [
        "activation = ['softmax', 'softplus', 'softsign', 'relu',\n",
        "              'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
        "param_grid = dict(activation=activation)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR! Session/line number was not unique in database. History logging moved to new session 59\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBcOh3RcWleh"
      },
      "source": [
        "Now we can print out the results to see which _activation function_ gives us the best performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huFawdVbUxnw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "41278510-3350-4ddb-c57d-f3ea88b3c535"
      },
      "source": [
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.705729 using {'activation': 'relu'}\n",
            "0.692708 (0.038976) with: {'activation': 'softmax'}\n",
            "0.688802 (0.038582) with: {'activation': 'softplus'}\n",
            "0.700521 (0.027126) with: {'activation': 'softsign'}\n",
            "0.705729 (0.028940) with: {'activation': 'relu'}\n",
            "0.701823 (0.027866) with: {'activation': 'tanh'}\n",
            "0.675781 (0.027805) with: {'activation': 'sigmoid'}\n",
            "0.692708 (0.004872) with: {'activation': 'hard_sigmoid'}\n",
            "0.697917 (0.016053) with: {'activation': 'linear'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZ9i9DOcZWfz"
      },
      "source": [
        "We can observe that the `relu` activation function returns the best result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlgxtSQXZdsX"
      },
      "source": [
        "### How to Tune Dropout Regularization\n",
        "In this part, we will look at tuning the [dropout rate for regularization](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/) in an effort to limit overfitting and improve the model’s ability to generalize.\n",
        "\n",
        "To get good results, dropout is best combined with a weight constraint such as the max norm constraint.\n",
        "\n",
        "For more on using dropout in deep learning models with Keras see [this post](http://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/).\n",
        "\n",
        "This involves fitting both the dropout percentage and the weight constraint. We will try dropout percentages between `0.0` and `0.9` (`1.0` does not make sense) and maxnorm weight constraint values between `0` and `5`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Th_HZYzEWtb3"
      },
      "source": [
        "# redefine the `create_model` function here\n",
        "def create_model(dropout_rate=0.0, weight_constraint=0):\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(12, input_dim=8, kernel_initializer='uniform',\n",
        "                 activation='linear', kernel_constraint=max_norm(weight_constraint)))\n",
        "\tmodel.add(Dropout(dropout_rate))\n",
        "\tmodel.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\treturn model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bTXF4owZ734"
      },
      "source": [
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7jZ8uLSZ96P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "0c4bef04-7785-4172-c272-4268e202c524"
      },
      "source": [
        "# define the grid search parameters\n",
        "weight_constraint = [1, 2, 3, 4, 5]\n",
        "dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "param_grid = dict(dropout_rate=dropout_rate, weight_constraint=weight_constraint)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eKj0bggikol"
      },
      "source": [
        "Now we can print out the results to see which _dropout regularization_ gives us the best performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quWzjk__aCbo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "1e533fcd-b584-4d66-f710-2d0c45f078a4"
      },
      "source": [
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.729167 using {'dropout_rate': 0.4, 'weight_constraint': 1}\n",
            "0.697917 (0.018136) with: {'dropout_rate': 0.0, 'weight_constraint': 1}\n",
            "0.697917 (0.020752) with: {'dropout_rate': 0.0, 'weight_constraint': 2}\n",
            "0.705729 (0.013279) with: {'dropout_rate': 0.0, 'weight_constraint': 3}\n",
            "0.708333 (0.023939) with: {'dropout_rate': 0.0, 'weight_constraint': 4}\n",
            "0.718750 (0.011500) with: {'dropout_rate': 0.0, 'weight_constraint': 5}\n",
            "0.705729 (0.008027) with: {'dropout_rate': 0.1, 'weight_constraint': 1}\n",
            "0.721354 (0.023510) with: {'dropout_rate': 0.1, 'weight_constraint': 2}\n",
            "0.723958 (0.027126) with: {'dropout_rate': 0.1, 'weight_constraint': 3}\n",
            "0.721354 (0.020505) with: {'dropout_rate': 0.1, 'weight_constraint': 4}\n",
            "0.703125 (0.011049) with: {'dropout_rate': 0.1, 'weight_constraint': 5}\n",
            "0.707031 (0.009568) with: {'dropout_rate': 0.2, 'weight_constraint': 1}\n",
            "0.700521 (0.010253) with: {'dropout_rate': 0.2, 'weight_constraint': 2}\n",
            "0.708333 (0.011201) with: {'dropout_rate': 0.2, 'weight_constraint': 3}\n",
            "0.692708 (0.022402) with: {'dropout_rate': 0.2, 'weight_constraint': 4}\n",
            "0.704427 (0.015073) with: {'dropout_rate': 0.2, 'weight_constraint': 5}\n",
            "0.716146 (0.021236) with: {'dropout_rate': 0.3, 'weight_constraint': 1}\n",
            "0.714844 (0.006379) with: {'dropout_rate': 0.3, 'weight_constraint': 2}\n",
            "0.718750 (0.031894) with: {'dropout_rate': 0.3, 'weight_constraint': 3}\n",
            "0.714844 (0.035943) with: {'dropout_rate': 0.3, 'weight_constraint': 4}\n",
            "0.708333 (0.012890) with: {'dropout_rate': 0.3, 'weight_constraint': 5}\n",
            "0.729167 (0.023510) with: {'dropout_rate': 0.4, 'weight_constraint': 1}\n",
            "0.713542 (0.030647) with: {'dropout_rate': 0.4, 'weight_constraint': 2}\n",
            "0.710938 (0.014616) with: {'dropout_rate': 0.4, 'weight_constraint': 3}\n",
            "0.692708 (0.021236) with: {'dropout_rate': 0.4, 'weight_constraint': 4}\n",
            "0.694010 (0.008027) with: {'dropout_rate': 0.4, 'weight_constraint': 5}\n",
            "0.710938 (0.019137) with: {'dropout_rate': 0.5, 'weight_constraint': 1}\n",
            "0.708333 (0.012890) with: {'dropout_rate': 0.5, 'weight_constraint': 2}\n",
            "0.705729 (0.006639) with: {'dropout_rate': 0.5, 'weight_constraint': 3}\n",
            "0.710938 (0.008438) with: {'dropout_rate': 0.5, 'weight_constraint': 4}\n",
            "0.701823 (0.003683) with: {'dropout_rate': 0.5, 'weight_constraint': 5}\n",
            "0.714844 (0.019918) with: {'dropout_rate': 0.6, 'weight_constraint': 1}\n",
            "0.716146 (0.021710) with: {'dropout_rate': 0.6, 'weight_constraint': 2}\n",
            "0.704427 (0.003683) with: {'dropout_rate': 0.6, 'weight_constraint': 3}\n",
            "0.717448 (0.030314) with: {'dropout_rate': 0.6, 'weight_constraint': 4}\n",
            "0.705729 (0.017566) with: {'dropout_rate': 0.6, 'weight_constraint': 5}\n",
            "0.705729 (0.007366) with: {'dropout_rate': 0.7, 'weight_constraint': 1}\n",
            "0.687500 (0.014616) with: {'dropout_rate': 0.7, 'weight_constraint': 2}\n",
            "0.701823 (0.007366) with: {'dropout_rate': 0.7, 'weight_constraint': 3}\n",
            "0.704427 (0.008027) with: {'dropout_rate': 0.7, 'weight_constraint': 4}\n",
            "0.686198 (0.034987) with: {'dropout_rate': 0.7, 'weight_constraint': 5}\n",
            "0.684896 (0.015733) with: {'dropout_rate': 0.8, 'weight_constraint': 1}\n",
            "0.691406 (0.020915) with: {'dropout_rate': 0.8, 'weight_constraint': 2}\n",
            "0.688802 (0.006639) with: {'dropout_rate': 0.8, 'weight_constraint': 3}\n",
            "0.691406 (0.005524) with: {'dropout_rate': 0.8, 'weight_constraint': 4}\n",
            "0.705729 (0.004872) with: {'dropout_rate': 0.8, 'weight_constraint': 5}\n",
            "0.674479 (0.011201) with: {'dropout_rate': 0.9, 'weight_constraint': 1}\n",
            "0.678385 (0.011201) with: {'dropout_rate': 0.9, 'weight_constraint': 2}\n",
            "0.666667 (0.018688) with: {'dropout_rate': 0.9, 'weight_constraint': 3}\n",
            "0.677083 (0.023073) with: {'dropout_rate': 0.9, 'weight_constraint': 4}\n",
            "0.675781 (0.013902) with: {'dropout_rate': 0.9, 'weight_constraint': 5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2-bsjh3itRQ"
      },
      "source": [
        "We can see that the dropout rate of `20%` and the maxnorm weight constraint of `1` resulted in the best accuracy of ~73%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQ10gbzKiysC"
      },
      "source": [
        "### How to Tune the Number of Neurons in the Hidden Layer\n",
        "The number of neurons in a layer is an important parameter to tune. Generally the number of neurons in a layer controls the representational capacity of the network, at least at that point in the topology.\n",
        "\n",
        "Also, generally, a large enough single layer network can approximate any other neural network, at least in theory. But if the input data is small, and the number of neurons is too large, the model may function unexpectedly.\n",
        "\n",
        "In this part, we will look at tuning the number of neurons in a single hidden layer. We will try values from 1 to 30 in steps of 5.\n",
        "\n",
        "A larger network requires more training and at least the batch size and number of epochs should ideally be optimized with the number of neurons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CG02wCicin-6"
      },
      "source": [
        "# Function to create model, required for KerasClassifier\n",
        "def create_model(neurons=1):\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(neurons, input_dim=8, kernel_initializer='uniform', activation='linear', kernel_constraint=max_norm(4)))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\treturn model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVfWq9aejY_j"
      },
      "source": [
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDP4F3v0jcqL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "dd997121-b6e1-4244-e01d-b17529e74295"
      },
      "source": [
        "# define the grid search parameters\n",
        "neurons = [1, 5, 10, 15, 20, 25, 30]\n",
        "param_grid = dict(neurons=neurons)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcEqxxOejvpB"
      },
      "source": [
        "Now we can print out the results to see what _number of neurons_ gives us the best performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAZ_wzEajgV7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "6625674e-7e9d-4e5b-a944-8a9fc9d079e2"
      },
      "source": [
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.722656 using {'neurons': 25}\n",
            "0.708333 (0.010253) with: {'neurons': 1}\n",
            "0.708333 (0.038582) with: {'neurons': 5}\n",
            "0.701823 (0.012890) with: {'neurons': 10}\n",
            "0.714844 (0.019918) with: {'neurons': 15}\n",
            "0.708333 (0.009744) with: {'neurons': 20}\n",
            "0.722656 (0.019918) with: {'neurons': 25}\n",
            "0.687500 (0.009568) with: {'neurons': 30}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKwFa1GZj3s7"
      },
      "source": [
        "We can see that the best results were achieved with a network with `25` neurons in the hidden layer with an accuracy of ~72%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yu75-LnakRZv"
      },
      "source": [
        "## Summary\n",
        "In this post, you discovered how you can tune the hyperparameters of your deep learning networks in Python using Keras and scikit-learn.\n",
        "\n",
        "Specifically, you learned:\n",
        "\n",
        "- How to wrap Keras models for use in scikit-learn and how to use grid search.\n",
        "- How to grid search a suite of different standard neural network parameters for Keras models.\n",
        "- How to design your own hyperparameter optimization experiments.\n",
        "\n",
        "This concludes both parts of the model optimization of neural networks - feel free to apply these concepts and techniques on your own project."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IcB37FlkQrR"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}