{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bjS88r5MRzF"
      },
      "source": [
        "# Deep Learning & Artificial Intelligence\n",
        "## Neural Networks Foundations, Chapter 2\n",
        "### Dr. Jie Tao, Fairfield University"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl4yYNlzN_X1"
      },
      "source": [
        "## Math behind Neural Networks\n",
        "\n",
        "Understanding how neural networks work often requires in-depth understanding of several mathematical concepts, which includes but not limited to:\n",
        "- tensors and their operations\n",
        "- differentiation\n",
        "- gradient descent\n",
        "- derivatives\n",
        "- computation graph\n",
        "- vectors and matrix\n",
        "\n",
        "Additionally, you also need to know the basics of:\n",
        "- (logistic) regression\n",
        "- (binary) classification\n",
        "- Broadcasting in Python, and basics of NumPy\n",
        "\n",
        "Which you already learned in your previous courses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFIB0-ZyRcwI"
      },
      "source": [
        "### What are Tensors?\n",
        "\n",
        "You must remember N-dimensional arrays in `NumPy`, as a matter of fact that is the __most popular__ data type we use in Deep Learning, and all mainstream machine learning models.\n",
        "\n",
        "**Fun fact**: TensorFlow gets its name because of tensors.\n",
        "\n",
        "But what are tensors?\n",
        "- Tensors are containers for data\n",
        "  - Almost always __numerical__ data\n",
        "- If you know matrix:\n",
        "  - A matrix is a `2D` tensor\n",
        "- So tensors are implementations of matrices\n",
        "  - With an arbitrary number of dimensions (e.g., `2`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIYYIGLpTK8r"
      },
      "source": [
        "### What are Scalars?\n",
        "\n",
        "Any number in a tensor is called a _scalar_.\n",
        "- Scalar, aka. scalar tensor, is _0-dimensional_\n",
        "- A `NumPy` `float32` or `float64` number is a __scalar__\n",
        "  - remember you can display the dimensions of a ``NumPy`` array using the `ndim` attribute?\n",
        "  - For a scalar, `ndim == 0`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-art6_zNV8B"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.array(42)\n",
        "assert x.ndim == 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3L8o58TQVQiM"
      },
      "source": [
        "### What are vectors?\n",
        "\n",
        "An array of numbers is called a _vector_.\n",
        "\n",
        "- A vector is 1-dimensional tensor\n",
        "- Note that a vector is usually a ``NumPy`` 1D array, not a __list__\n",
        "  - It's because arrays allow us to contain __different__ types on data in them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vEOB2SxUvfY"
      },
      "source": [
        "vec = np.array([1, 2, 3])\n",
        "assert vec.ndim == 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojWbIVejbaQF"
      },
      "source": [
        "### High-dimensional Tensors\n",
        "\n",
        "- By default, a tensor has a ``ndim >= 2``\n",
        "  - If it's a 2D tensor, it is a regular matrix\n",
        "  - If it's more than 2D, then it's called a _high-dimensional_ tensor\n",
        "  - For instance, usually we represent _one_ image as a 2D or 3D tensor:\n",
        "    - If it's 2D --> represent pixels\n",
        "    - If it's 3D --> represent pixels and the color channel\n",
        "  - In that sense, if you have _multiple_ images, then it will be a 3D/4D image\n",
        "    - The additional dimension represents the collection of all images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RODby3m7bXom",
        "outputId": "46e482eb-3f97-4300-dbd5-2bb06827a4e2"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "train_images.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5m2JCqLe2W3"
      },
      "source": [
        "Look at the results above:\n",
        "- `60000` means we have 60,000 images in the _training_ dataset\n",
        "- each image has `28` pixels along the horizontal and vertical dimensions, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcrgL2Dxf7wW"
      },
      "source": [
        "### Key Attributes of Tensors\n",
        "\n",
        "A tensor has three key features:\n",
        "- Rank: number of _main_ dimensions. We use _rank_ to determine the size of a tensor. For instance, for a `2D` tensor, rank == 2. We often use `ndim` for that;\n",
        "- Shape: show __all__ the dimensionalities of the tensor. We often use ``.shape`` for that.\n",
        "  - For instance, a regular `3D` tensor should show something like ``(60000, 28, 28)``.\n",
        "  - If you see something like ``(60000,)``, that means either this is a `1D` tensor, or the shapes in the 2D tensors (in each of the 60,000 are not uniform\n",
        "- Data Type: type of scalars in a tensor. We can use `dtype` to retrive them. Like said above, usually these are `float32` or `float64`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYEv_gRndiF2",
        "outputId": "7ca8aa36-82cd-4f0e-8000-1fb1c8e12ab1"
      },
      "source": [
        "# getting the rank of a tensor - 3D tensor so rank == 3\n",
        "train_images.ndim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nggO9xlnqh28",
        "outputId": "c58dcd07-00cf-461f-90af-e56a9ea07f1d"
      },
      "source": [
        "# Shape of tensor\n",
        "train_images.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mU2sGZEqyO6",
        "outputId": "90779e69-4494-4ddd-8769-483987a441b9"
      },
      "source": [
        "# data type - in this case they are integers\n",
        "train_images.dtype"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('uint8')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZG4MxE-zw-3t"
      },
      "source": [
        "### Real World Use of Tensors\n",
        "\n",
        "We typically map real world data into tensors like below:\n",
        "\n",
        "- Tabular data - `2D` tensors with shape as ``(sampels, features)``\n",
        "- Time-series or other sequential data - `3D` tensors with shape as ``(samples, timestep, features)``\n",
        "- Image data: `4D` tensors with shape as ``(samples, height, weight, channels)`` or ``(samples, channels, height, weight)``\n",
        "  - in the `mnist` example, since we are dealing with _grayscale_ data, `channel == 1` and is omitted\n",
        "- Video data: `5D` tensors with shape as ``(samples, frames, height, weight, channels)`` or ``(samples, frames, channels, height, weight)``\n",
        "  - Since videos are sequence of images, video data are combination of `image data` and `sequence data`.\n",
        "  - In that case `frames` are the same as `timesteps`\n",
        "\n",
        "Refer to your textbook for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olmkGV70rABt"
      },
      "source": [
        "### Manipulating Tensors in NumPy\n",
        "\n",
        "If you are using `Keras` or `TensorFlow` for deep learning, we usually prepare our data in `NumPy`.\n",
        "\n",
        "Fun fact: if you are using `PyTorch` then it's 50% `NumPy` and 50% `PyTorch`.\n",
        "\n",
        "The most important operation here is __slicing__. Refer to the [NumPy Official Docs](https://numpy.org/doc/stable/reference/arrays.indexing.html) if you need a refresher.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFW8zb3aq1rr",
        "outputId": "14512c2a-aa4d-4607-e052-870967033649"
      },
      "source": [
        "data_slice = train_images[:10]\n",
        "data_slice.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tensors and NumPy arrays\n",
        "\n",
        "You may have already discovered that tensors and `NumPy` arrays are very similar. In real world, we do use them interchangeably."
      ],
      "metadata": {
        "id": "AyKnGKcdb50V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "array_a = np.arange(6).astype(float)\n",
        "array_a, type(array_a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMNBbMaAcWED",
        "outputId": "cd7f2a2d-d03d-4810-b7c4-ee1950de20f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0., 1., 2., 3., 4., 5.]), numpy.ndarray)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### array to tensor\n",
        "tensor_a = torch.tensor(array_a)\n",
        "tensor_a, type(tensor_a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eS9iP3WkcsWH",
        "outputId": "28be5046-9f00-4a18-8902-6f57cb106318"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0., 1., 2., 3., 4., 5.], dtype=torch.float64), torch.Tensor)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### convert back to array\n",
        "array_a_rep = tensor_a.numpy()\n",
        "assert np.array_equal(array_a_rep, array_a) ### equal to the original array"
      ],
      "metadata": {
        "id": "RD7MtToUdDcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to talk more about tensor operations in Workshop 1. For now, let's just assume `NumPy` arrays are tensors, that is particularly true in `tensorflow`/`keras`."
      ],
      "metadata": {
        "id": "sXLg3cDkd5SP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BTOFabbs1F5"
      },
      "source": [
        "### The Notion of Batch\n",
        "\n",
        "Slicing is very important since we use that to create _batches_. In order to introduce the notion of batch, we need to review what is a _sample_:\n",
        "- A sample is a row in your data, aka an instance. For intance in `train_images` we have `60000` samples.\n",
        "- A batch is a slice of samples. In deep learning, we update the model parameters after walking through a batch.\n",
        "  - `batch_size` is an important hyperparameter that we use to tune our networks\n",
        "  - At the end of each batch, the model makes predictions and the _error_ is computed\n",
        "- A training dataset can be divided into one or more batchs, we will learn how it's divided when we discuss the idea of __gradient descent__.\n",
        "\n",
        "__PRO TIP__: even though you can set `batch_size` arbitrarily, we often use $2^n$ for that, so typical values are ``16, 32, 128, 256, ...`` And it dependes on your data size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwLU2dgJSXeE"
      },
      "source": [
        "### Tensor Operations\n",
        "\n",
        "Think of a simple Single Layered Perceptron (SLP) ``dense`` layer in ``keras``, which can be implemented via the code below:\n",
        "\n",
        "``keras.layers.Dense(512, activation='relu')``\n",
        "\n",
        "This layer can be simplified as a mathematical function: if the input is a ``2D`` tensor then the output is a ``2D`` tensor. We can define the function as follows (where $X$ is a ``2D`` input, $W$ is the weights on $X$, and $b$ is a vector):\n",
        "\n",
        "$$ y = relu((W \\cdot X) + b) $$\n",
        "\n",
        "Without the `relu` activation, you can see ``dense`` is simple a linear combination - what we saw a lot in regression models.\n",
        "\n",
        "What are the operations above:\n",
        "- $ \\cdot $ (dot) this is a [dot product](https://en.wikipedia.org/wiki/Dot_product) of $W$ and $X$\n",
        "- $ + $ just a simple addition, we need to make sure if $X$ is ``2D`` and $b$ is ``2D`` as well\n",
        "- ``relu`` a simple operation: `relu(x) == max(x, 0)`.\n",
        "\n",
        "$\\cdot$ is a tensor operation.\n",
        "\n",
        "![dot product](https://drek4537l1klr.cloudfront.net/chollet/Figures/02fig05.jpg  \"Illustration of Dot Product\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tjm3uj4VR4D"
      },
      "source": [
        "### Element-wise Operations\n",
        "\n",
        "Previous $+$ and `relu` operations are _element-wise_ operations: meaning the operation is applied to each scalar in a tensor. You can think of element-wise operations implemented using ``for`` loops in Python.\n",
        "\n",
        "For instance, the `relu` operation can be implemented below (you can find the implementation of addition in the textbook):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mH-Ohgwfsqtb"
      },
      "source": [
        "def naive_relu(x):\n",
        "    assert len(x.shape) == 2\n",
        "\n",
        "    x = x.copy()\n",
        "    for i in range(x.shape[0]):\n",
        "        for j in range(x.shape[1]):\n",
        "            x[i, j] = max(x[i, j], 0)\n",
        "    return x\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YItxINaxY8ID"
      },
      "source": [
        "With the help of ``NumPy`` (since ``NumPy`` automatically __broadcast__ any operation to the array), we can implement the above SLP layer without a ``for`` loop.\n",
        "\n",
        "``y = np.max(np.dot(W, X) + b)``"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayHJ4-wpaEXS"
      },
      "source": [
        "### Tensor Reshaping\n",
        "\n",
        "In order to perform tensor operations, the two tensors should be of the __same__ shape. If the two tensors are of _different_ shapes, we can always use `.reshape` to make them into the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeSYpODCYoeD",
        "outputId": "24362f85-bdb4-44b1-8559-68055d2461de"
      },
      "source": [
        "x = np.array([[0., 1.],\n",
        "                 [2., 3.],\n",
        "                 [4., 5.]])\n",
        "x.shape\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HJFkEQiZvc-",
        "outputId": "0d62bd31-4f3b-4e1e-9ff1-a8b5d5e0e709"
      },
      "source": [
        "x = x.reshape((6, 1))\n",
        "x.shape\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtrwZfuKcy70",
        "outputId": "68ddf45f-6e21-462c-e2b4-aa7e0133b289"
      },
      "source": [
        "x = x.reshape((2, 3))\n",
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 2.],\n",
              "       [3., 4., 5.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J64MMDuPc6gW"
      },
      "source": [
        "A specific type of reshape is transpose, which means change the original first dimension (\"rows\") into second dimension (\"columns\"), and vise versa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNZmAHorPvbw"
      },
      "source": [
        "## Gradient Descent - Optimizing the NN\n",
        "\n",
        "Gradient Descent is the key to optimize any NN - optimizing in this context means finding the __best__ _model parameters_ in training.\n",
        "\n",
        "Let's start by reviewing the SLP layer we saw before:\n",
        "\n",
        "$$ \\hat{y} = relu((W \\cdot X) + b) $$\n",
        "\n",
        "$W$ and $b$ above are the _model parameters_ in the layer. The objective is to find the __best__ values for $W$ and $b$ so $\\hat{y}$ is close to $y$. $W$ and $b$ contains information learned from the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZnEB_6BRBXe"
      },
      "source": [
        "### Training a NN\n",
        "\n",
        "0. Fill $W$ and $b$ with _small, random_ values - no need to calculate $\\hat{y}$ since $W$ and $b$ are random;\n",
        "1. Sample a __batch__ of $X$ and $y$ - these batches are ordered, refer to [here](#scrollTo=1BTOFabbs1F5&line=10&uniqifier=1) for the notion of _batch_;\n",
        "2. Calculate $\\hat{y}$ based on $X$ - this step is called __forward pass__;\n",
        "3. Compute the loss on the batch - which measures the difference between $y$ and $\\hat{y}$ - aka. _error_ in ML;\n",
        "4. Update $W$ and $b$ so the _loss_ decreases;\n",
        "5. Repeat steps 1 - 4 until loss no longer decreases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jC0WyGMVSdVj"
      },
      "source": [
        "### Loss Function\n",
        "\n",
        "Loss function is one of the keys in training NNs. A lot of the evaluation metrics, such as `MAE` or `classification accuracy`, can be used as loss functions.\n",
        "\n",
        "However, in NNs we use the __Nagative Log-Likelihood__ (NLL) as the main loss function. Refer to [this article](https://towardsdatascience.com/log-loss-function-math-explained-5b83cd8d9c83) for more information. We will talk about loss functions next week."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_axrUX2Trs9"
      },
      "source": [
        "### Challenges in Training NN\n",
        "\n",
        "Calculating loss is straightforward - just simple tensor operations as we seen before. The key challenge is:\n",
        "\n",
        "\n",
        "\n",
        "> How do you determine whether we should _increase_ or _decrease_ $W$ and $b$, and by how much?\n",
        "\n",
        "Inituitively we can borrow the idea of hyperparameter tuning from ML: freezing all other weights in a NN, just change one scalar value at a time, see how the loss changes. If the loss decreases, moving on to the next. But this is __very inefficient__!!\n",
        "\n",
        "The solution is called __gradient__, since the operations in NNs are __differentiable__. Below is a refresher on these concepts.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_94XrhQU7Vt"
      },
      "source": [
        "### What is a derivative?\n",
        "\n",
        "Given a _continuous, smooth_ function $ y = f(x)$, mapping a real number $x$ to another real number $y$.\n",
        "\n",
        "Since the function is continuous, a small change in $x$ ($\\epsilon_{x}$) will lead to a small change in $y$ ($\\epsilon_{y}$):\n",
        "\n",
        "$$ y + \\epsilon_{y} = f(x + \\epsilon_{x}) $$\n",
        "\n",
        "Additionally, since the function is _smooth_ (non-linear), if $\\epsilon_{x}$ is small enough, at a certain point $p$, we can use a linear function with a slope $\\alpha$, so that $ \\epsilon_{y} = \\alpha \\times \\epsilon_{x} $:\n",
        "\n",
        "$$ y + \\alpha \\times \\epsilon_{x} = f(x + \\epsilon_{x}) $$\n",
        "\n",
        "In which, $\\alpha$ is called a derivative of $f$ at $p$. If $\\alpha$ is positive, $y$ increases when $x$ increases, and vice versa. And the absolute value of $\\alpha$ (called __magnitude__) measures how fast $y$ changes with $x$. Sounds familiar? This is the same as the coefficient in any linear model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L95WoGsmXVLF"
      },
      "source": [
        "### Illustration of Derivatives\n",
        "\n",
        "![derivative](https://drek4537l1klr.cloudfront.net/chollet/Figures/02fig10.jpg)\n",
        "\n",
        "For $f(x)$ to be __differentiable__ (means \"having derivatives\"), $f(x)$ needs to be __continuous__ and __smooth__. For all these $f(x)$, these exists a _derivative_ function $f'(x)$ that maps values of $x$ to the slope of the local linear approximation of $f$ in those points.\n",
        "\n",
        "For instance, if $f(x) = cos(x)$, $f'(x) = - sin(x)$. If $f(x) = \\alpha \\times x + b $, $f'(x) = \\alpha $.\n",
        "\n",
        "With the help of $f'(x)$, we can control how to move $f(x)$. Suppose we want to decrease $f(x)$ and if $f'(x) > 0$, we need to make $\\epsilon_{x} < 0$; and vice versa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpGeGUAVadAj"
      },
      "source": [
        "### Gradients: the Derivative of Tensor Operations\n",
        "\n",
        "Before we were dealing with derivatives of _scalar_ values, if we talk about _multi-dimensional_ inputs (tensors), the derivatives become a gradient.\n",
        "\n",
        "If $\\hat{y} = f(X) = W \\cdot X $, the loss value is $ loss(\\hat{y}, y) $, then we can have a loss function $ loss(\\hat{y}, y) = g(W) $. For any value of $ W_0 \\in W $, $g(W_0)$ has the same shape of $W$.\n",
        "\n",
        "If $ f(x) $ is of a scalar value (as we seen before), $f'(x)$ is the _slope_ of the $ f(x) $ curve. Similarly, $g(W_0)$ is the __curvature__ of $g(W)$.\n",
        "\n",
        "Keep in mind the training objective is to _minimize_ $ loss(\\hat{y}, y) $, which means is to minimize $ g(W) $. Say we start with $ W_0 $, the next point should be $ W_1 = W_0 - step \\times g(W_0) $, if $ g(W_0) > 0$; so we get $ g(W_1) < g(W_0) $. By repeating this step, we can ultimately get $ g(W_n) = min(g(W)) $.\n",
        "\n",
        "Note that $step$ is a required hyperparameter here, later we will refer to this as __learning rate__.\n",
        "- Typically __learning rate__ is positive;\n",
        "- Also __learning rate__ should be small, if it's too big we may skip the _minimal_ value;\n",
        "- Of course if it's too small the training is not efficient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssuTZjvhgs9m"
      },
      "source": [
        "### Stochastic gradient descent\n",
        "\n",
        "Theoretically (or mathematically), for any function $ g(W)$, at the minimal point its derivative is `0`. So to minimize the loss function, we just need to find all points where $ g(W) = 0 $. This is a polynomial function of $N$ variables, where $N$ is the number of neurons in a layer. If $N$ is small, we can easily solve it. But in a NN we are talking about $N$ of thousands, if not millions, so we need an algorithm to solve it.\n",
        "\n",
        "0. Fill all parameters with _small, random_ values\n",
        "1. Draw a batch of training samples $X$ and corresponding targets $y$.\n",
        "3. Run the network on $X$ to obtain predictions $ \\hat{y} $.\n",
        "4. Compute the loss of the network on the batch $loss(\\hat{y}, y)$.\n",
        "5. Compute the gradient of the loss with regard to the network’s parameters (a **backward pass**).\n",
        "6. Move the parameters a little in the opposite direction from the gradient — for example $W = step \\times gradient$ — thus reducing the loss on the batch a bit.\n",
        "7. Repeating steps 1 - 6 until the gradient is `0`.\n",
        "\n",
        "This is called a _mini-batch Stochastic Gradient Descent_ (mini-batch SGD) algorithm. Note that if the _batch_ is `1`, then it is a _true_ SGD algorithm. If the _batch_ size equals to the training sample size, it's a _batch_ SGD algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWeWCngjjMHA"
      },
      "source": [
        "### Illustration of SGD\n",
        "\n",
        "If there is only 1 parameter in a NN:\n",
        "\n",
        "![1D-SGD](https://drek4537l1klr.cloudfront.net/chollet/Figures/02fig11.jpg)\n",
        "\n",
        "If there are 2 parameters:\n",
        "\n",
        "![2D-SGD](https://drek4537l1klr.cloudfront.net/chollet/Figures/02fig12.jpg)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwxWmXSFj26M"
      },
      "source": [
        "### Variants of SGD - Optimizers of NN\n",
        "\n",
        "Researchers have suggested looking into the previous changes in the weights, rather than just the current gradients. This is called SGD with momentum. Examples include `Adagrad`, `RMSProp`, and so on.\n",
        "\n",
        "These are known as optimize of NN, which we will learn in the next week.\n",
        "\n",
        "SGD with momentum solves two problems:\n",
        "1. **local minima**: We need to find the __global minima__ (the absolute minimum) of the loss function - thus the learning rate cannot be too big;\n",
        "2. __convergence speed__: if the learning rate is too small, the convergence speed is low - meaning training will take a lot of time.\n",
        "\n",
        "![local minima](https://drek4537l1klr.cloudfront.net/chollet/Figures/02fig13.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHB50IStlle1"
      },
      "source": [
        "### Backpropgation - the Latest Breakthrough of NN\n",
        "\n",
        "Recall that we define any _continuous, smooth_ function $f(x)$ is __differentiable__, we can explicitly find the derivative $f'(x)$. However, in deep learning, because of the multiple layers, the gradients are _chained_: meaning the gradient from the current layer is linked with all previous layers.\n",
        "\n",
        "For instance, let's assume a NN ($F$) with 2 layers (with 2 tensor operations $f$, $g$, and weights as $W_a$, $W_b$):\n",
        "\n",
        "$$ F(W_b, W_a) = g(W_b, f(W_a)) $$\n",
        "\n",
        "Based on the [chain rule](https://en.wikipedia.org/wiki/Chain_rule), we know that:\n",
        "\n",
        "$$ [F(X)]' = f'(g(X)) \\times g'(X) $$\n",
        "\n",
        "Based on this, we have __backpropgation__, which starts with the _final_ loss value, and works backwards from the last (next to *output*) layer to the first (second to *input*) layer. In each step of the chain, the goal is to _minimize_ the current loss value.\n",
        "\n",
        "Thanks to __symbolic differentiation__, modern packages like `TensorFlow` can compute the gradient __function__ for the whole chain - which means you do not have to implement _backpropgation_ on your own. But if you want to learn more about this, refer to [this wonderful presentation](https://cs230.stanford.edu/files/C1M2.pdf) by one of the giants (Andrew Ng) in the field.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ui20NpAPpsg7"
      },
      "source": [
        "# Deep Learning & Artificial Intelligence\n",
        "## Neural Networks Foundations, Chapter 2\n",
        "### Dr. Jie Tao, Fairfield University"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MROx_8Swc4Rs"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}