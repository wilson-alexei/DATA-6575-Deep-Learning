{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3U0r_Brv8An"
      },
      "source": [
        "# Deep Learning & Artificial Intelligence\n",
        "## Advanced Tricks and Latest Developments with Deep Learning\n",
        "### Dr. Jie Tao, Fairfield University"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJ_TuJsvwLly"
      },
      "source": [
        "## Hyperparameter Optimization\n",
        "\n",
        "- Similar to any ML models, we need to tune the hyperparameters of the models in order to search for the __optimal results__\n",
        "- This process, aka., hyperparameter tuning\n",
        "  - Review that *hyperparameters* refer to the the specifications of the models to be trained, while _parameters_ refer to the values learned during the training process\n",
        "- Specifically for deep learning models, hyperparameters usually refer to (but not limited to):\n",
        "  - number of layers\n",
        "  - number of neurons or filters in each layer\n",
        "  - activation function in each layer\n",
        "  - other design decisions like `Dropout` or `BatchNormalization`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcNQJPNC0Icw"
      },
      "source": [
        "### More Arts than Sciences\n",
        "\n",
        "- Experienced data scientists build intuition over time about what works best in certain situations\n",
        "- Also, it's usually a __trial-and-error__ process\n",
        "  - recall what you did in the machine learning class?\n",
        "- There are no formal rules or _silver bullet_ for hyperparameter tuning or model selection\n",
        "- It is the norm you should __NOT__ rely on your _arbitrary_ decisions\n",
        "  - So you will have to train the model repeatedly to find the __optimal__ hyperparameters\n",
        "- You have two strategies:\n",
        "  - Either you will **manually** search all possible combinations of the hyperparameters\n",
        "  - Or you can search in an __automatic__ and __systematic__ way"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mxr4iyjLH8sy"
      },
      "source": [
        "### Review the Process of Hyperparameter Optimization\n",
        "\n",
        "1. Choose a set of hyperparameters (__randomly__ or __heuristically__)\n",
        "2. Build the corresponding model\n",
        "3. Train (fit) the model to the _training data_, and evaluate the performance (with __selected metrics__) of the model using the _validation data_\n",
        "4. Choose the next set of hyperparameters (automatically)\n",
        "5. Repeat the process until you reach the (psuedo) optimal performance\n",
        "6. Eventually, using the tuned model to __predict__ the _test data_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Se8S98MJZAT"
      },
      "source": [
        "### How to Tune Hyperparameters\n",
        "\n",
        "- Among these steps, step `3` is very important. There are different techniques available:\n",
        "  - Bayesian optimization\n",
        "  - genetic algorithms\n",
        "  - simple random search\n",
        "  - ...\n",
        "- We already know that we update the weights (__parameters__) with the _backpropagation_ algorithm\n",
        "- On the contrary, hyperparameter tuning is extremely hard:\n",
        "  - Computing the __feedback signal__: this is __expensive__ since you have to train the models from scratch repeatedly\n",
        "  - Unlike the parameters, the hyperparamesters are usually __discrete__ and non-differentiable - so you have to use gradient-free option, which is usually far less effcient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKiVBs_vWUuX"
      },
      "source": [
        "### Something Beyond Random Search\n",
        "\n",
        "- Usually, we have very limited tools to search for the optimal set of hyperparameters:\n",
        "  - The tecniques mentioned above are too expensive for today's computing power\n",
        "- Thus, usually it is just __random search__\n",
        "  - The method allows us to choose hyperparameters randomly and evaluate the performances repeatedly\n",
        "  - It is the most __naive__ method\n",
        "- One recent package called [hyperas](https://github.com/maxpumperla/hyperas), which assists us in this difficult task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c7ArHkEmxnr"
      },
      "source": [
        "### A `hyperas` Tutorial\n",
        "\n",
        "[OP](https://towardsdatascience.com/a-guide-to-an-efficient-way-to-build-neural-network-architectures-part-i-hyper-parameter-8129009f131b)\n",
        "- Like any ML/DL process, one of the most important task is to choose the most appropriate (evaluation) metric and loss function\n",
        "  - In this tutorial, we are doing the fashion-MNIST image classification, so `acc` as the evaluation metric (since the data is *balanced* across classes) and `categorical crossentropy` as the loss function (since it is a multi-class classification problem) seem appropriate.\n",
        "- We also need to normalize the data\n",
        "  - Let's first try the normalization we know, then we will try something new called `batchNormalization`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNoOJz6OwDrm",
        "outputId": "f50fd972-9c18-4e74-fc5d-e7b7e30e0b09"
      },
      "source": [
        "#### load data\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "print('train shapes', X_train.shape, y_train.shape)\n",
        "print('test shapes', X_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "train shapes (60000, 28, 28) (60000,)\n",
            "test shapes (10000, 28, 28) (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArkOIOfcoiO8"
      },
      "source": [
        "#### preprocessing\n",
        "#### combine 2D images to 1D tensors\n",
        "X_train = X_train.reshape(60000, 784)\n",
        "X_test = X_test.reshape(10000, 784)\n",
        "#### set the data type as float\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "#### normalize so it's between [0,1]\n",
        "X_train /= 255\n",
        "X_test/= 255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cdm16IO_o7uj"
      },
      "source": [
        "#### one hot encoding of the classes\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "nb_classes = 10\n",
        "y_train = to_categorical(y_train, nb_classes)\n",
        "y_test= to_categorical(y_test, nb_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6zadHt3paiI"
      },
      "source": [
        "In any hyperparameter tuning process, it is very important to build a base model. That is the model you will compare against."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLEUqhtwpjFY",
        "outputId": "e194136a-7e3a-4e8c-cf3c-8a82ba6a7bbe"
      },
      "source": [
        "#### base model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "model = Sequential([\n",
        " Dense(10,input_shape=(784,),activation='softmax')\n",
        "])\n",
        "model.compile(optimizer=SGD(lr=0.1),\n",
        " loss='categorical_crossentropy',\n",
        " metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 10)                7850      \n",
            "=================================================================\n",
            "Total params: 7,850\n",
            "Trainable params: 7,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vxt8UbgqAIf",
        "outputId": "b28eac36-5ade-4de1-e0c8-46bcfcaf0f66"
      },
      "source": [
        "#### fit/training\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=128,\n",
        "                    validation_split=0.2)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "375/375 [==============================] - 2s 3ms/step - loss: 0.9521 - accuracy: 0.6864 - val_loss: 0.5759 - val_accuracy: 0.8019\n",
            "Epoch 2/10\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.5552 - accuracy: 0.8174 - val_loss: 0.5083 - val_accuracy: 0.8275\n",
            "Epoch 3/10\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.5049 - accuracy: 0.8314 - val_loss: 0.4852 - val_accuracy: 0.8317\n",
            "Epoch 4/10\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.4843 - accuracy: 0.8366 - val_loss: 0.4828 - val_accuracy: 0.8314\n",
            "Epoch 5/10\n",
            "375/375 [==============================] - 1s 3ms/step - loss: 0.4635 - accuracy: 0.8433 - val_loss: 0.4715 - val_accuracy: 0.8390\n",
            "Epoch 6/10\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.4532 - accuracy: 0.8453 - val_loss: 0.4576 - val_accuracy: 0.8441\n",
            "Epoch 7/10\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.4512 - accuracy: 0.8469 - val_loss: 0.4485 - val_accuracy: 0.8482\n",
            "Epoch 8/10\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.4406 - accuracy: 0.8514 - val_loss: 0.4458 - val_accuracy: 0.8468\n",
            "Epoch 9/10\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.4359 - accuracy: 0.8487 - val_loss: 0.4468 - val_accuracy: 0.8482\n",
            "Epoch 10/10\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.4331 - accuracy: 0.8501 - val_loss: 0.4438 - val_accuracy: 0.8482\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxpWMOFdqQ-Y"
      },
      "source": [
        "So we can see the `val_loss` is `0.4427` and `val_accuracy` is `0.8464` for the base model. Not bad but we can improve."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUc0zemYqzp0"
      },
      "source": [
        "### Need for Hyperparameter Tuning\n",
        "\n",
        "- Let's review why we need hyperparameter tuning again:\n",
        "  - To find the right balance between **bias** and **variance**. In other words, you don't want a model which is very accurate in training but not as much in validation/testing.\n",
        "  - To prevent ourselves from falling prey to the **vanishing/exploding gradient** problem: tweaking the *learning rate*, *activation function* and *number of layers* can help us with that problem.\n",
        "  - Encountering saddle points and **local optima**. Changing learning rates and activation can help us with that problem.\n",
        "  - Model reaches **no convergence**: using _adaptive learning rates_ may help with that issue.\n",
        "  - **Extremely low gradients** with `sigmoid` and `tanh` functions. These two functions may not be very good for very deep networks.\n",
        "  - **Slow** training time. More complexity does not equal to higher performance. Sometimes we can find the minimal architecture for a model to reach the _\"best\"_ performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ChdgMZViAv2"
      },
      "source": [
        "### What can we tune? & Pro Tips\n",
        "\n",
        "We can normally tune the following hyperparamters (maybe we won't tune all of them at the same time but this is a complete list):\n",
        "- Number of Layers: higher --> overfitting/vanishing gradients; low --> low performance; depending on size of training data\n",
        "- Number of neurons per layer: low --> high bias, high variance; high --> low bias, low variance; depending on size of training data\n",
        "- Activation function: ReLU is a good choice for starters\n",
        "- Optimizers: `Adam` is generally good, `RMSProp` is good for getting over local optima; `Adadelta` is good for sparse data\n",
        "- Learning rate: dependent on the optimizers (`SGD`: 0.1, `Adam`: 0.1/0.01). You should also consider the _learning rate decay_.\n",
        "- Initialization: not so important, HE-normal for `ReLU` and Glorot-normal for `Sigmoid` are good choices.\n",
        "- Batch size: low --> hard to converge; high --> slow training. Try power of 2; depending on size of training data\n",
        "- Number of epochs: high --> overfitting, low --> underfitting. Try higher but use `earlystopping` or `dropout`.\n",
        "- Dropout: try different drop out ratio between `[0,1]`.\n",
        "- L1/L2 regularizations: used to control the bias-variance tradeoff. Normally used when `dropout` is not working well"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEEjSJR8kkxm"
      },
      "source": [
        "To use `hyperas`, we need to install it (Colab does not have it pre-installed). Note that `hyperas` is specifically designed for `keras`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0zdwiWVkvaw",
        "outputId": "dfac46f1-4792-4f29-aadb-dd6b0e83d4ed"
      },
      "source": [
        "!pip install hyperas"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting hyperas\n",
            "  Downloading https://files.pythonhosted.org/packages/04/34/87ad6ffb42df9c1fa9c4c906f65813d42ad70d68c66af4ffff048c228cd4/hyperas-0.4.1-py3-none-any.whl\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from hyperas) (5.6.1)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from hyperas) (5.0.8)\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.6/dist-packages (from hyperas) (0.1.2)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (from hyperas) (1.0.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from hyperas) (2.4.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.6/dist-packages (from hyperas) (0.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (4.7.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (2.6.1)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.6.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (4.3.3)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (1.4.3)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.4.4)\n",
            "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (2.11.2)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.8.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (3.2.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (2.6.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (0.2.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (2.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (0.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (4.41.1)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (3.11.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (1.15.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (5.3.1)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (5.0.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (7.6.3)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (4.10.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (5.2.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (2.10.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->nbconvert->hyperas) (4.4.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.4->nbconvert->hyperas) (1.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->hyperas) (20.8)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->hyperas) (0.5.1)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->hyperas) (5.1.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->hyperas) (1.5.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->hyperas) (0.9.2)\n",
            "Requirement already satisfied: jupyter-client>=5.2.0 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->hyperas) (5.3.5)\n",
            "Requirement already satisfied: pyzmq>=17.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->hyperas) (20.0.0)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->hyperas) (1.9.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->hyperas) (3.5.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->hyperas) (1.0.0)\n",
            "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->hyperas) (5.5.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->hyperas) (1.0.18)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->bleach->nbconvert->hyperas) (2.4.7)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.8.1->notebook->jupyter->hyperas) (0.7.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client>=5.2.0->notebook->jupyter->hyperas) (2.8.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->hyperas) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->hyperas) (0.8.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->hyperas) (51.3.3)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->hyperas) (4.8.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter->hyperas) (0.2.5)\n",
            "Installing collected packages: hyperas\n",
            "Successfully installed hyperas-0.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkjN3JfalmIz"
      },
      "source": [
        "You need to import the following things to use `hyperas`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aeh6xPohlrU3"
      },
      "source": [
        "from hyperopt import Trials, STATUS_OK, tpe\n",
        "from hyperas import optim\n",
        "from hyperas.distributions import choice, uniform"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02CuRnZqkz-W"
      },
      "source": [
        "### `hyperas` Helper Functions\n",
        "\n",
        "When using `hyperas`, since you are training the mode repeatedly, you need three helper functions.\n",
        "- Rather than doing these parts in loose codes like we did above.\n",
        "- `data_loader` function: load train and validation data. If you need to pre-process data, do that and then load the pre-processed data;\n",
        "- `hyperas_model` function: we define the archirecution of models in this function, also specify the hyperparameters we would like to tune here.\n",
        "- `hyperas_opt` function; this function fits the defined model from the model function to the training data, and evaluate the trained model against the validation data in each epoch.\n",
        "\n",
        "You can see that we did the three thing above before, just in loose codes.\n",
        "\n",
        "See example below for the functions on the fashion-MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SWZFzuADKqX"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "#### the data_loader function\n",
        "def data_loader():\n",
        "    '''\n",
        "    Load data, scaling, and one-hot encoding.\n",
        "\n",
        "    This function is separated from create_model() so that hyperopt\n",
        "    won't reload data for each evaluation run.\n",
        "\n",
        "    Output:\n",
        "    ---\n",
        "    Processed training data (X_train, y_train) and test data (X_test, y_test).\n",
        "    '''\n",
        "    ##### load data\n",
        "    (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "    #### if you want a fixed val set do below\n",
        "    # X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=12345)\n",
        "    #### reshaping\n",
        "    X_train = X_train.reshape(60000, 784)\n",
        "    X_test = X_test.reshape(10000, 784)\n",
        "    #### change data type\n",
        "    X_train = X_train.astype('float32')\n",
        "    X_test = X_test.astype('float32')\n",
        "    #### scaling\n",
        "    X_train /= 255\n",
        "    X_test /= 255\n",
        "    #### One-hot encoding of class labels\n",
        "    nb_classes = 10\n",
        "    y_train = to_categorical(y_train, nb_classes)\n",
        "    # y_val = to_categorical(y_val, nb_classes)\n",
        "    y_test = to_categorical(y_test, nb_classes)\n",
        "    return X_train, y_train, X_test, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3YQ_l3fEnaH"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.layers import Activation, Dropout\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "\n",
        "#### hyperas_model function\n",
        "def hyperas_model(X_train, y_train, X_val, y_val):\n",
        "\n",
        "    '''\n",
        "    Use thr training data to fit, and validation data to evaluate the model.\n",
        "    Test the model when evaluation is done.\n",
        "\n",
        "    Create Keras model with double curly brackets dropped-in as needed.\n",
        "    Return value has to be a valid python dictionary with two customary keys:\n",
        "        - loss: Specify a numeric evaluation metric to be minimized\n",
        "        - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
        "    The last one is optional, though recommended, namely:\n",
        "        - model: specify the model just created so that we can later use it again.\n",
        "    '''\n",
        "    #### define the model - very similar to how we define our model before\n",
        "    model = Sequential()\n",
        "    #### Add first dense layer\n",
        "    #### specify different values of number of neurons to test\n",
        "    model.add(Dense({{choice([128, 256, 512, 1024])}}, input_shape=(784,)))\n",
        "    #### activation function for the first dense layer\n",
        "    #### the reason we list it out is we want to test different activation functions\n",
        "    model.add(Activation({{choice(['relu', 'sigmoid'])}}))\n",
        "    #### Dropout layer\n",
        "    #### test different dropout values\n",
        "    model.add(Dropout({{uniform(0, 1)}}))\n",
        "    #### second dense layer\n",
        "    model.add(Dense({{choice([128, 256, 512, 1024])}}))\n",
        "    model.add(Activation({{choice(['relu', 'sigmoid'])}}))\n",
        "    #### second dropout layer\n",
        "    model.add(Dropout({{uniform(0, 1)}}))\n",
        "    #### a condition to test whether a third layer pair is to be added\n",
        "    if {{choice(['two', 'three'])}} == 'three':\n",
        "        model.add(Dense({{choice([128, 256, 512, 1024])}}))\n",
        "        model.add(Activation({{choice(['relu', 'sigmoid'])}}))\n",
        "        model.add(Dropout({{uniform(0, 1)}}))\n",
        "    #### output layer\n",
        "    #### determined by number of classes and classification problem\n",
        "    #### no need to test\n",
        "    model.add(Dense(10))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    #### define optimizers, test different learning rates\n",
        "    adam = Adam(lr={{choice([10**-3, 10**-2, 10**-1])}})\n",
        "    rmsprop = RMSprop(lr={{choice([10**-3, 10**-2, 10**-1])}})\n",
        "    sgd = SGD(lr={{choice([10**-3, 10**-2, 10**-1])}})\n",
        "    #### actual test\n",
        "    choiceval = {{choice(['adam', 'sgd', 'rmsprop'])}}\n",
        "    if choiceval == 'adam':\n",
        "        optim = adam\n",
        "    elif choiceval == 'rmsprop':\n",
        "        optim = rmsprop\n",
        "    else:\n",
        "        optim = sgd\n",
        "    #### complie model\n",
        "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer=optim)\n",
        "    #### fit to training set and evaluate using validation data\n",
        "    history = model.fit(X_train, y_train,\n",
        "              batch_size={{choice([64, 128])}},\n",
        "              epochs=10,\n",
        "              verbose=2,\n",
        "              validation_split=0.2)\n",
        "    #### record the best model perofrmance on the validation set\n",
        "    validation_acc = np.amax(history.history['val_accuracy'])\n",
        "    print('Best validation acc of epoch:', validation_acc)\n",
        "    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5_P5vM8hKOX"
      },
      "source": [
        "__NOTE__: Below we need to use the path of the notebook, make sure you get the correct path (_right click on the notebook and \"copy path\"_) and then update the `nb_path` variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3EDXCzKnHUeo",
        "outputId": "d4a6064e-bdb7-4caf-9383-30505cc1cddd"
      },
      "source": [
        "#### MAKE SURE YOU CHANGE THIS TO YOUR OWN PATH\n",
        "nb_path = 'drive/MyDrive/Colab Notebooks/L8-AdvancedTopics'\n",
        "\n",
        "best_run, best_model = optim.minimize(model=hyperas_model,\n",
        "                                          data=data_loader,\n",
        "                                          algo=tpe.suggest,\n",
        "                                          max_evals=5,\n",
        "                                          trials=Trials(),\n",
        "                                          notebook_name= nb_path)\n",
        "X_train, Y_train, X_test, y_test = data_loader()\n",
        "print(\"Evalutation of best performing model:\")\n",
        "print(best_model.evaluate(X_test, y_test))\n",
        "print(\"Best performing model chosen hyper-parameters:\")\n",
        "print(best_run)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>> Imports:\n",
            "#coding=utf-8\n",
            "\n",
            "try:\n",
            "    from tensorflow.keras.datasets import fashion_mnist\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from tensorflow.keras.utils import to_categorical\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from tensorflow.keras.models import Sequential\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from tensorflow.keras.layers import Dense\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from tensorflow.keras.optimizers import SGD\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperopt import Trials, STATUS_OK, tpe\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperas import optim\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from hyperas.distributions import choice, uniform\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from sklearn.model_selection import train_test_split\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import numpy as np\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from tensorflow.keras.layers import Activation, Dropout\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
            "except:\n",
            "    pass\n",
            "\n",
            ">>> Hyperas search space:\n",
            "\n",
            "def get_space():\n",
            "    return {\n",
            "        'Dense': hp.choice('Dense', [128, 256, 512, 1024]),\n",
            "        'Activation': hp.choice('Activation', ['relu', 'sigmoid']),\n",
            "        'Dropout': hp.uniform('Dropout', 0, 1),\n",
            "        'Dense_1': hp.choice('Dense_1', [128, 256, 512, 1024]),\n",
            "        'Activation_1': hp.choice('Activation_1', ['relu', 'sigmoid']),\n",
            "        'Dropout_1': hp.uniform('Dropout_1', 0, 1),\n",
            "        'Dropout_2': hp.choice('Dropout_2', ['two', 'three']),\n",
            "        'Dense_2': hp.choice('Dense_2', [128, 256, 512, 1024]),\n",
            "        'Activation_2': hp.choice('Activation_2', ['relu', 'sigmoid']),\n",
            "        'Dropout_3': hp.uniform('Dropout_3', 0, 1),\n",
            "        'lr': hp.choice('lr', [10**-3, 10**-2, 10**-1]),\n",
            "        'lr_1': hp.choice('lr_1', [10**-3, 10**-2, 10**-1]),\n",
            "        'lr_2': hp.choice('lr_2', [10**-3, 10**-2, 10**-1]),\n",
            "        'choiceval': hp.choice('choiceval', ['adam', 'sgd', 'rmsprop']),\n",
            "        'batch_size': hp.choice('batch_size', [64, 128]),\n",
            "    }\n",
            "\n",
            ">>> Data\n",
            "  1: \n",
            "  2: '''\n",
            "  3: Load data, scaling, and one-hot encoding. \n",
            "  4: \n",
            "  5: This function is separated from create_model() so that hyperopt\n",
            "  6: won't reload data for each evaluation run.\n",
            "  7: \n",
            "  8: Output:\n",
            "  9: ---\n",
            " 10: Processed training data (X_train, y_train) and test data (X_test, y_test).\n",
            " 11: '''\n",
            " 12: ##### load data \n",
            " 13: (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
            " 14: #### if you want a fixed val set do below\n",
            " 15: # X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=12345)\n",
            " 16: #### reshaping\n",
            " 17: X_train = X_train.reshape(60000, 784)\n",
            " 18: # X_val = X_val.reshape(12000, 784)\n",
            " 19: #### change data type\n",
            " 20: X_train = X_train.astype('float32')\n",
            " 21: # X_val = X_val.astype('float32')\n",
            " 22: #### scaling\n",
            " 23: X_train /= 255\n",
            " 24: # X_val /= 255\n",
            " 25: #### One-hot encoding of class labels\n",
            " 26: nb_classes = 10\n",
            " 27: y_train = to_categorical(y_train, nb_classes)\n",
            " 28: # y_val = to_categorical(y_val, nb_classes)\n",
            " 29: y_test = to_categorical(y_test, nb_classes)\n",
            " 30: \n",
            " 31: \n",
            " 32: \n",
            ">>> Resulting replaced keras model:\n",
            "\n",
            "   1: def keras_fmin_fnct(space):\n",
            "   2: \n",
            "   3:     \n",
            "   4:     '''\n",
            "   5:     Use thr training data to fit, and validation data to evaluate the model.\n",
            "   6:     Test the model when evaluation is done.\n",
            "   7: \n",
            "   8:     Create Keras model with double curly brackets dropped-in as needed.\n",
            "   9:     Return value has to be a valid python dictionary with two customary keys:\n",
            "  10:         - loss: Specify a numeric evaluation metric to be minimized\n",
            "  11:         - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
            "  12:     The last one is optional, though recommended, namely:\n",
            "  13:         - model: specify the model just created so that we can later use it again.\n",
            "  14:     '''\n",
            "  15:     #### define the model - very similar to how we define our model before\n",
            "  16:     model = Sequential()\n",
            "  17:     #### Add first dense layer\n",
            "  18:     #### specify different values of number of neurons to test\n",
            "  19:     model.add(Dense(space['Dense'], input_shape=(784,)))\n",
            "  20:     #### activation function for the first dense layer\n",
            "  21:     #### the reason we list it out is we want to test different activation functions\n",
            "  22:     model.add(Activation(space['Activation']))\n",
            "  23:     #### Dropout layer\n",
            "  24:     #### test different dropout values\n",
            "  25:     model.add(Dropout(space['Dropout']))\n",
            "  26:     #### second dense layer\n",
            "  27:     model.add(Dense(space['Dense_1']))\n",
            "  28:     model.add(Activation(space['Activation_1']))\n",
            "  29:     #### second dropout layer\n",
            "  30:     model.add(Dropout(space['Dropout_1']))\n",
            "  31:     #### a condition to test whether a third layer pair is to be added\n",
            "  32:     if space['Dropout_2'] == 'three':\n",
            "  33:         model.add(Dense(space['Dense_2']))\n",
            "  34:         model.add(Activation(space['Activation_2']))\n",
            "  35:         model.add(Dropout(space['Dropout_3']))\n",
            "  36:     #### output layer\n",
            "  37:     #### determined by number of classes and classification problem\n",
            "  38:     #### no need to test    \n",
            "  39:     model.add(Dense(10))\n",
            "  40:     model.add(Activation('softmax'))\n",
            "  41: \n",
            "  42:     #### define optimizers, test different learning rates\n",
            "  43:     adam = Adam(lr=space['lr'])\n",
            "  44:     rmsprop = RMSprop(lr=space['lr_1'])\n",
            "  45:     sgd = SGD(lr=space['lr_2'])\n",
            "  46:     #### actual test\n",
            "  47:     choiceval = space['choiceval']\n",
            "  48:     if choiceval == 'adam':\n",
            "  49:         optim = adam\n",
            "  50:     elif choiceval == 'rmsprop':\n",
            "  51:         optim = rmsprop\n",
            "  52:     else:\n",
            "  53:         optim = sgd\n",
            "  54:     #### complie model   \n",
            "  55:     model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer=optim)\n",
            "  56:     #### fit to training set and evaluate using validation data\n",
            "  57:     history = model.fit(X_train, y_train,\n",
            "  58:               batch_size=space['batch_size'],\n",
            "  59:               epochs=10,\n",
            "  60:               verbose=2,\n",
            "  61:               validation_split=0.2)\n",
            "  62:     #### record the best model perofrmance on the validation set\n",
            "  63:     validation_acc = np.amax(history.history['val_accuracy']) \n",
            "  64:     print('Best validation acc of epoch:', validation_acc)\n",
            "  65:     return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}\n",
            "  66: \n",
            "Epoch 1/10\n",
            "375/375 - 4s - loss: 1.2560 - accuracy: 0.5774 - val_loss: 1.2094 - val_accuracy: 0.6489\n",
            "\n",
            "Epoch 2/10\n",
            "375/375 - 3s - loss: 1.0515 - accuracy: 0.6466 - val_loss: 0.8761 - val_accuracy: 0.6824\n",
            "\n",
            "Epoch 3/10\n",
            "375/375 - 3s - loss: 1.1143 - accuracy: 0.6314 - val_loss: 0.8470 - val_accuracy: 0.7313\n",
            "\n",
            "Epoch 4/10\n",
            "375/375 - 3s - loss: 1.1638 - accuracy: 0.6246 - val_loss: 0.8906 - val_accuracy: 0.6643\n",
            "\n",
            "Epoch 5/10\n",
            "375/375 - 3s - loss: 1.2094 - accuracy: 0.6122 - val_loss: 1.1406 - val_accuracy: 0.6537\n",
            "\n",
            "Epoch 6/10\n",
            "375/375 - 3s - loss: 1.2426 - accuracy: 0.5905 - val_loss: 1.0960 - val_accuracy: 0.6230\n",
            "\n",
            "Epoch 7/10\n",
            "375/375 - 3s - loss: 1.2968 - accuracy: 0.5880 - val_loss: 1.0756 - val_accuracy: 0.6355\n",
            "\n",
            "Epoch 8/10\n",
            "375/375 - 3s - loss: 1.2581 - accuracy: 0.5830 - val_loss: 1.0618 - val_accuracy: 0.6112\n",
            "\n",
            "Epoch 9/10\n",
            "375/375 - 3s - loss: 1.3152 - accuracy: 0.5768 - val_loss: 1.4604 - val_accuracy: 0.6369\n",
            "\n",
            "Epoch 10/10\n",
            "375/375 - 3s - loss: 1.2663 - accuracy: 0.5719 - val_loss: 1.1932 - val_accuracy: 0.5763\n",
            "\n",
            "Best validation acc of epoch:\n",
            "0.731333315372467\n",
            "Epoch 1/10\n",
            "750/750 - 7s - loss: 1.9454 - accuracy: 0.2774 - val_loss: 1.3984 - val_accuracy: 0.4075\n",
            "\n",
            "Epoch 2/10\n",
            "750/750 - 7s - loss: 1.7867 - accuracy: 0.3119 - val_loss: 1.3477 - val_accuracy: 0.4299\n",
            "\n",
            "Epoch 3/10\n",
            "750/750 - 7s - loss: 1.7745 - accuracy: 0.3219 - val_loss: 1.3151 - val_accuracy: 0.5467\n",
            "\n",
            "Epoch 4/10\n",
            "750/750 - 7s - loss: 1.7770 - accuracy: 0.3261 - val_loss: 1.3633 - val_accuracy: 0.4195\n",
            "\n",
            "Epoch 5/10\n",
            "750/750 - 7s - loss: 1.7706 - accuracy: 0.3315 - val_loss: 1.3279 - val_accuracy: 0.5450\n",
            "\n",
            "Epoch 6/10\n",
            "750/750 - 7s - loss: 1.7761 - accuracy: 0.3302 - val_loss: 1.3158 - val_accuracy: 0.4551\n",
            "\n",
            "Epoch 7/10\n",
            "750/750 - 7s - loss: 1.7535 - accuracy: 0.3308 - val_loss: 1.3297 - val_accuracy: 0.4453\n",
            "\n",
            "Epoch 8/10\n",
            "750/750 - 7s - loss: 1.7553 - accuracy: 0.3320 - val_loss: 1.2192 - val_accuracy: 0.5318\n",
            "\n",
            "Epoch 9/10\n",
            "750/750 - 7s - loss: 1.7210 - accuracy: 0.3417 - val_loss: 1.1984 - val_accuracy: 0.5261\n",
            "\n",
            "Epoch 10/10\n",
            "750/750 - 7s - loss: 1.7093 - accuracy: 0.3444 - val_loss: 1.1615 - val_accuracy: 0.5638\n",
            "\n",
            "Best validation acc of epoch:\n",
            "0.5638333559036255\n",
            "Epoch 1/10\n",
            "375/375 - 6s - loss: 3.5478 - accuracy: 0.1115 - val_loss: 2.6788 - val_accuracy: 0.1013\n",
            "\n",
            "Epoch 2/10\n",
            "375/375 - 5s - loss: 3.0664 - accuracy: 0.1159 - val_loss: 2.8067 - val_accuracy: 0.1958\n",
            "\n",
            "Epoch 3/10\n",
            "375/375 - 5s - loss: 3.1153 - accuracy: 0.1165 - val_loss: 2.6640 - val_accuracy: 0.1013\n",
            "\n",
            "Epoch 4/10\n",
            "375/375 - 5s - loss: 3.0437 - accuracy: 0.1140 - val_loss: 2.3458 - val_accuracy: 0.0957\n",
            "\n",
            "Epoch 5/10\n",
            "375/375 - 5s - loss: 3.1507 - accuracy: 0.1189 - val_loss: 2.8855 - val_accuracy: 0.1777\n",
            "\n",
            "Epoch 6/10\n",
            "375/375 - 5s - loss: 3.1520 - accuracy: 0.1178 - val_loss: 2.5144 - val_accuracy: 0.1013\n",
            "\n",
            "Epoch 7/10\n",
            "375/375 - 5s - loss: 3.2173 - accuracy: 0.1209 - val_loss: 3.5006 - val_accuracy: 0.1987\n",
            "\n",
            "Epoch 8/10\n",
            "375/375 - 5s - loss: 3.1609 - accuracy: 0.1192 - val_loss: 2.1560 - val_accuracy: 0.1923\n",
            "\n",
            "Epoch 9/10\n",
            "375/375 - 5s - loss: 3.0378 - accuracy: 0.1191 - val_loss: 2.2273 - val_accuracy: 0.2000\n",
            "\n",
            "Epoch 10/10\n",
            "375/375 - 5s - loss: 3.1368 - accuracy: 0.1189 - val_loss: 2.1215 - val_accuracy: 0.1973\n",
            "\n",
            "Best validation acc of epoch:\n",
            "0.20000000298023224\n",
            "Epoch 1/10\n",
            "750/750 - 8s - loss: 2.1926 - accuracy: 0.2097 - val_loss: 1.9527 - val_accuracy: 0.4907\n",
            "\n",
            "Epoch 2/10\n",
            "750/750 - 8s - loss: 1.7467 - accuracy: 0.4343 - val_loss: 1.4551 - val_accuracy: 0.6737\n",
            "\n",
            "Epoch 3/10\n",
            "750/750 - 7s - loss: 1.3548 - accuracy: 0.5513 - val_loss: 1.1692 - val_accuracy: 0.6587\n",
            "\n",
            "Epoch 4/10\n",
            "750/750 - 8s - loss: 1.1473 - accuracy: 0.6091 - val_loss: 1.0119 - val_accuracy: 0.7018\n",
            "\n",
            "Epoch 5/10\n",
            "750/750 - 8s - loss: 1.0214 - accuracy: 0.6461 - val_loss: 0.9122 - val_accuracy: 0.7104\n",
            "\n",
            "Epoch 6/10\n",
            "750/750 - 8s - loss: 0.9375 - accuracy: 0.6679 - val_loss: 0.8401 - val_accuracy: 0.7131\n",
            "\n",
            "Epoch 7/10\n",
            "750/750 - 8s - loss: 0.8734 - accuracy: 0.6898 - val_loss: 0.7853 - val_accuracy: 0.7342\n",
            "\n",
            "Epoch 8/10\n",
            "750/750 - 8s - loss: 0.8248 - accuracy: 0.6998 - val_loss: 0.7443 - val_accuracy: 0.7380\n",
            "\n",
            "Epoch 9/10\n",
            "750/750 - 8s - loss: 0.7865 - accuracy: 0.7124 - val_loss: 0.7169 - val_accuracy: 0.7408\n",
            "\n",
            "Epoch 10/10\n",
            "750/750 - 8s - loss: 0.7584 - accuracy: 0.7190 - val_loss: 0.6911 - val_accuracy: 0.7464\n",
            "\n",
            "Best validation acc of epoch:\n",
            "0.7464166879653931\n",
            "Epoch 1/10\n",
            "375/375 - 4s - loss: 3.3100 - accuracy: 0.1061 - val_loss: 2.2008 - val_accuracy: 0.3117\n",
            "\n",
            "Epoch 2/10\n",
            "375/375 - 4s - loss: 3.0874 - accuracy: 0.1166 - val_loss: 2.1242 - val_accuracy: 0.5311\n",
            "\n",
            "Epoch 3/10\n",
            "375/375 - 4s - loss: 2.9271 - accuracy: 0.1244 - val_loss: 2.0628 - val_accuracy: 0.6005\n",
            "\n",
            "Epoch 4/10\n",
            "375/375 - 4s - loss: 2.8056 - accuracy: 0.1380 - val_loss: 2.0099 - val_accuracy: 0.6165\n",
            "\n",
            "Epoch 5/10\n",
            "375/375 - 4s - loss: 2.7050 - accuracy: 0.1462 - val_loss: 1.9611 - val_accuracy: 0.6354\n",
            "\n",
            "Epoch 6/10\n",
            "375/375 - 4s - loss: 2.6156 - accuracy: 0.1537 - val_loss: 1.9154 - val_accuracy: 0.6401\n",
            "\n",
            "Epoch 7/10\n",
            "375/375 - 4s - loss: 2.5262 - accuracy: 0.1624 - val_loss: 1.8711 - val_accuracy: 0.6313\n",
            "\n",
            "Epoch 8/10\n",
            "375/375 - 4s - loss: 2.4582 - accuracy: 0.1724 - val_loss: 1.8284 - val_accuracy: 0.6370\n",
            "\n",
            "Epoch 9/10\n",
            "375/375 - 4s - loss: 2.3782 - accuracy: 0.1843 - val_loss: 1.7873 - val_accuracy: 0.6469\n",
            "\n",
            "Epoch 10/10\n",
            "375/375 - 4s - loss: 2.3201 - accuracy: 0.1956 - val_loss: 1.7480 - val_accuracy: 0.6279\n",
            "\n",
            "Best validation acc of epoch:\n",
            "0.6469166874885559\n",
            "100%|██████████| 5/5 [04:28<00:00, 53.68s/it, best loss: -0.7464166879653931]\n",
            "Evalutation of best performing model:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-be25670a5098>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evalutation of best performing model:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best performing model chosen hyper-parameters:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_run\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1387\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    860\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2939\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m-> 2941\u001b[0;31m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3356\u001b[0m               call_context_key in self._function_cache.missed):\n\u001b[1;32m   3357\u001b[0m             return self._define_function_with_shape_relaxation(\n\u001b[0;32m-> 3358\u001b[0;31m                 args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[0m\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[0;34m(self, args, kwargs, flat_args, filtered_flat_args, cache_key_context)\u001b[0m\n\u001b[1;32m   3278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3279\u001b[0m     graph_function = self._create_graph_function(\n\u001b[0;32m-> 3280\u001b[0;31m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0m\u001b[1;32m   3281\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1233 test_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1224 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1217 run_step  **\n        outputs = model.test_step(data)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1183 test_step\n        y_pred = self(x, training=False)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/input_spec.py:259 assert_input_compatibility\n        ' but received input with shape ' + display_shape(x.shape))\n\n    ValueError: Input 0 of layer sequential_16 is incompatible with the layer: expected axis -1 of input shape to have value 784 but received input with shape (None, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saTjabaKDKW8"
      },
      "source": [
        "## Batch Normalization\n",
        "\n",
        "- __Normalization__ means to make different samples _more similar_ to ML models\n",
        "  - This usually makes the model more generalizable (lower variance), and consequently lower loss on test data\n",
        "  - One type we used to is __scaling__\n",
        "- The other one in ML is making most of the data following __Gaussian__ distribution (we do not do that in DL since we do not assume Guassian)\n",
        "  - z-score transformation is one of the most popular way of normalization\n",
        "- In ML, we normalize our data __before__ feeding it to the models\n",
        "  - This does not work well with DL since we can only guarantee that the input to the initial layer is normalized, but the output from that layer onwards is not.\n",
        "- Thus we use `BatchNormalization`, which is a `Layer` provided by `keras`\n",
        "  - If you want to use a __deeper__ network, you should consider using `BatchNormalization` since it helps with _gradient propagation_\n",
        "- You can use `BatchNormalization` like below:\n",
        "```python\n",
        "conv_model.add(layers.Conv2D(32, 3, activation='relu'))\n",
        "conv_model.add(layers.BatchNormalization())\n",
        "dense_model.add(layers.Dense(32, activation='relu'))\n",
        "dense_model.add(layers.BatchNormalization())\n",
        "```\n",
        "\n",
        "__Note__: refer to the textbook for the arguments of `BatchNormalization`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNb0WkJWu4c9"
      },
      "source": [
        "### Callbacks Other than `EarlyStopping`\n",
        "\n",
        "- We already know that we can use the `EarlyStopping` callback provided by `keras` to avoid __overfitting__ our models.\n",
        "- But besides it, there are other types of callbacks that are useful in different scenarios\n",
        "- You can refer to the textbook, or the [keras docs](https://keras.io/api/callbacks/) for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5qZQ7PBwFHY"
      },
      "source": [
        "# Deep Learning & Artificial Intelligence\n",
        "## Advanced Tricks and Latest Developments with Deep Learning\n",
        "### Dr. Jie Tao, Fairfield University"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jx6WDLGjwG7A"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}