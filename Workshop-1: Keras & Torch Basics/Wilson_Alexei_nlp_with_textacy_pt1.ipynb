{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9QEeEvkRJS_"
      },
      "source": [
        "# NATURAL LANGUAGE PROCESSING WITH TEXTACY & SPACY\n",
        "\n",
        "__Spacy__ is a very high performance NLP library for doing several tasks of NLP with ease and speed. Let us explore another library built on top of __SpaCy__ called __TextaCy__.\n",
        "\n",
        "## TEXTACY\n",
        "+ Textacy is a Python library for performing higher-level natural language processing (NLP) tasks,\n",
        "built on the high-performance Spacy library.\n",
        "+ Textacy focuses on tasks facilitated by the availability of tokenized, POS-tagged, and parsed text.\n",
        "+ Uses\n",
        "    + Text preprocessing\n",
        "    + Keyword in Context\n",
        "    + Topic modeling\n",
        "    + Information Extraction\n",
        "    + Keyterm extraction,\n",
        "    + Text and Readability statistics,\n",
        "    + Emotional valence analysis,\n",
        "    + Quotation attribution\n",
        "\n",
        "### INSTALLATION\n",
        "You can install using `pip install textacy` or `conda install -c conda-forge textacy`.\n",
        "NB: In case you are having issues with installing on windows you can use conda instead of pip.\n",
        "\n",
        "### Downloading Dataset\n",
        "You can use the following command to download the `capitol_words` dataset, whcih we will use in this tutorial.\n",
        "`python -m textacy download capital_words`\n",
        "\n",
        "<!--### FOR LANGUAGE DETECTION\n",
        "You can either use `pip install textacy[lang]` or `pip install cld2-cffi` to install the required language pack for textacy.\n",
        "\n",
        "__NOTE__: All required the package, dependencies, and add-on packs are pre-installed for this tutorial.-->\n",
        "\n",
        "## Getting Started"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_hRzaSIzDhy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d302178e-f0f6-4340-913e-5ca547fa2d41"
      },
      "source": [
        "!pip install textacy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting textacy\n",
            "  Downloading textacy-0.13.0-py3-none-any.whl (210 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (5.3.0)\n",
            "Requirement already satisfied: catalogue~=2.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (2.0.8)\n",
            "Collecting cytoolz>=0.10.1 (from textacy)\n",
            "  Downloading cytoolz-0.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting floret~=0.10.0 (from textacy)\n",
            "  Downloading floret-0.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (314 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.7/314.7 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jellyfish>=0.8.0 (from textacy)\n",
            "  Downloading jellyfish-0.11.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (1.2.0)\n",
            "Requirement already satisfied: networkx>=2.7 in /usr/local/lib/python3.10/dist-packages (from textacy) (3.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (1.22.4)\n",
            "Collecting pyphen>=0.10.0 (from textacy)\n",
            "  Downloading pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (2.27.1)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (1.2.2)\n",
            "Requirement already satisfied: spacy~=3.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (3.5.2)\n",
            "Requirement already satisfied: tqdm>=4.19.6 in /usr/local/lib/python3.10/dist-packages (from textacy) (4.65.0)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from cytoolz>=0.10.1->textacy) (0.12.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->textacy) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->textacy) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->textacy) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->textacy) (3.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->textacy) (3.1.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (2.4.6)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (6.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy~=3.0->textacy) (4.5.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy~=3.0->textacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy~=3.0->textacy) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy~=3.0->textacy) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy~=3.0->textacy) (2.1.2)\n",
            "Installing collected packages: pyphen, jellyfish, floret, cytoolz, textacy\n",
            "Successfully installed cytoolz-0.12.1 floret-0.10.3 jellyfish-0.11.2 pyphen-0.14.0 textacy-0.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-AXQqy3RJTE"
      },
      "source": [
        "# Loading Packages\n",
        "import textacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVyj0wtNRJTF"
      },
      "source": [
        "example = \"Textacy is a Python library for performing higher-level natural language processing (NLP) tasks, \\\n",
        "built on the high-performance Spacy library. With the basics — tokenization, part-of-speech tagging, parsing \\\n",
        "— offloaded to another library, textacy focuses on tasks facilitated by the availability of tokenized, POS-tagged, \\\n",
        "and parsed text: keyterm extraction, readability statistics, emotional valence analysis, quotation attribution, \\\n",
        "and more.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vH4pEG6YRJTF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "a47e3e82-1592-48df-88a4-2de9c48d2413"
      },
      "source": [
        "example"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Textacy is a Python library for performing higher-level natural language processing (NLP) tasks, built on the high-performance Spacy library. With the basics — tokenization, part-of-speech tagging, parsing — offloaded to another library, textacy focuses on tasks facilitated by the availability of tokenized, POS-tagged, and parsed text: keyterm extraction, readability statistics, emotional valence analysis, quotation attribution, and more.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NWcX76URJTF"
      },
      "source": [
        "## TEXT PREPROCESSING WITH TEXTACY\n",
        "Following methods can be used to preprocess your text data:\n",
        "\n",
        "+ `textacy.preprocess_text()`\n",
        "+ `textacy.preprocess.`\n",
        "    + Punctuation Lowercase\n",
        "    + Urls\n",
        "    + Phone numbers\n",
        "    + Currency\n",
        "    + Emails\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8w35GA0-RJTG"
      },
      "source": [
        "raw_text = \"\"\" The best programs, are the ones written when the programmer is supposed to be working on something else.\\\n",
        "Mike bought the book for $50 although in Paris it will cost $30 dollars. Don’t document the problem, \\\n",
        "fix it.This is from https://twitter.com/codewisdom?lang=en. \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnfG-0rMRJTG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "7bf71070-ea4d-49d6-85ad-8ba21bc44dd7"
      },
      "source": [
        "# Removing urls\n",
        "from textacy import preprocessing\n",
        "processed_text = preprocessing.replace.urls(raw_text,repl='TWITTER')\n",
        "processed_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The best programs, are the ones written when the programmer is supposed to be working on something else.Mike bought the book for $50 although in Paris it will cost $30 dollars. Don’t document the problem, fix it.This is from TWITTER '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MckKhKi9RJTG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "55c99394-c08b-4565-9d36-eb963bb68aa7"
      },
      "source": [
        "# Removing Punctuation and Uppercase\n",
        "\n",
        "processed_text = preprocessing.remove.punctuation(processed_text)\n",
        "processed_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The best programs  are the ones written when the programmer is supposed to be working on something else Mike bought the book for $50 although in Paris it will cost $30 dollars  Don t document the problem  fix it This is from TWITTER '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqqZmr5JRJTH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "6b2ee553-45da-4553-df1c-e19235e5b3e0"
      },
      "source": [
        "# Replacing Currency Symbols\n",
        "processed_text = preprocessing.replace.currency_symbols(processed_text,repl='USD')\n",
        "processed_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The best programs  are the ones written when the programmer is supposed to be working on something else Mike bought the book for USD50 although in Paris it will cost USD30 dollars  Don t document the problem  fix it This is from TWITTER '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "By3kbsR9RJTH"
      },
      "source": [
        "Notice that we created a variable `processed_text` in every cell block above? That is because that usually text preprocessing is a pipeline - with multiple steps in it. Here are the steps we completed above:\n",
        "+ Removing Punctuation and Uppercase\n",
        "+ Removing urls\n",
        "+ Replacing Currency Symbols\n",
        "\n",
        "So we are using the variable to pass text data from each step to the next.\n",
        "\n",
        "There are much more text preprocessing steps. [Here](https://www.kdnuggets.com/2019/04/text-preprocessing-nlp-machine-learning.html) is a good summary of these steps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmWylmQxRJTI"
      },
      "source": [
        "Refer to the [docs](https://textacy.readthedocs.io/en/0.10.1/api_reference/text_processing.html) for more details of the `textacy.preprocess` and its family methods.\n",
        "\n",
        "### READING A TEXT OR A DOCUMENT\n",
        "+ `textacy.Doc(your_text)`\n",
        "+ `textacy.io.read_text(your_text)`\n",
        "\n",
        "Textacy would not receive a lot of attractions if it only can remove URLs or punctuations; however, all additional/more advanced techniques/analyses required on `formatting` the data.\n",
        "\n",
        "TextaCy/SpaCy uses a `Doc` as a container for any text objects. [Here](https://textacy.readthedocs.io/en/0.10.1/api_reference/lang_doc_corpus.html) is nice documentation of the `doc` object.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-cuRivr1g9I",
        "outputId": "daf70579-f706-4d88-94df-eafb67368f89"
      },
      "source": [
        "import spacy.cli\n",
        "spacy.cli.download(\"en_core_web_sm\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PXXEDsbRJTI"
      },
      "source": [
        "# With SpacyDoc\n",
        "# Requires Language Pkg Model\n",
        "# nlp = spacy.load(\"en_core_web_sm\")\n",
        "docx_textacy = textacy.make_spacy_doc(example, lang='en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkmTMHgRRJTI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fcdcaf3-ebaa-41af-9919-734c3678d8d3"
      },
      "source": [
        "docx_textacy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Textacy is a Python library for performing higher-level natural language processing (NLP) tasks, built on the high-performance Spacy library. With the basics — tokenization, part-of-speech tagging, parsing — offloaded to another library, textacy focuses on tasks facilitated by the availability of tokenized, POS-tagged, and parsed text: keyterm extraction, readability statistics, emotional valence analysis, quotation attribution, and more."
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7x2EBytRJTJ"
      },
      "source": [
        "We can look at the `type` of the `docx_textacy` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggAHBf1oRJTJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf608fb9-98d8-42ad-bc77-c2a8c6a52493"
      },
      "source": [
        "type(docx_textacy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.tokens.doc.Doc"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2In25K7RJTJ"
      },
      "source": [
        "Following code read a `doc` from a local file:\n",
        "\n",
        "1. use the .read() method\n",
        "`file_textacy = textacy.Doc(open(\"example.txt\").read())`\n",
        "\n",
        "2. create a generator\n",
        "`file_textacy2 = textacy.io.read_text('example.txt',lines=True)`\n",
        "\n",
        "then:\n",
        "\n",
        "`for text in file_textacy2:`\n",
        "\n",
        "    `docx_file = textacy.Doc(text)`\n",
        "    \n",
        "    `print(docx_file)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfwe0YaxRJTJ"
      },
      "source": [
        "### Advanced Text Analytics\n",
        "\n",
        "1. Named-Entity Recognition\n",
        "\n",
        "Named-entity recognition (NER) (also known as entity identification, entity chunking and entity extraction) is a subtask of information extraction that seeks to locate and classify named entity mentions in unstructured text into pre-defined categories such as the person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc. [Source: [Wikipedia](https://en.wikipedia.org/wiki/Named-entity_recognition)]\n",
        "\n",
        "TextaCy has a built-in method for NER."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IpUpjcqRJTJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfd97327-530d-47cb-81b7-cfadda7b2c07"
      },
      "source": [
        "# Using Textacy Named Entity Extraction\n",
        "list(textacy.extract.entities(docx_textacy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Textacy, NLP, Spacy]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sb4jj2FPRJTJ"
      },
      "source": [
        "2. n-grams\n",
        "\n",
        "In the fields of computational linguistics and probability, an n-gram is a contiguous sequence of n items from a given sample of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application. The n-grams typically are collected from a text or speech corpus. When the items are words, n-grams may also be called shingles.\n",
        "\n",
        "TextaCy has a built-in method for n-grams.\n",
        "\n",
        "N-grams, a.k.a __Bag-of-Words__, is a very important quantifying approach for text data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dC99xgRHRJTK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c84d0a16-0efd-45c3-ab70-625a0708fbfa"
      },
      "source": [
        "# NGrams with Textacy\n",
        "# NB SpaCy method would be to use noun Phrases\n",
        "# Tri Grams\n",
        "\n",
        "list(textacy.extract.ngrams(docx_textacy,3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[library for performing,\n",
              " level natural language,\n",
              " natural language processing,\n",
              " performance Spacy library,\n",
              " focuses on tasks,\n",
              " availability of tokenized,\n",
              " emotional valence analysis]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akYy6o3FRJTK"
      },
      "source": [
        "3. text statistics\n",
        "This usually includes computing basic counts and various readability statistics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzpSQFgY2lV4"
      },
      "source": [
        "from textacy.text_stats import TextStats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_3j2V8cRJTK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af2245d9-4f45-4c10-b71a-95b95ec3dc60"
      },
      "source": [
        "ts = TextStats(docx_textacy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/textacy/text_stats/api.py:89: DeprecationWarning: The `TextStats` class is deprecated as of v0.12. Instead, call the stats functions directly -- `text_stats.TextStats(doc).n_sents` => `text_stats.n_sents(doc)` --or set them as custom doc extensions and access them on the ``Doc`` -- `textacy.set_doc_extensions('text_stats'); doc._.n_sents` .\n",
            "  utils.deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPjDHso6RJTK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "019cbaae-251e-4a45-9d48-182a240212d1"
      },
      "source": [
        "# Number of words\n",
        "ts.n_words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUvz5xDiRJTL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0357d024-788a-47cb-8234-b71f3657d29e"
      },
      "source": [
        "# Basic counts of unuque words\n",
        "ts.n_unique_words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46-5vrwARJTL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "outputId": "6af83bf1-40a2-4a34-efbb-c21129c4c734"
      },
      "source": [
        "# readability scores\n",
        "ts.flesch_kincaid_grade_level"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-2746c799839f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# readability scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflesch_kincaid_grade_level\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'TextStats' object has no attribute 'flesch_kincaid_grade_level'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5-LjKlIRJTL"
      },
      "source": [
        "Some of these basic counts and readability stats seem intimidating. Feel free to Google them to better understand them.\n",
        "\n",
        "4. Dealing with a collection of documents (corpus)\n",
        "\n",
        "Many NLP tasks require datasets comprised of a large number of texts, which are often stored on disk in one or multiple files. textacy makes it easy to efficiently stream text and (text, metadata) pairs from disk, regardless of the format or compression of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfQGQNoxRJTL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82b3fe88-7c6d-49fb-a19f-a811083a640c"
      },
      "source": [
        "import textacy.datasets  # note the import\n",
        "ds = textacy.datasets.CapitolWords()\n",
        "ds.download()\n",
        "records = ds.records(speaker_name={\"Hillary Clinton\", \"Barack Obama\"})\n",
        "next(records)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11.9M/11.9M [00:00<00:00, 62.1MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Record(text='I yield myself 15 minutes of the time controlled by the Democrats.', meta={'date': '2001-02-13', 'congress': 107, 'speaker_name': 'Hillary Clinton', 'speaker_party': 'D', 'title': 'MORNING BUSINESS', 'chamber': 'Senate'})"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4Oxnnc1RJTL"
      },
      "source": [
        "A `textacy.Corpus` is an ordered collection of spaCy Doc s, all processed by the same language pipeline. Let’s continue with the Capitol Words dataset and make a corpus from a stream of records. (Note: This may take a few minutes.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSycPGx3RJTL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24f1bc4c-46b1-4b64-aefe-2904a4388144"
      },
      "source": [
        "cw = textacy.datasets.CapitolWords()\n",
        "records = cw.records(limit=100)\n",
        "spacy_lang = textacy.load_spacy_lang(\"en_core_web_sm\", disable=(\"parser\",))\n",
        "corpus = textacy.Corpus(spacy_lang, data=records)\n",
        "print(corpus)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus(100 docs, 70562 tokens)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DJnS_g-5Kae"
      },
      "source": [
        "Say we only want the speeches from Mr. Bernie Sanders:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lkg0fmY5IUL",
        "outputId": "78f70c39-5576-4d47-ecc8-915c2a951dc3"
      },
      "source": [
        "for text, meta in ds.records(speaker_name=\"Bernie Sanders\", limit=3):\n",
        "    print(\"\\n{}, {}\\n{}\".format(meta[\"title\"], meta[\"date\"], text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "JOIN THE SENATE AND PASS A CONTINUING RESOLUTION, 1996-01-04\n",
            "Mr. Speaker, 480,000 Federal employees are working without pay, a form of involuntary servitude; 280,000 Federal employees are not working, and they will be paid. Virtually all of these workers have mortgages to pay, children to feed, and financial obligations to meet.\n",
            "Mr. Speaker, what is happening to these workers is immoral, is wrong, and must be rectified immediately. Newt Gingrich and the Republican leadership must not continue to hold the House and the American people hostage while they push their disastrous 7-year balanced budget plan. The gentleman from Georgia, Mr. Gingrich, and the Republican leadership must join Senator Dole and the entire Senate and pass a continuing resolution now, now to reopen Government.\n",
            "Mr. Speaker, that is what the American people want, that is what they need, and that is what this body must do.\n",
            "\n",
            "DISPOSING OF SENATE AMENDMENT TO H.R. 1643, EXTENSION OF MOST-FAVORED- NATION TREATMENT FOR BULGARIA, 1996-01-05\n",
            "Mr. Speaker, I thank the gentleman for yielding time to me.\n",
            "Mr. Speaker, this legislation is both good news and bad news. The good news is that the Republicans have finally decided to stop holding three-quarters of a million American Federal workers as hostage. In Vermont we have close to 2,000 Federal workers who are working today but are not getting paid. These people have mortgages to pay. They have children to feed. They have financial obligations to meet. It is immoral. It is wrong to hold them and every other Federal Worker who is furloughed and not being paid as hostage.\n",
            "It is also wrong to hold millions of Americans who need passports, who need environmental protection, who need Meals on Wheels, who need all of the services that this Government should be providing as hostage, who have paid for these services but are not getting them.\n",
            "The bad news is that, while Federal workers will be paid, many of them will not be given the resources that they need to do their jobs properly. That is insane. Why do we give people the money to go to work but then not allow them the resources to properly fulfill their function?\n",
            "Mr. Speaker, the truth of the matter is that this is not a debate about a 7-year balanced budget. If our Republican friends were serious about balancing the budget in 7 years, which I think can be done, they would not be spending $50 billion more on defense spending despite the end of the cold war. They would not be spending more money on the CIA despite the end of the cold war. They would not be giving huge tax breaks to the rich when the richest people in America today are richer than they have ever been before.\n",
            "\n",
            "EXAMINING THE SPEAKER'S UPCOMING TRAVEL SCHEDULE, 1996-01-05\n",
            "Mr. Speaker, if we want to understand why in this country the richest people are becoming richer while most working people are seeing a decline in their standard of living, if we want to understand why the Contract With America provides for huge tax breaks for the wealthiest people and the largest corporations while it cuts back massively on programs for the elderly, working people, and low-income people, we might want to examine Newt Gingrich's travel schedule for the coming week.\n",
            "Mr. Gingrich will be in Seattle, WA, where he will have dinner with his colleagues and his friends for the Washington State Republican Party for $1,000 each. He will be in Dallas, TX, for a dinner for only $10,000 apiece. He will be in Dearborn, MI, for another private fireside reception at $10,000.\n",
            "Who goes to these events? Most people that I know do not spend $1,000 for a dinner.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XJr6F6ERJTM"
      },
      "source": [
        "You can filter the corpus using certain conditions, which would cover your specific use cases:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aviYpYm-RJTM"
      },
      "source": [
        "Corpus is a list-like object that can be iterated on - each element in Corpus is a `textacy.Doc` object.\n",
        "\n",
        "Which means we can do slicing as we slice any list in Python:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8yjbpyGRJTM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17754e64-3183-4375-c555-2570313a9393"
      },
      "source": [
        "# any element\n",
        "corpus[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Mr. Speaker, 480,000 Federal employees are working without pay, a form of involuntary servitude; 280,000 Federal employees are not working, and they will be paid. Virtually all of these workers have mortgages to pay, children to feed, and financial obligations to meet.\n",
              "Mr. Speaker, what is happening to these workers is immoral, is wrong, and must be rectified immediately. Newt Gingrich and the Republican leadership must not continue to hold the House and the American people hostage while they push their disastrous 7-year balanced budget plan. The gentleman from Georgia, Mr. Gingrich, and the Republican leadership must join Senator Dole and the entire Senate and pass a continuing resolution now, now to reopen Government.\n",
              "Mr. Speaker, that is what the American people want, that is what they need, and that is what this body must do."
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o9CfeY_RJTM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "161f580b-17ea-4376-c669-38751bac2792"
      },
      "source": [
        "# a sub-list\n",
        "[doc for doc in corpus[:3]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Mr. Speaker, 480,000 Federal employees are working without pay, a form of involuntary servitude; 280,000 Federal employees are not working, and they will be paid. Virtually all of these workers have mortgages to pay, children to feed, and financial obligations to meet.\n",
              " Mr. Speaker, what is happening to these workers is immoral, is wrong, and must be rectified immediately. Newt Gingrich and the Republican leadership must not continue to hold the House and the American people hostage while they push their disastrous 7-year balanced budget plan. The gentleman from Georgia, Mr. Gingrich, and the Republican leadership must join Senator Dole and the entire Senate and pass a continuing resolution now, now to reopen Government.\n",
              " Mr. Speaker, that is what the American people want, that is what they need, and that is what this body must do.,\n",
              " Mr. Speaker, a relationship, to work and survive, has got to be honest and we have got to deal with each other in good faith. For a government to govern well, we have to be honest and we have to deal with each other in good faith.\n",
              " The President has vetoed every measure we have sent to him that would balance the budget. He has a constitutional right to do that. If he believes that our budget devastates the elderly, he has a moral obligation to fight us. I will never, never say bad things about somebody that follows their beliefs because that is what they should do. There comes a time, though, that one has an obligation to do more than just say no.\n",
              " Mr. President, if you do not like our view of a balanced budget, give us your view. We cannot negotiate against ourselves anymore. You have a legal and a moral obligation to fight us when you think we are wrong. You have a legal and moral obligation to fulfill your commitment you made 40 days ago to put a budget on the table that balances. Please fulfill your obligation.,\n",
              " Mr. Speaker, I thank the gentleman for yielding time to me.\n",
              " Mr. Speaker, this legislation is both good news and bad news. The good news is that the Republicans have finally decided to stop holding three-quarters of a million American Federal workers as hostage. In Vermont we have close to 2,000 Federal workers who are working today but are not getting paid. These people have mortgages to pay. They have children to feed. They have financial obligations to meet. It is immoral. It is wrong to hold them and every other Federal Worker who is furloughed and not being paid as hostage.\n",
              " It is also wrong to hold millions of Americans who need passports, who need environmental protection, who need Meals on Wheels, who need all of the services that this Government should be providing as hostage, who have paid for these services but are not getting them.\n",
              " The bad news is that, while Federal workers will be paid, many of them will not be given the resources that they need to do their jobs properly. That is insane. Why do we give people the money to go to work but then not allow them the resources to properly fulfill their function?\n",
              " Mr. Speaker, the truth of the matter is that this is not a debate about a 7-year balanced budget. If our Republican friends were serious about balancing the budget in 7 years, which I think can be done, they would not be spending $50 billion more on defense spending despite the end of the cold war. They would not be spending more money on the CIA despite the end of the cold war. They would not be giving huge tax breaks to the rich when the richest people in America today are richer than they have ever been before.]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZypcy67RJTM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67731c1d-587f-4f33-e35a-1ba22997868f"
      },
      "source": [
        "# You can delete elements from `corpus`\n",
        "del corpus[:10]\n",
        "corpus"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<textacy.corpus.Corpus at 0x7f07934e2620>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1A4LoteKRJTM"
      },
      "source": [
        "We can also get basic statistics of the `corpus` object:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvUszv2RRJTN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d20078b-6001-4213-d4a6-96b03a260e60"
      },
      "source": [
        "corpus.n_docs, corpus.n_sents, corpus.n_tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(90, 0, 67217)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjW7eSdqRJTN"
      },
      "source": [
        "# Word Counts - as a dictionary\n",
        "counts = corpus.word_counts()\n",
        "# word_freqs(weighting='count', as_strings=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWDLmZAPRJTN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ae2119a-fa1c-4685-abe1-e2c70fa459c2"
      },
      "source": [
        "# we can get the top-5 frequent words form the `counts` dictionary\n",
        "sorted(counts.items(), key=lambda x: x[1], reverse=True)[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(8004577259940138793, 243),\n",
              " (15275761157247972012, 240),\n",
              " (7593739049417968140, 238),\n",
              " (14889849580704678361, 210),\n",
              " (8021635565988888124, 209)]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_ajPXOXRJTN"
      },
      "source": [
        "We also introduce a new text metric called __term frequency - inversed document frequency__ (`tf-idf`).\n",
        "\n",
        "In information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. tf–idf is one of the most popular term-weighting schemes today; 83% of text-based recommender systems in digital libraries use tf–idf.[Source: Wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf).\n",
        "\n",
        "Term frequency (`tf`) in `tf-idf` is the word frequency we get from above dictionary (`counts`). Now we need to calculate the `idf` part.\n",
        "\n",
        "The inverse document frequency is a measure of how much information the word provides, i.e., if it's common or rare across all documents. It is the logarithmically scaled inverse fraction of the documents that contain the word (obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xo8HKjNRJTN"
      },
      "source": [
        "\\begin{equation*}\n",
        "idf(t, D) = log(\\frac{N_D}{N_t})\n",
        "\\end{equation*}\n",
        "\n",
        "In which, $ N_D $ is the number of documents (`doc`) in `corpus` D; and $ N_t $ is the number of `doc`s in $ D $ containing term $ t $.\n",
        "\n",
        "Looks complicated, right? Fortunately we can use TextaCy's built-in methods to calculate `idf`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEefeDT5RJTN"
      },
      "source": [
        "idf = corpus.word_doc_counts(weighting='idf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNv8DNtwRJTO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc7d35d4-4a87-4006-c753-5bf3e4470ec5"
      },
      "source": [
        "sorted(idf.items(), key=lambda x: x[1], reverse=True)[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(15689657794018624313, 4.51085950651685),\n",
              " (10480242113835854118, 4.51085950651685),\n",
              " (9211202706150281085, 4.51085950651685),\n",
              " (5971168095749524238, 4.51085950651685),\n",
              " (17101725512904321536, 4.51085950651685)]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UU-Luej4RJTO"
      },
      "source": [
        "Since now we have both __tf__ as a dict object `counts`; and idf as a dict object `idf`, we can calculate the complete __tf-idf__ metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kq5j5jAORJTO"
      },
      "source": [
        "tf_idf = {k: counts[k]/idf[k] for k in idf.keys() & counts}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdJT9La4RJTO"
      },
      "source": [
        "### YOUR TURN HERE\n",
        "\n",
        "Please print out the top-20 terms with the highest `tf-idf` values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4N00CypRJTO"
      },
      "source": [
        "#### Complete your code here\n",
        "store = list(tf_idf.keys())[:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhmSnC7tRJTO"
      },
      "source": [
        "Some of above results make sense, such as 'president' and 'bill' and 'act'. But terms like '-PRON-' (referring to pronouns such as 'you' or 'I') and ''s' do not make sense. Similar conclusion can be drawn onto words such as 'a', 'an', 'the', ...\n",
        "\n",
        "These words are called __stop words__. In computing, stop words are words which are filtered out before or after processing of natural language data (text). Though \"stop words\" usually refers to the most common words in a language, there is no single universal list of stop words used by all natural language processing tools, and indeed not all tools even use such a list. Some tools specifically avoid removing these stop words to support phrase search. [Source: Wikipedia](https://en.wikipedia.org/wiki/Stop_words).\n",
        "\n",
        "We can check if a word (a.k.a. token) is stop by using the `is_stop` attribute provided with `textacy.token` object.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pw8uGt33RJTP"
      },
      "source": [
        "my_doc = corpus[0]\n",
        "\n",
        "for token in my_doc:\n",
        "  print(token, token.is_stop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO-kfQGRRJTP"
      },
      "source": [
        "### YOUR TURN HERE\n",
        "\n",
        "Remove all stop words from `my_doc`.\n",
        "\n",
        "__HINT__: using an `if` statement (with the `is_stop` attribute) in the `for` loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "HYQt5AxDRJTP"
      },
      "source": [
        "#### Complete your code here\n",
        "filtered_doc = []\n",
        "for word in my_doc:\n",
        "    if not word.is_stop:\n",
        "        filtered_doc.append(word.text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_doc[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S51wIO5QtDp4",
        "outputId": "a5b6e323-6575-4daf-88a0-4cc6078dd0f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Mr.',\n",
              " 'Speaker',\n",
              " ',',\n",
              " 'unavoidably',\n",
              " 'absent',\n",
              " 'votes',\n",
              " 'default',\n",
              " 'legislation',\n",
              " '.',\n",
              " 'present']"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPIgr6t8RJTP"
      },
      "source": [
        "Sometimes we only care about __word tokens__, which means we need to filter out _numbers_, _punctuations_, and so forth.\n",
        "\n",
        "TextaCy provides a `is_alpha` attribute for that purpose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8poFj_qvRJTP"
      },
      "source": [
        "for token in my_doc:\n",
        "    print(token, token.is_alpha)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-C04VSNFRJTP"
      },
      "source": [
        "### YOUR TURN HERE\n",
        "\n",
        "What if we want non-stop and word tokens?\n",
        "\n",
        "__HINT__: combine the two above steps together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "RXwbdOI9RJTP"
      },
      "source": [
        "#### Complete you code here\n",
        "filtered_tokens = []\n",
        "for token in my_doc:\n",
        "    if not token.is_stop:\n",
        "        filtered_tokens.append(token)\n",
        "\n",
        "# Join the filtered tokens back into a string\n",
        "filtered_doc = \" \".join([token.text for token in filtered_tokens])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_doc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "fyqpTV4MuYDC",
        "outputId": "bb6ba48f-2b27-4d27-827f-d46caa19f3c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Mr. Speaker , unavoidably absent votes default legislation . present , voted \" nay \" motions table appeal ruling Chair regards resolutions offered Mr. Gephardt ( rollcall . 26 ) Ms. Jackson - Lee ( rollcall . 27 ) , voted \" nay \" ordering previous question House Resolution 355 ( rollcall . 28 ) . voted \" nay \" H. Con . Res . 141 ( rollcall . 29 ) . voted \" yea \" H.R. 2924 ( rollcall . 30 ) .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fm-6c0hRJTP"
      },
      "source": [
        "From the results, have you noticed that different forms of the same token may appear in the text? For instance, 'run', 'ran', and 'running' are all different forms of the root word 'run'. Counting them as different words may bias any subsequent model. Thus, it would be ideal to make different forms of the same word to the root. This process is called __lemmatization__.\n",
        "\n",
        "_Lemmatization_ usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the _lemma_. If confronted with the token saw, stemming might return just s, whereas lemmatization would attempt to return either see or saw depending on whether the use of the token was as a verb or a noun. The two may also differ in that stemming most commonly collapses derivationally related words, whereas lemmatization commonly only collapses the different inflectional forms of a lemma. Linguistic processing for stemming or lemmatization is often done by an additional plug-in component to the indexing process, and a number of such components exist, both commercial and open-source. [Source: Stanford NLP Group](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)\n",
        "\n",
        "TextaCy provides a `lemma_` attribute for `token` object for exactly this purpose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQc73DwgRJTQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b682d1a2-dcd0-45cb-deff-27e7e12dadf7"
      },
      "source": [
        "for token in my_doc:\n",
        "    print(token, token.lemma_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mr. Mr.\n",
            "Speaker Speaker\n",
            ", ,\n",
            "I I\n",
            "was be\n",
            "unavoidably unavoidably\n",
            "absent absent\n",
            "during during\n",
            "the the\n",
            "votes vote\n",
            "on on\n",
            "default default\n",
            "legislation legislation\n",
            ". .\n",
            "If if\n",
            "I I\n",
            "had have\n",
            "been be\n",
            "present present\n",
            ", ,\n",
            "I I\n",
            "would would\n",
            "have have\n",
            "voted vote\n",
            "\" \"\n",
            "nay nay\n",
            "\" \"\n",
            "on on\n",
            "the the\n",
            "motions motion\n",
            "to to\n",
            "table table\n",
            "the the\n",
            "appeal appeal\n",
            "of of\n",
            "the the\n",
            "ruling ruling\n",
            "of of\n",
            "the the\n",
            "Chair Chair\n",
            "with with\n",
            "regards regard\n",
            "to to\n",
            "the the\n",
            "resolutions resolution\n",
            "offered offer\n",
            "by by\n",
            "Mr. Mr.\n",
            "Gephardt Gephardt\n",
            "( (\n",
            "rollcall rollcall\n",
            "No No\n",
            ". .\n",
            "26 26\n",
            ") )\n",
            "and and\n",
            "Ms. Ms.\n",
            "Jackson Jackson\n",
            "- -\n",
            "Lee Lee\n",
            "( (\n",
            "rollcall rollcall\n",
            "No no\n",
            ". .\n",
            "27 27\n",
            ") )\n",
            ", ,\n",
            "I I\n",
            "would would\n",
            "have have\n",
            "voted vote\n",
            "\" \"\n",
            "nay nay\n",
            "\" \"\n",
            "on on\n",
            "the the\n",
            "ordering ordering\n",
            "of of\n",
            "the the\n",
            "previous previous\n",
            "question question\n",
            "on on\n",
            "House House\n",
            "Resolution Resolution\n",
            "355 355\n",
            "( (\n",
            "rollcall rollcall\n",
            "No no\n",
            ". .\n",
            "28 28\n",
            ") )\n",
            ". .\n",
            "I I\n",
            "would would\n",
            "have have\n",
            "voted vote\n",
            "\" \"\n",
            "nay nay\n",
            "\" \"\n",
            "on on\n",
            "H. H.\n",
            "Con Con\n",
            ". .\n",
            "Res Res\n",
            ". .\n",
            "141 141\n",
            "( (\n",
            "rollcall rollcall\n",
            "No no\n",
            ". .\n",
            "29 29\n",
            ") )\n",
            ". .\n",
            "I I\n",
            "would would\n",
            "have have\n",
            "voted vote\n",
            "\" \"\n",
            "yea yea\n",
            "\" \"\n",
            "on on\n",
            "H.R. H.R.\n",
            "2924 2924\n",
            "( (\n",
            "rollcall rollcall\n",
            "No No\n",
            ". .\n",
            "30 30\n",
            ") )\n",
            ". .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbD-qmnDRJTQ"
      },
      "source": [
        "# Exercise\n",
        "\n",
        "In this exercise, you are going to complete following tasks.\n",
        "1. From the `corpus` variable, generate a new list named `jb_lst` that contains all speeches from `Joseph Biden`. (__HINT__: use similar code as we filter `cw` for 'Bernie Sanders'.)\n",
        "2. Select first 5 speeches (`doc`) from `jb_lst` and store them in a new list named `jb_selected`.\n",
        "3. For each element in `jb_selected`, print out:\n",
        "    a. Named Entities (`named_entities()`)\n",
        "    b. Text Statistics (`TextStats()`)\n",
        "4. For each element in `jb_selected`, print out the top 20 words based on their tf-idf score.\n",
        "5. For each element in `jb_selected`, print out __lemmas__ (`lemma_`) for each token if it is not a stop word (`is_stop`) and is a word token (`is_alpha`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9d_8cFaRJTQ"
      },
      "source": [
        "#### Complete your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kgZnsb-RJTQ"
      },
      "source": [
        "This is the end of part 1. We will resume on text analytics (NLP) after the break."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92s_N152RJTQ"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}