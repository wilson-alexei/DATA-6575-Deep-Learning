{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91bO2AM5UQjE"
      },
      "source": [
        "# Deep Learning for NLP: Sentiment Analysis\n",
        "\n",
        "Natural Language Processing (NLP) is a field of Artificial Intelligence that gives the machines the ability to read, understand and derive meaning from human languages [Source](https://towardsdatascience.com/your-guide-to-natural-language-processing-nlp-48ea2511f6e1).\n",
        "\n",
        "It is a discipline that focuses on the interaction between data science and human language, and is scaling to lots of industries. Today NLP is booming thanks to the huge improvements in the access to data and the increase in computational power, which are allowing practitioners to achieve meaningful results in areas like healthcare, media, finance and human resources, among others.\n",
        "\n",
        "One of the most important applications in NLP is sentiment analysis. Sentiment analysis is a text analysis method that detects polarity (e.g. a positive or negative opinion) within text, whether a whole document, paragraph, sentence, or clause.\n",
        "\n",
        "Understanding people’s emotions is essential for businesses since customers are able to express their thoughts and feelings more openly than ever before. By automatically analyzing customer feedback, from survey responses to social media conversations, brands are able to listen attentively to their customers, and tailor products and services to meet their needs.\n",
        "\n",
        "In this tutorial, we are going to discuss how to implement sentiment analysis. Firstly, we need to gain an understanding of what is sentiment analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhmmb8C5VXKH"
      },
      "source": [
        "## What is Sentiment Analysis?\n",
        "\n",
        "In particular, when we talk about sentiment analysis nowadays, we refer to the sentiment analysis with machine learning techniques. A sentiment analysis task is usually modeled as a classification problem, whereby a classifier is fed a text and returns a category, e.g. positive, negative, or neutral.\n",
        "\n",
        "Here’s how a machine learning classifier can be implemented:\n",
        "\n",
        "![Sentiment Analysis](https://miro.medium.com/max/351/1*jBxNVEethCnoR7LHqp9yRg.png)\n",
        "\n",
        "In the following sections we are going to explain the respective steps in the figure above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLgTvg9eWJNk"
      },
      "source": [
        "### The Training and Prediction Processes\n",
        "In the training process (a), our model learns to associate a particular input (i.e. a _sentence_ or _paragraph_) to the corresponding output (_tag_ meaning sentiment) based on the training samples used for training. The feature extractor transfers the each text input (again, a sentence or paragraph) into a __feature vector__. Pairs of feature vectors and tags (e.g. _positive, negative_, or _neutral_) are then fed into the machine learning algorithm to generate a model.\n",
        "\n",
        "In the prediction process (b), the feature extractor is used to transform unseen text inputs (test set) into feature vectors. These feature vectors are then fed into the model, which generates predicted tags (again, _positive, negative_, or _neutral_)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CSnKr-JXNjZ"
      },
      "source": [
        "### Feature Extraction from Text\n",
        "The first step in a machine learning text classifier is to transform the text extraction or text vectorization, and the classical approach has been bag-of-words or bag-of-ngrams with their frequency.\n",
        "\n",
        "More recently, new feature extraction techniques have been applied based on word embeddings (also known as word vectors [word2vec](https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa)). This kind of representations makes it possible for words with similar meaning to have a similar representation, which can improve the performance of classifiers.\n",
        "\n",
        "You can refer to more details regarding sentiment analysis [here](https://monkeylearn.com/sentiment-analysis/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4FWVNFHNJl1"
      },
      "source": [
        "## Implementation with the IMDB Dataset\n",
        "\n",
        "In this post, you will discover how you can predict the sentiment of movie reviews as either positive or negative in Python using the Keras deep learning library.\n",
        "\n",
        "After reading this post you will know:\n",
        "\n",
        "- About the IMDB sentiment analysis problem for natural language processing and how to load it in Keras.\n",
        "- How to use word embedding in Keras for natural language problems.\n",
        "- How to develop and evaluate a multi-layer perception model for the IMDB problem.\n",
        "- How to develop a one-dimensional convolutional neural network model for the IMDB problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEIceo5AVfoB"
      },
      "source": [
        "### Overview of the Dataset and the Classification Problem\n",
        "\n",
        "The dataset is the Large Movie Review Dataset often referred to as the IMDB dataset, which is now provided with `keras`.\n",
        "\n",
        "The Large Movie Review Dataset (often referred to as the IMDB dataset) contains 25,000 highly polar moving reviews (good or bad) for training and the same amount again for testing. The problem is to determine whether a given moving review has a positive or negative sentiment.\n",
        "\n",
        "The data was collected by Stanford researchers and was used in a 2011 paper [PDF](http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf) where a split of 50/50 of the data was used for training and test. An accuracy of 88.89% was achieved.\n",
        "\n",
        "The data was also used as the basis for a Kaggle competition titled “[Bag of Words Meets Bags of Popcorn](https://www.kaggle.com/c/word2vec-nlp-tutorial/data)” in late 2014 to early 2015. Accuracy was achieved above `97%` with winners achieving `99%`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_wGG9HPWgId"
      },
      "source": [
        "### Loading the IMDB Dataset\n",
        "\n",
        "The `keras.datasets.imdb.load_data()` allows you to load the dataset in a format that is ready for use in neural network and deep learning models.\n",
        "\n",
        "The words have been replaced by integers that indicate the absolute popularity of the word in the dataset. The sentences in each review are therefore comprised of a sequence of integers. In other words, the original text data is already represented as numbers.\n",
        "\n",
        "Calling `imdb.load_data()` the first time will download the IMDB dataset to your computer and store it in your home directory under `~/.keras/datasets/imdb.pkl` as a 32 megabyte file.\n",
        "\n",
        "Usefully, the `imdb.load_data()` provides additional arguments including the number of top words to load (where words with a lower integer are marked as zero in the returned data), the number of top words to skip (to avoid stopwords like “the”‘s) and the maximum length of reviews to support.\n",
        "\n",
        "Let’s load the dataset and calculate some properties of it. We will start off by loading some libraries and loading the entire IMDB dataset as a training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWQdH12VZW8_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "82ec3c05-520e-421e-aa6f-9bcb7adc6bbb"
      },
      "source": [
        "import numpy as np\n",
        "from keras.datasets import imdb\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cy5GVMzHTvqd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "bde89c53-707f-4ad5-e606-ce6ca299fd64"
      },
      "source": [
        "\n",
        "# load the dataset\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data()\n",
        "X = np.concatenate((X_train, X_test), axis=0)\n",
        "y = np.concatenate((y_train, y_test), axis=0)\n",
        "\n",
        "# summarize size\n",
        "print(\"Training data: \")\n",
        "print(X.shape[0], 'rows')\n",
        "print(y.shape[0], 'labels')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 1s 0us/step\n",
            "Training data: \n",
            "50000 rows\n",
            "50000 labels\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nckuGyJX5I8"
      },
      "source": [
        "We can also print the unique class values. We can see that it is a binary classification problem for good and bad sentiment in the review."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pBvcTZcXSrh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "598d14a4-35f0-4f5d-9923-02fdc9c44f46"
      },
      "source": [
        "# Summarize number of classes\n",
        "# 0: negative; 1: positive\n",
        "print(\"Classes: \")\n",
        "print(np.unique(y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classes: \n",
            "[0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ5DDq0SYWHS"
      },
      "source": [
        "Next we can get an idea of the total number of unique words in the dataset. Interestingly, we can see that there are just under 100,000 words across the entire dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azaFeGneX8uz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2ef363f4-d086-44c3-e9a7-dff950ff9a07"
      },
      "source": [
        "# Summarize number of words\n",
        "print(\"Number of words: \")\n",
        "print(len(np.unique(np.hstack(X))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words: \n",
            "88585\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5q_MbsvYgTS"
      },
      "source": [
        "Finally, we can get an idea of the average review length. We can see that the average review has just around `234` words with a standard deviation of around `173` words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUa9CUilYU13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b51fdba0-9ff1-448d-c5eb-3764dfb74e7e"
      },
      "source": [
        "# Summarize review length\n",
        "print(\"Review length: \")\n",
        "result = [len(x) for x in X]\n",
        "print(\"Mean %.2f words (%f)\" % (np.mean(result), np.std(result)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review length: \n",
            "Mean 234.76 words (172.911495)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJ1sLR_ZZEKz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "outputId": "1afaa7fa-2ae2-4665-dcbc-e2ab1eb9e982"
      },
      "source": [
        "# plot review length\n",
        "plt.figure(figsize=(6,10))\n",
        "plt.boxplot(result)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAI/CAYAAACbAQmrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAeiElEQVR4nO3df2yc9Z3g8bcTmyjJNiHUumA7SGSlCSfaP3YlZCp8Ujj32k2jSph/vqKlKc22TVWWjFJiRLbnhFWTkxLJ6YHVCjVpS4kEJV9pl2m0myvHNtpWZ8S62WpPW1qKEWVFbBPWJT8gToOTzP3hSc4Jxp7Hsf14+L5fEornmRnPx5L79tPvPPM8deVyGUlSGhbkPYAkae4YfUlKiNGXpIQYfUlKiNGXpIQYfUlKSH3eA0zB40klaXrqJto436PP4OBg3iNIE2psbGR4eDjvMaT3aW5u/sD7XN6RpIQYfUlKiNGXpIQYfUlKiNGXpIQYfUlKiNGXpIQYfUlKiNGXpIQYfUlKiNGXpIQYfUlKiNGXpIQYfUlKiNGXpIQYfUlKiNGXpIQYfUlKyJSXSwwh3AQcAFYyds3afTHGx0IIfwN8FfiPykO/GWM8XHnOXwNfBi4AxRjjc5Xt64DHgIXA92OMu2f2x5EkTaauXJ782uMhhCagKcb4qxDCR4B/ATqAALwbY+y+6vG3Aj8GWoFm4B+BNZW7XwE+BRwDfgl8Lsb4m0levuw1cjVfeY1czVeVa+ROeGH0KZd3YoxDMcZfVb5+B/gt0DLJU+4Cnokxnosx/h54lbE/AK3AqzHG12KM7wHPVB4r1ZRSqUR7ezuLFy+mvb2dUqmU90hS1aZc3hkvhHAz8OfAPwNtwAMhhC8CR4GtMcYTjP1BeHHc047x//9IvHHV9tunN7aUj1KpxJ49e+ju7mb9+vUcPnyYzs5OADo6OnKeTppa1dEPIfwJ8LfAlhjj6RDC48BOxtb5dwJ7gb+81oFCCJuATQAxRhobG6/1W0oz5rvf/S779+/nzjvvpL6+nrvuuovly5fzjW98g6985St5jydNqarohxAaGAv+UzHGvwOIMR4fd/9+4O8rNweAm8Y9fVVlG5NsvyzGuA/YV7lZds1U88nLL7/MLbfcwvDw8OU1/VtuuYWXX37Z9X3NG5U1/QlNuaYfQqgDfgD8Nsb47XHbm8Y97G7g15WvDwH3hBAWhRBWAwWgj7E3bgshhNUhhOuAeyqPlWpGoVCgr6/vim19fX0UCoWcJpKyqWZPvw3YAPxbCOFfK9u+CXwuhPBnjC3vvA58DSDG+FIIIQK/Ac4DfxVjvAAQQngAeI6xQzZ/GGN8aQZ/FmnWFYtFOjs7L6/p9/b20tnZycMPP5z3aFJVpjxkM2cesql5p1Qq0dPTQ39/P4VCgWKx6Ju4mlcmO2TT6EvT5HH6mq+u6Th9SdKHh9GXpIQYfUlKiNGXpIQYfUlKiNGXpIQYfUlKiNGXpIQYfUlKiNGXpIQYfUlKiNGXpIQYfUlKiNGXpIQYfUlKiNGXpIQYfUlKiNGXpIQYfUlKiNGXpIQYfUlKiNGXpIQYfUlKiNGXpIQYfUlKiNGXpIQYfUlKiNGXMiqVSrS3t7N48WLa29splUp5jyRVrT7vAaRaUiqV2LNnD93d3axfv57Dhw/T2dkJQEdHR87TSVNzT1/KoKenh+7ubtra2mhoaKCtrY3u7m56enryHk2qitGXMujv76e1tfWKba2trfT39+c0kZSN0ZcyKBQK9PX1XbGtr6+PQqGQ00RSNkZfyqBYLNLZ2Ulvby+jo6P09vbS2dlJsVjMezSpKnXlcjnvGSZTHhwczHsG6QqlUomenh76+/spFAoUi0XfxNW80tzcDFA30X1GX5qmxsZGhoeH8x5Dep/Jou/yjiQlxOhLUkKMviQlxOhLUkKMviQlxOhLUkKMviQlxOhLUkKMviQlxOhLUkKMviQlxOhLUkKMviQlxOhLUkKMviQlxOhLUkKMviQlxOhLUkKMviQlxOhLUkKMviQlxOhLUkKMviQlxOhLUkKMviQlxOhLUkKMviQlxOhLUkKMviQlxOhLUkKMviQlxOhLGZVKJdrb21m8eDHt7e2USqW8R5KqVp/3AFItKZVK7Nmzh+7ubtavX8/hw4fp7OwEoKOjI+fppKnVlcvlvGeYTHlwcDDvGaTL2tvbWbduHT/96U/p7++nUChcvn3kyJG8x5MAaG5uBqib6D739KUMXnnlFUZGRti7d+/lPf2tW7dy7NixvEeTquKavpRBQ0MDGzdupK2tjYaGBtra2ti4cSMNDQ15jyZVxehLGYyOjvLEE0/Q29vL6Ogovb29PPHEE4yOjuY9mlQVoy9lsGbNGu6++262b9/OsmXL2L59O3fffTdr1qzJezSpKkZfyqBYLFIqldi5cyenT59m586dlEolisVi3qNJVfHoHSmjUqlET0/P5aN3isWih2tqXpns6B2jL01TY2Mjw8PDeY8hvc9k0Xd5R5ISYvQlKSFGX5ISYvQlKSFGX5ISYvQlKSFGX5ISYvQlKSFGX5ISYvQlKSFGX5ISYvQlKSFGX5ISMuU1ckMINwEHgJVAGdgXY3wshHADcBC4GXgdCDHGEyGEOuAxYD0wAnwpxviryve6D+iqfOtdMcYnZ/bHkSRNppo9/fPA1hjjrcAngL8KIdwKbAN+FmMsAD+r3Ab4DFCo/LcJeByg8kfiEeB2oBV4JISwYgZ/FknSFKaMfoxx6NKeeozxHeC3QAtwF3BpT/1J4NJVJO4CDsQYyzHGF4HrQwhNwF8Az8cY344xngCeB9bN6E8jSZpUpjX9EMLNwJ8D/wysjDEOVe56k7HlHxj7g/DGuKcdq2z7oO2SpDky5Zr+JSGEPwH+FtgSYzwdQrh8X4yxHEKYkUtwhRA2MbYsRIyRxsbGmfi20oyrr6/391M1p6rohxAaGAv+UzHGv6tsPh5CaIoxDlWWb96qbB8Abhr39FWVbQPAnVdt/6erXyvGuA/YV7lZ9nJ0mq+8XKLmq8rlEic05fJO5WicHwC/jTF+e9xdh4D7Kl/fB/xk3PYvhhDqQgifAE5VloGeAz4dQlhReQP305VtkqQ5Us2efhuwAfi3EMK/VrZ9E9gNxBDCl4F/By6t9xxm7HDNVxk7ZHMjQIzx7RDCTuCXlcd9K8b49oz8FJKkqtSVyzOyFD9byoODg3nPIE3I5R3NV5XlnbqJ7vMTuZKUEKMvSQkx+pKUEKMvSQkx+pKUEKMvSQkx+pKUEKMvSQkx+pKUEKMvSQkx+pKUEKMvSQkx+pKUEKMvSQkx+pKUEKMvSQkx+pKUEKMvSQkx+pKUEKMvSQkx+pKUEKMvZVQqlWhvb2fx4sW0t7dTKpXyHkmqmtGXMiiVSuzYsYORkRHK5TIjIyPs2LHD8KtmGH0pg127drFw4UL27t3LO++8w969e1m4cCG7du3KezSpKkZfymBoaIhHH32UtrY2GhoaaGtr49FHH2VoaCjv0aSqGH1JSojRlzJoampiy5Yt9Pb2Mjo6Sm9vL1u2bKGpqSnv0aSqGH0pg66uLi5cuMDWrVtZtmwZW7du5cKFC3R1deU9mlSV+rwHkGpJR0cHAD09PQAsWbKEbdu2Xd4uzXd15XI57xkmUx4cHMx7BmlCjY2NDA8P5z2G9D7Nzc0AdRPd5/KOJCXE6EtSQoy+JCXE6EtSQoy+JCXE6EtSQoy+JCXE6EtSQoy+JCXE6EtSQoy+JCXE6EtSQoy+JCXE6EtSQoy+JCXE6EsZlUol2tvbWbx4Me3t7ZRKpbxHkqrmlbOkDEqlEnv27KG7u5v169dz+PBhOjs7Abx6lmqCe/pSBj09PXR3d9PW1kZDQwNtbW10d3dfvnyiNN8ZfSmD/v5+Wltbr9jW2tpKf39/ThNJ2Rh9KYNCoUBfX98V2/r6+igUCjlNJGVj9KUMisUinZ2d9Pb2Mjo6Sm9vL52dnRSLxbxHk6pSVy6X855hMuXBwcG8Z5CuUCqV6Onpob+/n0KhQLFY9E1czSvNzc0AdRPd556+JCXEQzalDDxkU7XOPX0pAw/ZVK0z+lIGHrKpWmf0pQw8ZFO1zuhLGXjIpmqdh2xKGXnIpua7yQ7ZNPrSNDU2NjI8PJz3GNL7eJy+JAkw+pKUFKMvSQkx+pKUEKMvSQkx+pKUEKMvZeSF0VXLPMumlIFn2VStc09fysCzbKrWGX0pA8+yqVpn9KUMPMumap3RlzLwLJuqdZ5wTcrIs2xqvvMsm9Is8Cybmq88y6YkCTD6kpQUoy9JCTH6UkaehkG1zNMwSBl4GgbVOvf0pQw8DYNqndGXMvA0DKp1Rl/KwNMwqNa5pi9lUCwW+frXv86SJUs4duwYq1atYmRkhG9961t5jyZVxT19aZrq6ib8wKM0rxl9KYOenh4ef/xxXnzxRc6ePcuLL77I448/7hu5qhlGX8qgv7+foaGhK47THxoa8o1c1QxPuCZlcNttt3HhwgW+853vXD5O/4EHHmDhwoUcPXo07/EkwBOuSZIqjL6UwfHjx+nq6mL79u0sW7aM7du309XVxfHjx/MeTaqK0ZcyKBQK3HjjjRw5coSzZ89y5MgRbrzxRo/TV82Y8jj9EMIPgc8Cb8UYP17Z9jfAV4H/qDzsmzHGw5X7/hr4MnABKMYYn6tsXwc8BiwEvh9j3D2zP4o0+y5dLvHSuXcuXS7x4Ycfzns0qSrVfDjrR8B3gANXbf+fMcbu8RtCCLcC9wAfA5qBfwwhrKnc/V3gU8Ax4JchhEMxxt9cw+zSnLt0UrXt27dzzz33UCgUePjhhz3ZmmrGlNGPMf4ihHBzld/vLuCZGOM54PchhFeBSycqeTXG+BpACOGZymONvmpOR0cHHR0dXi5RNelaTsPwQAjhi8BRYGuM8QTQArw47jHHKtsA3rhq++3X8NqSpGmYbvQfB3YC5cq/e4G/nImBQgibgE0AMUYaGxtn4ttKM66+vt7fT9WcaUU/xnj5+LQQwn7g7ys3B4Cbxj10VWUbk2y/+nvvA/ZVbpb9v8+ar1ze0XxV+XDWhKYV/RBCU4xxqHLzbuDXla8PAU+HEL7N2Bu5BaCPsU+GFUIIqxmL/T3A56fz2pKk6ZvyNAwhhB8DdwKNwHHgkcrtP2Nseed14GuX/giEEP47Y0s954EtMcb/Vdm+HniUsUM2fxhj/B9VzOdpGDRvuaev+Wqy0zB47h1pmoy+5ivPvSNJAoy+JCXF6EtSQoy+JCXE6EtSQoy+JCXE6EtSQoy+JCXE6EtSQoy+JCXE6EtSQoy+lFGpVKK9vZ3FixfT3t5OqVTKeySpakZfyqBUKrFjxw5GRkYol8uMjIywY8cOw6+aYfSlDHbt2sXo6OgV20ZHR9m1a1dOE0nZGH0pg6GhIRYtWsTevXt555132Lt3L4sWLWJoaGjqJ0vzgNGXMtq0aRNtbW00NDTQ1tbGpk2b8h5JqprRlzLat28fvb29jI6O0tvby759+6Z+kjRPTOsauVKqmpqaOHnyJJ///Oc5f/489fX1NDQ00NTUlPdoUlXc05cyWLduHefOnWPFihXU1dWxYsUKzp07x7p16/IeTaqK0ZcyeOGFF9i8eTM33HADdXV13HDDDWzevJkXXngh79GkqnhhdCmDm266iddee42GhobLF0YfHR3lT//0T3njjTfyHk8CvDC6NGMKhQJ9fX1XbOvr66NQKOQ0kZSN0ZcyKBaLdHZ2XnH0TmdnJ8ViMe/RpKq4vCNlVCqV6Onpob+/n0KhQLFYpKOjI++xpMsmW94x+tI0XVrTl+Yb1/SlGeRZNlXL/HCWlEGpVGLPnj10d3ezfv16Dh8+TGdnJ4BLPKoJ7ulLGfT09NDd3X3FuXe6u7vp6enJezSpKkZfyqC/v5/W1tYrtrW2ttLf35/TRFI2Rl/KwOP0VeuMvpSBx+mr1vlGrpRBR0cHR48e5Qtf+ALvvfce1113Hffee69v4qpmuKcvZVAqlTh06BArV66krq6OlStXcujQIQ/bVM0w+lIGu3btYuHChVdcLnHhwoVeI1c1w+hLGQwNDRFCYPv27Sxbtozt27cTQvAauaoZRl/K6ODBg+zcuZPTp0+zc+dODh48mPdIUtWMvpRBfX09o6OjV2wbHR2lvt5jIlQbPOGalMGqVatYsWIFS5cuZWBggJaWFs6cOcOJEyc4duxY3uNJgCdck2bMmjVr2LBhA0uWLAFgyZIlbNiwgTVr1uQ8mVQdoy9lUCwWefbZZ69Y03/22Wf9cJZqhss7UkZeREXznRdRkWaBF1HRfOWaviQJMPqSlBSjL0kJMfpSRl4jV7XMjxFKGXiNXNU69/SlDLxGrmqd0Zcy6O/v580337xieefNN9/0GrmqGR6nL2Vw2223cebMGZYvX3753DunTp1i6dKlHD16NO/xJMDj9KUZc/bsWd599102btzIH/7wBzZu3Mi7777L2bNn8x5NqorRlzI4efIk999/PwcPHuSjH/0oBw8e5P777+fkyZN5jyZVxehLGd1xxx0cOXKEs2fPcuTIEe644468R5KqZvSlDJqamtiyZQu9vb2Mjo7S29vLli1baGpqyns0qSpGX8qgq6uLCxcusHXrVj7ykY+wdetWLly4QFdXV96jSVXxw1lSBpc+gNXT00NdXR1Llixh27ZtfjBLNcNDNqVp8tTKmq88ZFOSBBh9SUqK0Zcy8iybqmW+kStlUCqVePDBBzl37hwAv/vd73jwwQcBz7Kp2uCevpTBQw89xLlz59iwYQNvvfUWGzZs4Ny5czz00EN5jyZVxT19KYORkRHuvfdedu/ezfLly9m9ezcXL17kqaeeyns0qSru6UsZffKTn5z0tjSfGX0po82bN19xGobNmzfnPZJUNZd3pAzWrl3Lz3/+c7761a9y6tQpli9fzpkzZ1i7dm3eo0lVcU9fyuDpp59m7dq1nD59GoDTp0+zdu1ann766Zwnk6rjaRikafI0DJqvPA2DJAkw+pKUFKMvZdTV1cXq1atZtGgRq1ev9lz6qilGX8qgq6uLAwcOsG3bNk6cOMG2bds4cOCA4VfN8I1cKYPVq1ezbds2vva1r11+I/d73/seu3fv5ve//33e40mAb+RKM+a9997jmWeeoaWlhUWLFtHS0sIzzzzDe++9l/doUlWMvpTRK6+8wtKlSwFYunQpr7zySs4TSdUz+tI0nDlz5op/pVph9CUpIUZfymjBggXEGHn33XeJMbJggf8zUu3wt1XK6OLFi+zfv59Tp06xf/9+Ll68mPdIUtU8y6Y0Dc8//zwtLS15jyFl5p6+lMF1112Xabs03xh9KYP6+on/z/EHbZfmG39TpQxGRka47rrruHjxIufPn6e+vp4FCxYwMjKS92hSVdzTlzJasGABTU1NV/wr1Qr39KWM/vjHPzIwMMDFixcv/yvVCndRpGm4FHqDr1pj9CUpIUZfkhJi9CUpIUZfkhJi9CUpIVMeshlC+CHwWeCtGOPHK9tuAA4CNwOvAyHGeCKEUAc8BqwHRoAvxRh/VXnOfcClC4nuijE+ObM/iiRpKtXs6f8IWHfVtm3Az2KMBeBnldsAnwEKlf82AY/D5T8SjwC3A63AIyGEFdc6vCQpmymjH2P8BfD2VZvvAi7tqT8JdIzbfiDGWI4xvghcH0JoAv4CeD7G+HaM8QTwPO//QyJJmmXTXdNfGWMcqnz9JrCy8nUL8Ma4xx2rbPug7ZKkOXTNp2GIMZZDCOWZGAYghLCJsaUhYow0NjbO1LeWZpW/q6oF043+8RBCU4xxqLJ881Zl+wBw07jHrapsGwDuvGr7P030jWOM+4B9lZvl4eHhaY4ozS1/VzVfNDc3f+B9013eOQTcV/n6PuAn47Z/MYRQF0L4BHCqsgz0HPDpEMKKyhu4n65skyTNoWoO2fwxY3vpjSGEY4wdhbMbiCGELwP/DoTKww8zdrjmq4wdsrkRIMb4dghhJ/DLyuO+FWO8+s1hSdIsqyuXZ2w5fjaUBwcH855Bumyy6+IODAzM4STSB6ss79RNdJ+fyJWkhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUpI/bU8OYTwOvAOcAE4H2O8LYRwA3AQuBl4HQgxxhMhhDrgMWA9MAJ8Kcb4q2t5fWmmtLS0zMn3GBgYuObXka7FNUW/4r/GGIfH3d4G/CzGuDuEsK1y+2HgM0Ch8t/twOOVf6XcVRvjycJu0FULZmN55y7gycrXTwId47YfiDGWY4wvAteHEJpm4fUlSR/gWqNfBv53COFfQgibKttWxhiHKl+/CaysfN0CvDHuuccq26Sa8UF78+7lq1Zc6/LOf4kxDoQQ/hPwfAjh5fF3xhjLIYRylm9Y+eOxqfJ8Ghsbr3FEaWadO3cOgEWLFl3+WqoV1xT9GONA5d+3QgjPAq3A8RBCU4xxqLJ881bl4QPATeOevqqy7ervuQ/YV7lZHh4evvoh0rzh76fmo+bm5g+8b9rLOyGEpSGEj1z6Gvg08GvgEHBf5WH3AT+pfH0I+GIIoS6E8Ang1LhlIEnSHLiWNf2VwP8JIfxfoA/4hxjjT4HdwKdCCP3Af6vcBjgMvAa8CuwH7r+G15YkTUNduZxpyX2ulQcHB/OeQZpQS0uLb+BqXqos79RNdJ+fyJWkhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUqI0ZekhBh9SUpIfd4DSLPhYx/7GCdPnpz112lpaZnV73/99dfz0ksvzeprKC1GXx9KJ0+eZGBgYFZfo7GxkeHh4Vl9jdn+o6L0uLwjSQkx+pKUEKMvSQkx+pKUEKMvSQkx+pKUEKMvSQkx+pKUEKMvSQkx+pKUEKMvSQkx+pKUEKMvSQkx+pKUEKMvSQkx+pKUEKMvSQkx+pKUEKMvSQkx+pKUkLpyuZz3DJMpDw4O5j2DatBdT72c9wgz5if3/ue8R1CNaW5uBqib6D6jrw+llpYWBgYGZvU1GhsbGR4entXXmIufQx8+k0Xf5R1JSojRl6SEGH1JSojRl6SEGH1JSojRl6SEGH1JSkh93gNIs6WlpSXvEa7Z9ddfn/cI+pAx+vpQmosPNPnBKdUil3ckKSFGX5ISYvQlKSFGX5ISYvQlKSFGX5ISYvQlKSFGX5ISYvQlKSFGX5ISYvQlKSFGX5ISYvQlKSFGX5ISYvQlKSFGX5ISYvQlKSFGX5ISYvQlKSFzfo3cEMI64DFgIfD9GOPuuZ5BklI1p3v6IYSFwHeBzwC3Ap8LIdw6lzNIUsrmenmnFXg1xvhajPE94BngrjmeQZKSNdfLOy3AG+NuHwNun+MZpPdpaWmZk+cNDAxM63WkmTLna/pTCSFsAjYBxBhpbGzMeSKl4Ny5c5mfU19fz/nz52dhGmn2zHX0B4Cbxt1eVdl2WYxxH7CvcrM8PDw8R6NJ2TQ2NuLvp+aj5ubmD7xvrqP/S6AQQljNWOzvAT4/xzNIUrLm9I3cGON54AHgOeC3Y5viS3M5gySlrK5cLuc9w2TKg4ODec8gTcjlHc1XleWduonu8xO5kpQQoy9JCTH6kpQQoy9JCTH6kpQQoy9JCTH6kpQQoy9JCTH6kpQQoy9JCTH6kpQQoy9JCTH6kpQQoy9JCTH6kpQQoy9JCTH6kpQQoy9JCTH6kpSQeX+N3LwHkKQaNeE1cuvneoqMJhxamg9CCEdjjLflPYeUhcs7kpQQoy9JCTH60vTty3sAKav5/kauJGkGuacvSQmZ70fvSPNOCOGHwGeBt2KMH897HikL9/Sl7H4ErMt7CGk6jL6UUYzxF8Dbec8hTYfRl6SEGH1JSojRl6SEGH1JSogfzpIyCiH8GLgTaASOA4/EGH+Q61BSlYy+JCXE5R1JSojRl6SEGH1JSojRl6SEGH1JSojRl6SEGH1JSojRl6SE/D9EI8huqRMVbgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y65D0zYLZsWr"
      },
      "source": [
        "### Word Embedding\n",
        "A recent breakthrough in the field of natural language processing is called [word embedding](https://en.wikipedia.org/wiki/Word_embedding).\n",
        "\n",
        "This is a technique where words are encoded as real-valued vectors in a high-dimensional space, where the similarity between words in terms of meaning translates to closeness in the vector space.\n",
        "\n",
        "Discrete words are mapped to vectors of continuous numbers. This is useful when working with natural language problems with neural networks and deep learning models are we require numbers as input.\n",
        "\n",
        "Keras provides a convenient way to convert positive integer representations of words into a word embedding by an `Embedding` layer.\n",
        "\n",
        "The layer takes arguments that define the mapping including the maximum number of expected words also called the vocabulary size (e.g. the largest integer value that will be seen as an integer). The layer also allows you to specify the dimensionality for each word vector, called the output dimension.\n",
        "\n",
        "We would like to use a word embedding representation for the IMDB dataset.\n",
        "\n",
        "We will load our data (again) using the following settings:\n",
        "- Let’s say that we are only interested in the first 5,000 most used words in the dataset. Therefore our vocabulary size will be `5,000`. In this way, we select the first `5,000` words in the vocabulary.\n",
        "- We can choose to use a 32-dimension vector to represent each word. The more dimensions we use to represent the words, the finer difference we can capture in terms of the meaning of words.\n",
        "- Finally, we may choose to cap the maximum review length at 500 words, truncating reviews longer than that and padding reviews shorter than that with 0 values. Neural nets can only deal with input of the __same__ length.\n",
        "\n",
        "We would load the IMDB dataset as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksEqLnEsZLCN"
      },
      "source": [
        "top_words = 5000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKjzPLJTbnKh"
      },
      "source": [
        "We would then use the Keras utility to truncate or pad the dataset to a length of 500 for each observation using the `preprocessing.sequence.pad_sequences()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73srqHmJba4_"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "max_words = 500\n",
        "X_train = pad_sequences(X_train, maxlen=max_words)\n",
        "X_test = pad_sequences(X_test, maxlen=max_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dJ06ghYcDjn"
      },
      "source": [
        "Finally, later on, the first layer of our model would be an word embedding layer created using the `Embedding` layer as follows:\n",
        "\n",
        "```python\n",
        "Embedding(5000, 32, input_length=500)\n",
        "```\n",
        "\n",
        "The output of this first layer would be a matrix with the size 32×500 for a given review training or test pattern in integer format.\n",
        "\n",
        "Now that we know how to load the IMDB dataset in Keras and how to use a word embedding representation for it, let’s develop and evaluate some models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8qWKf0Acfyq"
      },
      "source": [
        "### Simple Multi-Layer Perceptron Model for the IMDB Dataset\n",
        "We can start off by developing a simple multi-layer perceptron (MLP) model with a single hidden layer.\n",
        "\n",
        "The word embedding representation is a true innovation and we will demonstrate what would have been considered world class results in 2011 with a relatively simple neural network.\n",
        "\n",
        "Let’s start off by importing the classes and functions required for this model and initializing the random number generator to a constant value to ensure we can easily reproduce the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3APkSNucB7u"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.layers import Embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNomOtsvc_Hp"
      },
      "source": [
        "We will flatten the Embedded layers output to one dimension, then use one dense hidden layer of 250 units with a `relu` activation function. The output layer has one neuron and will use a `sigmoid` activation to output values of 0 and 1 as predictions.\n",
        "\n",
        "The model uses logarithmic loss (`binary_crossentropy`) and is optimized using the efficient `adam` optimization procedure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Se4pykbwc3F3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "b5565901-5c0c-476b-9c2c-ce079980d904"
      },
      "source": [
        "# create the MLP model\n",
        "mlp_model = Sequential()\n",
        "# input layer\n",
        "mlp_model.add(Embedding(top_words, 32, input_length=max_words))\n",
        "# hidden layer\n",
        "mlp_model.add(Flatten())\n",
        "mlp_model.add(Dense(250, activation='relu'))\n",
        "# output layer - for binary classification we only need 1 neuron and the `sigmoid` activation\n",
        "mlp_model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "mlp_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "mlp_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 500, 32)           160000    \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 16000)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 250)               4000250   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 251       \n",
            "=================================================================\n",
            "Total params: 4,160,501\n",
            "Trainable params: 4,160,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-XVHSHUd-Qu"
      },
      "source": [
        "We can fit the model and use the test set as validation while training. This model __overfits__ very quickly so we will use very few training epochs, in this case just `2`.\n",
        "\n",
        "There is a lot of data so we will use a batch size of `128`. After the model is trained, we evaluate its accuracy on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUTdTnEgdxHn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "c6723f1c-ee6e-4ff4-da1b-12c096a1c251"
      },
      "source": [
        "# Fit the model\n",
        "mlp_model.fit(X_train, y_train, validation_split=0.1, epochs=2, batch_size=128, verbose=2)\n",
        "# Final evaluation of the model\n",
        "scores = mlp_model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.4f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "176/176 - 2s - loss: 0.4991 - accuracy: 0.7307 - val_loss: 0.3067 - val_accuracy: 0.8672\n",
            "Epoch 2/2\n",
            "176/176 - 2s - loss: 0.1877 - accuracy: 0.9285 - val_loss: 0.3096 - val_accuracy: 0.8712\n",
            "Accuracy: 87.3720%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDp-tZ6Hehdv"
      },
      "source": [
        "### YOUR TURN HERE\n",
        "\n",
        "I’m sure we can do better if we trained this network, perhaps using a larger embedding and adding more hidden layers, or use more neurons in the hidden layer(s). Can you try tweaking the model architecture to see what type of improvements you can make to the results?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-haDE_3eeHFA"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZ6VrHufe6oo"
      },
      "source": [
        "### One-Dimensional Convolutional Neural Network Model for the IMDB Dataset\n",
        "Convolutional neural networks (CNNs) were designed to honor the spatial structure in image data whilst being robust to the __position__ and __orientation__ of learned objects in the scene.\n",
        "\n",
        "This same principle can be used on sequences, such as the one-dimensional sequence of words in a movie review. The same properties that make the CNN model attractive for learning to recognize objects in images can help to learn structure in paragraphs of words, namely the techniques invariance to the specific position of features.\n",
        "\n",
        "Keras supports one-dimensional convolutions and pooling by the `Conv1D` and `MaxPooling1D` classes respectively.\n",
        "\n",
        "Again, let’s import the classes and functions needed for this network and initialize our random number generator to a constant value so that we can easily reproduce results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qo0uKfaWfLNt"
      },
      "source": [
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXfT1P9wfceH"
      },
      "source": [
        "We can now define our convolutional neural network model. This time, after the Embedding input layer, we insert a Conv1D layer. This convolutional layer has `32` feature maps and reads embedded word representations 3 vector elements (i.e. 3 words) of the word embedding at a time.\n",
        "\n",
        "The convolutional layer is followed by a 1D max pooling layer with a length and stride of `2` that halves the size of the feature maps from the convolutional layer. The rest of the network is the same as the neural network above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRf8-kcbfXxj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "3639602d-28dc-4f88-e8b3-856f403c6b73"
      },
      "source": [
        "# create the model\n",
        "conv1d_model = Sequential()\n",
        "# input layer\n",
        "conv1d_model.add(Embedding(top_words, 32, input_length=max_words))\n",
        "# hidden layer 1\n",
        "conv1d_model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "conv1d_model.add(MaxPooling1D(pool_size=2))\n",
        "# hidden layer 2\n",
        "conv1d_model.add(Flatten())\n",
        "conv1d_model.add(Dense(250, activation='relu'))\n",
        "# output layer\n",
        "conv1d_model.add(Dense(1, activation='sigmoid'))\n",
        "# compile model\n",
        "conv1d_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "conv1d_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 500, 32)           160000    \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 500, 32)           3104      \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 250, 32)           0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 8000)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 250)               2000250   \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 251       \n",
            "=================================================================\n",
            "Total params: 2,163,605\n",
            "Trainable params: 2,163,605\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fvDtkt1gQOg"
      },
      "source": [
        "We also fit the network the same as before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_QM0PyFf4v5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "5969673f-bbcb-46b2-871c-aed96d3b1ede"
      },
      "source": [
        "# Fit the model\n",
        "conv1d_model.fit(X_train, y_train, validation_split=0.1, epochs=2, batch_size=128, verbose=2)\n",
        "# Final evaluation of the model\n",
        "scores = conv1d_model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.4f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "176/176 - 2s - loss: 0.4742 - accuracy: 0.7389 - val_loss: 0.3010 - val_accuracy: 0.8768\n",
            "Epoch 2/2\n",
            "176/176 - 2s - loss: 0.2120 - accuracy: 0.9181 - val_loss: 0.2981 - val_accuracy: 0.8840\n",
            "Accuracy: 87.7640%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoMz-9G1gtOZ"
      },
      "source": [
        "We can see the improvement using the CNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6AmGXAphMdm"
      },
      "source": [
        "### YOUR TURN HERE\n",
        "\n",
        "I’m sure we can do better if we trained this network, perhaps  adding more hidden layers, or use more neurons in the hidden layer(s). Can you try tweaking the model architecture to see what type of improvements you can make to the results?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDsJ3sDphSom"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYHvvMjYgw1i"
      },
      "source": [
        "### Recurrent Neural Network (LSTM) for the IMDB Dataset\n",
        "Recurrent neural network is a type of neural networks that is proven to work well with sequence data. Since text is actually a sequence of words, a recurrent neural network is an automatic choice to solve text-related problems. In this section, we will use an LSTM (Long Short Term Memory network) which is a variant of RNN, to solve sentiment classification problem. The idea is that RNN would capture the order of words as a pattern in the classification model.\n",
        "\n",
        "Once again, execute the code until the word embedding section and after that run the following piece of code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29TDhMPAhmL_"
      },
      "source": [
        "from tensorflow.keras.layers import LSTM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbD28GqkgW6g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "d657337c-3b2b-405e-a07b-dfb91f639f8a"
      },
      "source": [
        "# create LSTM model\n",
        "lstm_model = Sequential()\n",
        "# input layer\n",
        "lstm_model.add(Embedding(top_words, 32, input_length=max_words))\n",
        "# hidden layer\n",
        "lstm_model.add(LSTM(128))\n",
        "# output layer\n",
        "lstm_model.add(Dense(1, activation='sigmoid'))\n",
        "# compile model\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "lstm_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 500, 32)           160000    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 128)               82432     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 242,561\n",
            "Trainable params: 242,561\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFclT9IxiEhe"
      },
      "source": [
        "We also fit the network the same as before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3HLzZqRh9e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "ca00f299-ca98-4d08-a8ac-131b9dbe611d"
      },
      "source": [
        "# Fit the model\n",
        "lstm_model.fit(X_train, y_train, validation_split=0.1, epochs=2, batch_size=128, verbose=2)\n",
        "# Final evaluation of the model\n",
        "scores = lstm_model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.4f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "176/176 - 9s - loss: 0.5021 - accuracy: 0.7564 - val_loss: 0.3333 - val_accuracy: 0.8632\n",
            "Epoch 2/2\n",
            "176/176 - 8s - loss: 0.2906 - accuracy: 0.8836 - val_loss: 0.3145 - val_accuracy: 0.8680\n",
            "Accuracy: 86.3520%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A39PDyM9ljYp"
      },
      "source": [
        "With a simple one layer LSTM model, the results are not as good as expected. However, there are a few ways to improve the model performance:\n",
        "- adding more LSTM layers to the model\n",
        "- adding more neurons to each LSTM layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQSyzNJsiJQE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "69729b53-8d29-4b9c-fe18-adbbb461bffe"
      },
      "source": [
        "# create LSTM model\n",
        "lstm_model = Sequential()\n",
        "# input layer\n",
        "lstm_model.add(Embedding(top_words, 32, input_length=max_words))\n",
        "# hidden layer 1\n",
        "lstm_model.add(LSTM(256, return_sequences=1))\n",
        "# hidden layer 2\n",
        "lstm_model.add(LSTM(128))\n",
        "# output layer\n",
        "lstm_model.add(Dense(1, activation='sigmoid'))\n",
        "# compile model\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "lstm_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 500, 32)           160000    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 500, 256)          295936    \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 128)               197120    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 653,185\n",
            "Trainable params: 653,185\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IpPIJpCoO_J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "c412fdcf-2fd9-458d-b2be-488d41bc45d2"
      },
      "source": [
        "# Fit the model\n",
        "lstm_model.fit(X_train, y_train, validation_split=0.1, epochs=2, batch_size=128, verbose=2)\n",
        "# Final evaluation of the model\n",
        "scores = lstm_model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.4f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "176/176 - 27s - loss: 0.5193 - accuracy: 0.7217 - val_loss: 0.5509 - val_accuracy: 0.6964\n",
            "Epoch 2/2\n",
            "176/176 - 26s - loss: 0.4740 - accuracy: 0.7712 - val_loss: 0.6993 - val_accuracy: 0.5304\n",
            "Accuracy: 51.5800%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v03xLXG70wXN"
      },
      "source": [
        "Using the LSTM model, we can capture the order of words and it impact on the classification results. However, sometimes we know that the context of a word (which possibly determine the semantic of the word), may not only include the words before the target word but also the words after the target word. Thus, we need to capture not only the regular order of words, but also the _reversed_ order of words in the orginal text.\n",
        "\n",
        "Luckily, `keras` provides a __wrapper__ (which is used to transform a layer) called `Bidirectional` for that purpose. Keep in mind that the bidirectional wrapper can only be used for RNN models.\n",
        "\n",
        "We can define a bidirectional LSTM model as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7qA8wRVoTei"
      },
      "source": [
        "from tensorflow.keras.layers import Bidirectional"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCXJ9KVQ11zN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "e9d34a47-fb60-4902-bb3a-08451d208232"
      },
      "source": [
        "# create LSTM model\n",
        "lstm_model = Sequential()\n",
        "# input layer\n",
        "lstm_model.add(Embedding(top_words, 32, input_length=max_words))\n",
        "# hidden layer 1\n",
        "lstm_model.add(Bidirectional(LSTM(256, return_sequences=1)))\n",
        "# hidden layer 2\n",
        "lstm_model.add(LSTM(128))\n",
        "# output layer\n",
        "lstm_model.add(Dense(1, activation='sigmoid'))\n",
        "# compile model\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "lstm_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, 500, 32)           160000    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 500, 512)          591872    \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 128)               328192    \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 1,080,193\n",
            "Trainable params: 1,080,193\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ii85VXX317WF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "13225ea5-146c-45ce-8261-4d097b3e507f"
      },
      "source": [
        "# Fit the model\n",
        "lstm_model.fit(X_train, y_train, validation_split=0.1, epochs=2, batch_size=128, verbose=2)\n",
        "# Final evaluation of the model\n",
        "scores = lstm_model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.4f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "176/176 - 48s - loss: 0.5515 - accuracy: 0.7031 - val_loss: 0.3735 - val_accuracy: 0.8388\n",
            "Epoch 2/2\n",
            "176/176 - 47s - loss: 0.3302 - accuracy: 0.8637 - val_loss: 0.3235 - val_accuracy: 0.8564\n",
            "Accuracy: 86.1960%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYIqwNs12wMw"
      },
      "source": [
        "### YOUR TURN HERE\n",
        "\n",
        "Can you try tweaking the model hyperparameters (e.g. changing the number of epochs, batch size, and optimizers), to see what type of improvements you can make to the results?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-Lzii-A3Csc"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53xyLa9e2JG-"
      },
      "source": [
        "## Conclusion\n",
        "Text classification is one of the most common natural language processing tasks. In this article we saw how to perform sentiment analysis, which is a type of text classification using Keras deep learning library. We used three different types of neural networks to classify public sentiment about different movies.\n",
        "\n",
        "Supposedly, LSTM, which is a variant of RNN should outperform both the CNN and MLP. However, since in this version of the IMDB review data, the words are represented only in 32-dimensional vectors, the semantics of them are not well captured.\n",
        "\n",
        "In the subsequent tutorials, we are going to dig deeper into the word embedding models, to find how to better capture the semantics of words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjwSr3Aw1-AU"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}