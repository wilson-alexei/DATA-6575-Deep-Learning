{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.4.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lK0izX0-QHh"
      },
      "source": [
        "# Gradient Trader Part 1: The Surprising Usefulness of Autoencoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X23wb7E7-QHi"
      },
      "source": [
        "## Using Autoencoders to Learn Most Salient Features from Time Series\n",
        "This post is about a simple tool in deep learning toolbox: Autoencoder. It can be applied to multi-dimensional financial time series.\n",
        "\n",
        "### Autoencoder\n",
        "Autoencoding is the practice of copying input to output or learning the identity function. It has an internal state called latent space h which is used to represent the input. Usually, this dimension is chosen to be smaller than the input(called undercomplete). Autoencoder is composed of two parts: an encoder f:x --> H and a decoder g:H --> y.\n",
        "\n",
        "<img src = 'https://blog.keras.io/img/ae/autoencoder_schema.jpg' />\n",
        "\n",
        "The hidden dimension should be smaller than x, the input dimension. This way, h is forced to take on useful properties and most salient features of the input space.\n",
        "\n",
        "Train an autoencoder to find function f,g such that:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qO5rgTM-QHk"
      },
      "source": [
        "\\begin{equation*}\n",
        "\\arg min ||X - (g * f)X||^2\n",
        "\\end{equation*}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5_Uukp8-QHl"
      },
      "source": [
        "### Recurrent Autoencoder\n",
        "\n",
        "For time series data, recurrent autoencoder are especially useful. The only difference is that the encoder and decoder are replaced by RNNs such as LSTMs. Think of RNN as a for loop over time step so the state is kept. It can be unrolled into a feedforward network.\n",
        "\n",
        "First, the input is encoded into an undercomplete latent vector h which is then decoded by the decoder. Recurrent autoencoder is a special case of sequence-to-sequence(seq2seq) architecture which is extremely powerful in neural machine translation where the neural network maps one language sequence to another.\n",
        "\n",
        "<img src = 'https://esciencegroup.files.wordpress.com/2016/03/seq2seq.jpg' />"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "q7rqA40q-QHl"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouVXDCJD-QHm"
      },
      "source": [
        "**Task**\n",
        "\n",
        "Copy a tensor of two sine functions that are initialized out of phase.\n",
        "\n",
        "The shape of the input is a tensor of shape (batch_size,time_step,input_dim).\n",
        "\n",
        "- batch_size is the number of batches for training. Looping over each sample is slower than applying a tensor operation on a batch of several samples.\n",
        "\n",
        "- time_step is the number of timeframes for the RNN to iterate over. In this tutorial it is 10 because 10 points are generated.\n",
        "\n",
        "- input_dim is the number of data points at each timestep. Here we have 2 functions, so this number is 2.\n",
        "\n",
        "To deal with financial data, simply replace the input_dim axis with desired data points.\n",
        "\n",
        "- Bid, Ask, Spread, Volume, RSI. For this setup, the input_dim would be 5.\n",
        "- Order book levels. We can rebin the order book along tick axis so each tick aggregates more liquidity. An example would be 10 levels that are 1 stdev apart. Then input_dim would be 10.\n",
        "\n",
        "Here is an artist’s rendition of a recurrent autoencoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0uCA0cL-QHp"
      },
      "source": [
        "plt.xkcd()\n",
        "\n",
        "x1 = np.linspace(-np.pi, np.pi)\n",
        "y1 = np.sin(x1)\n",
        "phi = 3\n",
        "x2 = np.linspace(-np.pi+phi, np.pi+phi)\n",
        "y2 = np.sin(x2)\n",
        "\n",
        "f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, sharex=True, sharey=True)\n",
        "ax1.plot(x1, y1)\n",
        "ax1.set_title('Recurrent Autoencoder')\n",
        "ax2.plot(x1, y1)\n",
        "ax3.plot(x2, y2)\n",
        "ax4.plot(x2, y2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_p9yXn2K-QHp"
      },
      "source": [
        "###Generator\n",
        "For each batch, generate 2 sine functions, each with 10 datapoints. The phase of the sine function is random.\n",
        "\n",
        "**This is the function generating time series pieces.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "DKy1DIww-QHq"
      },
      "source": [
        "import random\n",
        "def gen(batch_size):\n",
        "    seq_length = 10\n",
        "\n",
        "    batch_x = []\n",
        "    batch_y = []\n",
        "    for _ in range(batch_size):\n",
        "        rand = random.random() * 2 * np.pi\n",
        "\n",
        "        sig1 = np.sin(np.linspace(0.0 * np.pi + rand,\n",
        "                                  3.0 * np.pi + rand, seq_length * 2))\n",
        "        sig2 = np.cos(np.linspace(0.0 * np.pi + rand,\n",
        "                                  3.0 * np.pi + rand, seq_length * 2))\n",
        "        x1 = sig1[:seq_length]\n",
        "        y1 = sig1[seq_length:]\n",
        "        x2 = sig2[:seq_length]\n",
        "        y2 = sig2[seq_length:]\n",
        "\n",
        "        x_ = np.array([x1, x2])\n",
        "        y_ = np.array([y1, y2])\n",
        "        x_, y_ = x_.T, y_.T\n",
        "\n",
        "        batch_x.append(x_)\n",
        "        batch_y.append(y_)\n",
        "\n",
        "    batch_x = np.array(batch_x)\n",
        "    batch_y = np.array(batch_y)\n",
        "\n",
        "    return batch_x, batch_x#batch_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oik9OGvo-QHr"
      },
      "source": [
        "###Model\n",
        "The goal is to use two numbers to represent the sine functions. Normally, we use ϕ∈ℝ to represent the phase angle for a trignometric function. Let’s see if the neural network can learn this phase angle. The big picture here is to compress the input sine functions into two numbers and then decode them back.\n",
        "\n",
        "Define the architecture and let the neural network do its trick. It is a model with 3 layers, a LSTM encoder that “encodes” the input time series into a fixed length vector(in this case 2). A RepeatVector that repeats the fixed length vector to 10 timesteps to be used as input to the LSTM decoder. For the decoder, we can either initialize the hidden state(memory) with the latent vector and use output at time *t−1 as input for time t or we can use latent vector h as the input at each timestep. These are called conditional and unconditional decoders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HScTj2V-QHr"
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import LSTM, RepeatVector\n",
        "\n",
        "batch_size = 100\n",
        "X_train, _ = gen(batch_size)\n",
        "\n",
        "m = Sequential()\n",
        "m.add(LSTM(2, input_shape=(10, 2)))\n",
        "m.add(RepeatVector(10))\n",
        "m.add(LSTM(2, return_sequences=True))\n",
        "print (m.summary())\n",
        "m.compile(loss='mse', optimizer='adam')\n",
        "history = m.fit(X_train, X_train, epochs=500, batch_size=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMpVUMKM-QHs"
      },
      "source": [
        "Since this post is a demonstration of the technique, we use the smallest model possible which happens to be the best in this case. The “best” dimensionality will be one that results in the highest lossless compression. In practice, it’s mostly an art than science. In production, there are a plethora of trick to accelerate training and finding the right capacity of the latent vector. Topics include: architectures for dealing with asynchronus, non-stationary time series, preprocessing techniques such as wavelet transforms and FFT."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcP7CzrW-QHw"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tG7R5_ng-QHx"
      },
      "source": [
        "You may think that the neural network suddenly “got it” during training but this is just the optimzer escaping a saddle point. In fact, as we demonstrate below, the neural network(at least the decoder) doesn’t learn the math behind it at all.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncoN5ECB-QHy"
      },
      "source": [
        "X_test, _ = gen(1)\n",
        "decoded_imgs = m.predict(X_test)\n",
        "\n",
        "for i in range(2):\n",
        "    plt.plot(range(10), decoded_imgs[0, :, i])\n",
        "    plt.plot(range(10), X_test[0, :, i])\n",
        "plt.title('dos_numeros')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "hQyh_tm7-QHy"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}